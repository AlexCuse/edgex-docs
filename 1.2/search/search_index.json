{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction EdgeX Foundry is an open source, vendor neutral, flexible, interoperable, software platform at the edge of the network, that interacts with the physical world of devices , sensors, actuators, and other IoT objects. In simple terms, EdgeX is edge middleware - serving between physical sensing and actuating \"things\" and our information technology (IT) systems. The EdgeX platform enables and encourages the rapidly growing community of IoT solution providers to work together in an ecosystem of interoperable components to reduce uncertainty, accelerate time to market, and facilitate scale. By bringing this much-needed interoperability, EdgeX makes it easier to monitor physical world items, send instructions to them, collect data from them, move the data across the fog up to the cloud where it may be stored, aggregated, analyzed, and turned into information, actuated, and acted upon. So EdgeX enables data to travel northwards towards the cloud or enterprise and back to devices, sensors, and actuators. The initiative is aligned around a common goal: the simplification and standardization of the foundation for tiered edge computing architectures in the IoT market while still enabling the ecosystem to provide significant value-added differentiation. If you don't need further description and want to immediately use EdgeX Foundry use this link: Getting Started Guide EdgeX Foundry Use Cases Originally built to support industrial IoT needs, EdgeX today is used in a variety of use cases to include: Building automation \u2013 helping to manage shared workspace facilities Oil/gas \u2013 closed loop control of a gas supply valve Retail \u2013 multi-sensor reconciliation for loss prevention at the point of sale Water treatment \u2013 monitor and control chemical dosing Consumer IoT \u2013 the open source HomeEdge project is using elements of EdgeX as part of its smart home platform EdgeX Foundry Architectural Tenets EdgeX Foundry was conceived with the following tenets guiding the overall architecture: EdgeX Foundry must be platform agnostic with regard to Hardware (x86, ARM) Operating system (Linux, Windows, MacOS, ...) Distribution (allowing for the distribution of functionality through microservices at the edge, on a gateway, in the fog, on cloud, etc.) Deployment/orchestration (Docker, Snaps, K8s, roll-your-own, ... ) Protocols ( north or south side protocols) EdgeX Foundry must be extremely flexible Any part of the platform may be upgraded, replaced or augmented by other micro services or software components Allow services to scale up and down based on device capability and use case EdgeX Foundry should provide \" reference implementation \" services but encourages best of breed solutions EdgeX Foundry must provide for store and forward capability To support disconnected/remote edge systems To deal with intermittent connectivity EdgeX Foundry must support and facilitate \"intelligence\" moving closer to the edge in order to address Actuation latency concerns Bandwidth and storage concerns Operating remotely concerns EdgeX Foundry must support brown and green device/sensor field deployments EdgeX Foundry must be secure and easily managed Deployments EdgeX was originally built by Dell to run on its IoT gateways . While EdgeX can and does run on gateways, its platform agnostic nature and micro service architecture enables tiered distributed deployments. In other words, a single instance of EdgeX\u2019s micro services can be distributed across several host platforms. The host platform for one or many EdgeX micro services is called a node. This allows EdgeX to leverage compute, storage, and network resources wherever they live on the edge. Its loosely-coupled architecture enables distribution across nodes to enable tiered edge computing. For example, thing communicating services could run on a programmable logic controller (PLC), a gateway, or be embedded in smarter sensors while other EdgeX services are deployed on networked servers or even in the cloud. The scope of a deployment could therefore include embedded sensors, controllers, edge gateways, servers and cloud systems. EdgeX micro services can be deployed across an array of compute nodes to maximize resources while at the same time position more processing intelligence closer to the physical edge. The number and the function of particular micro services deployed on a given node depends on the use case and capability of the hardware and infrastructure. Apache 2 License EdgeX is distributed under Apache 2 License backed by the Apache Foundation. Apache 2 licensing is very friendly (\u201cpermissive\u201d) to open and commercial interests. It allows users to use the software for any purpose. It allows users to distribute, modify or even fork the code base without seeking permission from the founding project. It allows users to change or extend the code base without having to contribute back to the founding project. It even allows users to build commercial products without concerns for profit sharing or royalties to go back to the Linux Foundation or open source project organization. EdgeX Foundry Service Layers EdgeX Foundry is a collection of open source micro services. These micro services are organized into 4 service layers, and 2 underlying augmenting system services. The Service Layers traverse from the edge of the physical realm (from the Device Services Layer), to the edge of the information realm (that of the Application Services Layer), with the Core and Supporting Services Layers at the center. The 4 Service Layers of EdgeX Foundry are as follows: Core Services Layer Supporting Services Layer Application Services Layer Device Services Layer The 2 underlying System Services of EdgeX Foundry are as follows: Security System Management Core Services Layer Core services provide the intermediary between the north and south sides of EdgeX. As the name of these services implies, they are \u201ccore\u201d to EdgeX functionality. Core services is where most of the innate knowledge of what \u201cthings\u201d are connected, what data is flowing through, and how EdgeX is configured resides in an EdgeX instance. Core consists of the following micro services: Core data: a persistence repository and associated management service for data collected from south side objects. Command: a service that facilitates and controls actuation requests from the north side to the south side. Metadata: a repository and associated management service of metadata about the objects that are connected to EdgeX Foundry. Metadata provides the capability to provision new devices and pair them with their owning device services. Registry and Configuration: provides other EdgeX Foundry micro services with information about associated services within EdgeX Foundry and micro services configuration properties (i.e. - a repository of initialization values). Core services provide intermediary communications between the things and the IT systems. Supporting Services Layer The supporting services encompass a wide range of micro services to include edge analytics (also known as local analytics). Normal software application duties such as logging, scheduling, and data clean up (also known as scrubbing in EdgeX) are performed by micro services in the supporting services layer. These services often require some amount of core services in order to function. In all cases, supporting service can be considered optional \u2013 that is they can be left out of an EdgeX deployment depending on use case needs and system resources. Supporting services include: Rules Engine: the reference implementation edge analytics service that performs if-then conditional actuation at the edge based on sensor data collected by the EdgeX instance. This service will likely be replaced or augmented by use case specific analytics capability. Scheduling: an internal EdgeX \u201cclock\u201d that can kick off operations in any EdgeX service. At a configuration specified time, the service will call on any EdgeX service API URL via REST to trigger an operation. For example, the scheduling service periodically calls on core data APIs to clean up old sensed events that have been successfully exported out of EdgeX. Logging: provides a central logging facility for all of EdgeX services. Services send log entries into the logging facility via a REST API where log entries can be persisted in a database or log file. Note : this service is being deprecated and will be removed after the Geneva release. Services will still be able to log using standard output or log to a file. Most operating systems and logging facilities provide better logging aggregation then what EdgeX was providing through the logging service. Alerts and Notifications: provides EdgeX services with a central facility to send out an alert or notification. These are notices sent to another system or to a person monitoring the EdgeX instance (internal service communications are often handled more directly). Application Services Layer Application services are the means to extract, process/transform and send sensed data from EdgeX to an endpoint or process of your choice. EdgeX today offers application service examples to send data to many of the major cloud providers (Amazon IoT Hub, Google IoT Core, Azure IoT Hub, IBM Watson IoT\u2026), to MQTT(s) topics, and HTTP(s) REST endpoints. Application services are based on the idea of a \"functions pipeline\". A functions pipeline is a collection of functions that process messages (in this case EdgeX event messages) in the order specified. The first function in a pipeline is a trigger. A trigger begins the functions pipeline execution. A trigger, for example, is something like a message landing in a message queue. Each function then acts on the message. Common functions include filtering, transformation (i.e. to XML or JSON), compression, and encryption functions. The function pipeline ends when the message has gone through all the functions and is set to a sink. Putting the resulting message into an MQTT topic to be sent to Azure or AWS is an example of a sink completing an application service. Device Services Layer Device services connect \u201cthings\u201d \u2013 that is sensors and devices \u2013 into the rest of EdgeX. Device services are the edge connectors interacting with the \"things\" that include, but are not limited to: alarm systems, heating and air conditioning systems in homes and office buildings, lights, machines in any industry, irrigation systems, drones, currently automated transit such as some rail systems, currently automated factories, and appliances in your home. In the future, this may include driverless cars and trucks, traffic signals, fully automated fast food facilities, fully automated self-serve grocery stores, devices taking medical readings from patients, etc. Device services may service one or a number of things or devices (sensor, actuator, etc.) at one time. A device that a device service manages, could be something other than a simple, single, physical device. The device could be another gateway (and all of that gateway's devices), a device manager, a device aggregator that acts as a device, or collection of devices, to EdgeX Foundry. The device service communicates with the devices, sensors, actuators, and other IoT objects through protocols native to each device object. The device service converts the data produced and communicated by the IoT object into a common EdgeX Foundry data structure, and sends that converted data into the core services layer, and to other micro services in other layers of EdgeX Foundry. EdgeX comes with a number of device services speaking many common IoT protocols such as Modbus, BACnet, BLE, etc. System Services Layer Security Infrastructure Security elements of EdgeX Foundry protect the data and control of devices, sensors, and other IoT objects managed by EdgeX Foundry. Based on the fact that EdgeX is a \"vendor-neutral open source software platform at the edge of the network\", the EdgeX security features are also built on a foundation of open interfaces and pluggable, replaceable modules. There are two major EdgeX security components. A security store, which is used to provide a safe place to keep the EdgeX secrets. Examples of EdgeX secrets are the database access passwords used by the other services and tokens to connect to cloud systems. An API gateway serves as the reverse proxy to restrict access to EdgeX REST resources and perform access control related works. System Management System Management facilities provide the central point of contact for external management systems to start/stop/restart EdgeX services, get the status/health of a service, or get metrics on the EdgeX services (such as memory usage) so that the EdgeX services can be monitored. Software Development Kits (SDKs) Two types of SDKs are provided by EdgeX to assist in creating north and south side services \u2013 specifically to create application services and device services. SDKs for both the north and south side services make connecting new things or new cloud/enterprise systems easier by providing developers all the scaffolding code that takes care of the basic operations of the service. Thereby allowing developers to focus on specifics of their connectivity to the south or north side object without worrying about all the raw plumbing of a micro service. SDKs are language specific; meaning an SDK is written to create services in a particular programming language. Today, EdgeX offers the following SDKs: Golang Device Service SDK C Device Service SDK Golang Device Service SDK How EdgeX Works Sensor Data Collection EdgeX\u2019s primary job is to collect data from sensors and devices and make that data available to north side applications and systems. Data is collected from a sensor by a device service that speaks the protocol of that device. Example: a Modbus device service would communicate in Modbus to get a pressure reading from a Modbus pump. The device service translates the sensor data into an EdgeX event object and sends the event object to the core data service via REST communications (step 1). Core data persists the sensor data in the local edge database. Redis is used as the database by default (other databases can be used alternatively). In fact, persistence is not required and can be turned off. Data is persisted in EdgeX at the edge for two basics reasons: Edge nodes are not always connected. During periods of disconnected operations, the sensor data must be saved so that it can be transmitted northbound when connectivity is restored. This is referred to as store and forward capability. In some cases, analytics of sensor data needs to look back in history in order to understand the trend and to make the right decision based on that history. If a sensor reports that it is 72\u00b0 F right now, you might want to know what the temperature was ten minutes ago before you make a decision to adjust a heating or cooling system. If the temperature was 85\u00b0 F, you may decide that adjustments to lower the room temperature you made ten minutes ago were sufficient to cool the room. It is the context of historical data that are important to local analytic decisions. Core data puts sensor data events on a message topic destined for application services. Zero MQ is used as the messaging infrastructure by default (step 2). The application service transforms the data as needed and pushes the data to an endpoint. It can also filter, enrich, compress, encrypt or perform other functions on the event before sending it to the endpoint (step 3). The endpoint could be an HTTP/S endpoint, an MQTT topic, a cloud system (cloud topic), etc. Edge Analytics and Actuation In edge computing, simply collecting sensor data is only part of the job of an edge platform like EdgeX. Another important job of an edge platform is to be able to: Analyze the incoming sensor data locally Act quickly on that analysis Edge or local analytics is the processing that performs an assessment of the sensor data collected at the edge (\u201clocally\u201d) and triggers actuations or actions based on what it sees. Why edge analytics ? Local analytics are important for two reasons: Some decisions cannot afford to wait for sensor collected data to be fed back to an enterprise or cloud system and have a response returned. Additionally, some edge systems are not always connected to the enterprise or cloud \u2013 they have intermittent periods of connectivity. Local analytics allows systems to operate independently, at least for some stretches of time. For example: a shipping container\u2019s cooling system must be able to make decisions locally without the benefit of Internet connectivity for long periods of time when the ship is at sea. Local analytics also allow a system to act quickly in a low latent fashion when critical to system operations. As an extreme case, imagine that your car\u2019s airbag fired on the basis of data being sent to the cloud and analyzed for collisions. Your car has local analytics to prevent such a potentially slow and error prone delivery of the safety actuation in your automobile. EdgeX is built to act locally on data it collects from the edge. In other words, events are processed by local analytics and can be used to trigger action back down on a sensor/device. Just as application services prepare data for consumption by north side cloud systems or applications, application services can process and get EdgeX events (and the sensor data they contain) to any analytics package (see step 4). By default, EdgeX ships with a simple rules engine (the default EdgeX rules engine is Kuiper \u2013 an open source rules engine by EMQ X). Your own analytics package (or ML agent) could replace or augment the local rules engine. The analytic package can explore the sensor event data and make a decision to trigger actuation of a device. For example, it could check that the pressure reading of an engine is greater than 60 PSI. When such a rule is determined to be true, the analytic package calls on the core command service to trigger some action, like \u201copen a valve\u201d on some controllable device (see step 5). The core command service gets the actuation request and determines which device it needs to act on with the request; then calling on the owning device service to do the actuation (see step 6). Core command allows developers to put additional security measures or checks in place before actuating. The device service receives the request for actuation, translates that into a protocol specific request and forwards the request to the desired device (see step 7). Project Release Cadence Typically, EdgeX releases twice a year; once in the spring and once in the fall. Bug fix releases may occur more often. Each EdgeX release has a code name. The code name follows an alphabetic pattern similar to Android (code names sequentially follow the alphabet). The code name of each release is named after some geographical location in the world. The honor of naming an EdgeX release is given to a developer deemed to have contributed significantly to the project in the past six months. A release also has a version number. The release version follows sematic versioning to indicate the release is major or minor in scope. Major releases typically contain significant new features and functionality and are not always backward compatible with prior releases. Minor releases are backward compatible and usually contain bug fixes and fewer new features. See the project Wiki for more information on releases, versions and patches . Release Schedule Version Barcelona Oct 2017 0.5.0 California Jun 2017 0.6.0 Delhi Oct 2018 0.7.0 Edinburgh Jul 2019 1.0.0 Fuji Nov 2019 1.1.0 Geneva May 2020 1.2.0 Hanoi Fall 2020 TBD Ireland Spring 2021 TBD Jakarta Fall 2021 TBD Note : minor releases of the Device Services and Application Services (along with their associated SDKs) can be release independently. EdgeX community members convene in a face-to-face meeting right at the time of a release to plan the next release and roadmap future releases. See the Project Wiki for more detailed information on releases and roadmap . EdgeX History and Naming EdgeX Foundry began as a project chartered by Dell IoT Marketing and developed by the Dell Client Office of the CTO as an incubation project called Project Fuse in July 2015. It was initially created to run as the IoT software application on Dell\u2019s introductory line of IoT gateways. Dell entered the project into open source through the Linux Foundation on April 24, 2017. EdgeX was formally announced and demonstrated at Hanover Messe 2017. Hanover Messe is one of the world's largest industrial trade fairs. At the fair, the Linux Foundation also announced the association of 50 founding member organizations \u2013 the EdgeX ecosystem \u2013 to help further the project and the goals of creating a universal edge platform. The name \u2018foundry\u2019 was used to draw parallels to Cloud Foundry . EdgeX Foundry is meant to be a foundry for solutions at the edge just like Cloud Foundry is a foundry for solutions in the cloud. Cloud Foundry was originated by VMWare (Dell Technologies is a major shareholder of VMWare - recall that Dell Technologies was the original creator of EdgeX). The \u2018X\u2019 in EdgeX represents the transformational aspects of the platform and allows the project name to be trademarked and to be used in efforts such as certification and certification marks. The EdgeX Foundry Logo represents the nature of its role as transformation engine between the physical OT world and the digital IT world. The EdgeX community selected the octopus as the mascot or \u201cspirit animal\u201d of the project at its inception. Its eight arms and the suckers on the arms represent the sensors. The sensors bring the data into the octopus. Actually, the octopus has nine brains in a way. It has millions of neurons running down each arm; functioning as mini-brains in each of those arms. The arms of the octopus serve as \u201clocal analytics\u201d like that offered by EdgeX. The mascot is affectionately called \u201cEdgey\u201d by the community.","title":"Introduction"},{"location":"#introduction","text":"EdgeX Foundry is an open source, vendor neutral, flexible, interoperable, software platform at the edge of the network, that interacts with the physical world of devices , sensors, actuators, and other IoT objects. In simple terms, EdgeX is edge middleware - serving between physical sensing and actuating \"things\" and our information technology (IT) systems. The EdgeX platform enables and encourages the rapidly growing community of IoT solution providers to work together in an ecosystem of interoperable components to reduce uncertainty, accelerate time to market, and facilitate scale. By bringing this much-needed interoperability, EdgeX makes it easier to monitor physical world items, send instructions to them, collect data from them, move the data across the fog up to the cloud where it may be stored, aggregated, analyzed, and turned into information, actuated, and acted upon. So EdgeX enables data to travel northwards towards the cloud or enterprise and back to devices, sensors, and actuators. The initiative is aligned around a common goal: the simplification and standardization of the foundation for tiered edge computing architectures in the IoT market while still enabling the ecosystem to provide significant value-added differentiation. If you don't need further description and want to immediately use EdgeX Foundry use this link: Getting Started Guide","title":"Introduction"},{"location":"#edgex-foundry-use-cases","text":"Originally built to support industrial IoT needs, EdgeX today is used in a variety of use cases to include: Building automation \u2013 helping to manage shared workspace facilities Oil/gas \u2013 closed loop control of a gas supply valve Retail \u2013 multi-sensor reconciliation for loss prevention at the point of sale Water treatment \u2013 monitor and control chemical dosing Consumer IoT \u2013 the open source HomeEdge project is using elements of EdgeX as part of its smart home platform","title":"EdgeX Foundry Use Cases"},{"location":"#edgex-foundry-architectural-tenets","text":"EdgeX Foundry was conceived with the following tenets guiding the overall architecture: EdgeX Foundry must be platform agnostic with regard to Hardware (x86, ARM) Operating system (Linux, Windows, MacOS, ...) Distribution (allowing for the distribution of functionality through microservices at the edge, on a gateway, in the fog, on cloud, etc.) Deployment/orchestration (Docker, Snaps, K8s, roll-your-own, ... ) Protocols ( north or south side protocols) EdgeX Foundry must be extremely flexible Any part of the platform may be upgraded, replaced or augmented by other micro services or software components Allow services to scale up and down based on device capability and use case EdgeX Foundry should provide \" reference implementation \" services but encourages best of breed solutions EdgeX Foundry must provide for store and forward capability To support disconnected/remote edge systems To deal with intermittent connectivity EdgeX Foundry must support and facilitate \"intelligence\" moving closer to the edge in order to address Actuation latency concerns Bandwidth and storage concerns Operating remotely concerns EdgeX Foundry must support brown and green device/sensor field deployments EdgeX Foundry must be secure and easily managed","title":"EdgeX Foundry Architectural Tenets"},{"location":"#deployments","text":"EdgeX was originally built by Dell to run on its IoT gateways . While EdgeX can and does run on gateways, its platform agnostic nature and micro service architecture enables tiered distributed deployments. In other words, a single instance of EdgeX\u2019s micro services can be distributed across several host platforms. The host platform for one or many EdgeX micro services is called a node. This allows EdgeX to leverage compute, storage, and network resources wherever they live on the edge. Its loosely-coupled architecture enables distribution across nodes to enable tiered edge computing. For example, thing communicating services could run on a programmable logic controller (PLC), a gateway, or be embedded in smarter sensors while other EdgeX services are deployed on networked servers or even in the cloud. The scope of a deployment could therefore include embedded sensors, controllers, edge gateways, servers and cloud systems. EdgeX micro services can be deployed across an array of compute nodes to maximize resources while at the same time position more processing intelligence closer to the physical edge. The number and the function of particular micro services deployed on a given node depends on the use case and capability of the hardware and infrastructure.","title":"Deployments"},{"location":"#apache-2-license","text":"EdgeX is distributed under Apache 2 License backed by the Apache Foundation. Apache 2 licensing is very friendly (\u201cpermissive\u201d) to open and commercial interests. It allows users to use the software for any purpose. It allows users to distribute, modify or even fork the code base without seeking permission from the founding project. It allows users to change or extend the code base without having to contribute back to the founding project. It even allows users to build commercial products without concerns for profit sharing or royalties to go back to the Linux Foundation or open source project organization.","title":"Apache 2 License"},{"location":"#edgex-foundry-service-layers","text":"EdgeX Foundry is a collection of open source micro services. These micro services are organized into 4 service layers, and 2 underlying augmenting system services. The Service Layers traverse from the edge of the physical realm (from the Device Services Layer), to the edge of the information realm (that of the Application Services Layer), with the Core and Supporting Services Layers at the center. The 4 Service Layers of EdgeX Foundry are as follows: Core Services Layer Supporting Services Layer Application Services Layer Device Services Layer The 2 underlying System Services of EdgeX Foundry are as follows: Security System Management","title":"EdgeX Foundry Service Layers"},{"location":"#core-services-layer","text":"Core services provide the intermediary between the north and south sides of EdgeX. As the name of these services implies, they are \u201ccore\u201d to EdgeX functionality. Core services is where most of the innate knowledge of what \u201cthings\u201d are connected, what data is flowing through, and how EdgeX is configured resides in an EdgeX instance. Core consists of the following micro services: Core data: a persistence repository and associated management service for data collected from south side objects. Command: a service that facilitates and controls actuation requests from the north side to the south side. Metadata: a repository and associated management service of metadata about the objects that are connected to EdgeX Foundry. Metadata provides the capability to provision new devices and pair them with their owning device services. Registry and Configuration: provides other EdgeX Foundry micro services with information about associated services within EdgeX Foundry and micro services configuration properties (i.e. - a repository of initialization values). Core services provide intermediary communications between the things and the IT systems.","title":"Core Services Layer"},{"location":"#supporting-services-layer","text":"The supporting services encompass a wide range of micro services to include edge analytics (also known as local analytics). Normal software application duties such as logging, scheduling, and data clean up (also known as scrubbing in EdgeX) are performed by micro services in the supporting services layer. These services often require some amount of core services in order to function. In all cases, supporting service can be considered optional \u2013 that is they can be left out of an EdgeX deployment depending on use case needs and system resources. Supporting services include: Rules Engine: the reference implementation edge analytics service that performs if-then conditional actuation at the edge based on sensor data collected by the EdgeX instance. This service will likely be replaced or augmented by use case specific analytics capability. Scheduling: an internal EdgeX \u201cclock\u201d that can kick off operations in any EdgeX service. At a configuration specified time, the service will call on any EdgeX service API URL via REST to trigger an operation. For example, the scheduling service periodically calls on core data APIs to clean up old sensed events that have been successfully exported out of EdgeX. Logging: provides a central logging facility for all of EdgeX services. Services send log entries into the logging facility via a REST API where log entries can be persisted in a database or log file. Note : this service is being deprecated and will be removed after the Geneva release. Services will still be able to log using standard output or log to a file. Most operating systems and logging facilities provide better logging aggregation then what EdgeX was providing through the logging service. Alerts and Notifications: provides EdgeX services with a central facility to send out an alert or notification. These are notices sent to another system or to a person monitoring the EdgeX instance (internal service communications are often handled more directly).","title":"Supporting Services Layer"},{"location":"#application-services-layer","text":"Application services are the means to extract, process/transform and send sensed data from EdgeX to an endpoint or process of your choice. EdgeX today offers application service examples to send data to many of the major cloud providers (Amazon IoT Hub, Google IoT Core, Azure IoT Hub, IBM Watson IoT\u2026), to MQTT(s) topics, and HTTP(s) REST endpoints. Application services are based on the idea of a \"functions pipeline\". A functions pipeline is a collection of functions that process messages (in this case EdgeX event messages) in the order specified. The first function in a pipeline is a trigger. A trigger begins the functions pipeline execution. A trigger, for example, is something like a message landing in a message queue. Each function then acts on the message. Common functions include filtering, transformation (i.e. to XML or JSON), compression, and encryption functions. The function pipeline ends when the message has gone through all the functions and is set to a sink. Putting the resulting message into an MQTT topic to be sent to Azure or AWS is an example of a sink completing an application service.","title":"Application Services Layer"},{"location":"#device-services-layer","text":"Device services connect \u201cthings\u201d \u2013 that is sensors and devices \u2013 into the rest of EdgeX. Device services are the edge connectors interacting with the \"things\" that include, but are not limited to: alarm systems, heating and air conditioning systems in homes and office buildings, lights, machines in any industry, irrigation systems, drones, currently automated transit such as some rail systems, currently automated factories, and appliances in your home. In the future, this may include driverless cars and trucks, traffic signals, fully automated fast food facilities, fully automated self-serve grocery stores, devices taking medical readings from patients, etc. Device services may service one or a number of things or devices (sensor, actuator, etc.) at one time. A device that a device service manages, could be something other than a simple, single, physical device. The device could be another gateway (and all of that gateway's devices), a device manager, a device aggregator that acts as a device, or collection of devices, to EdgeX Foundry. The device service communicates with the devices, sensors, actuators, and other IoT objects through protocols native to each device object. The device service converts the data produced and communicated by the IoT object into a common EdgeX Foundry data structure, and sends that converted data into the core services layer, and to other micro services in other layers of EdgeX Foundry. EdgeX comes with a number of device services speaking many common IoT protocols such as Modbus, BACnet, BLE, etc.","title":"Device Services Layer"},{"location":"#system-services-layer","text":"Security Infrastructure Security elements of EdgeX Foundry protect the data and control of devices, sensors, and other IoT objects managed by EdgeX Foundry. Based on the fact that EdgeX is a \"vendor-neutral open source software platform at the edge of the network\", the EdgeX security features are also built on a foundation of open interfaces and pluggable, replaceable modules. There are two major EdgeX security components. A security store, which is used to provide a safe place to keep the EdgeX secrets. Examples of EdgeX secrets are the database access passwords used by the other services and tokens to connect to cloud systems. An API gateway serves as the reverse proxy to restrict access to EdgeX REST resources and perform access control related works. System Management System Management facilities provide the central point of contact for external management systems to start/stop/restart EdgeX services, get the status/health of a service, or get metrics on the EdgeX services (such as memory usage) so that the EdgeX services can be monitored.","title":"System Services Layer"},{"location":"#software-development-kits-sdks","text":"Two types of SDKs are provided by EdgeX to assist in creating north and south side services \u2013 specifically to create application services and device services. SDKs for both the north and south side services make connecting new things or new cloud/enterprise systems easier by providing developers all the scaffolding code that takes care of the basic operations of the service. Thereby allowing developers to focus on specifics of their connectivity to the south or north side object without worrying about all the raw plumbing of a micro service. SDKs are language specific; meaning an SDK is written to create services in a particular programming language. Today, EdgeX offers the following SDKs: Golang Device Service SDK C Device Service SDK Golang Device Service SDK","title":"Software Development Kits (SDKs)"},{"location":"#how-edgex-works","text":"","title":"How EdgeX Works"},{"location":"#sensor-data-collection","text":"EdgeX\u2019s primary job is to collect data from sensors and devices and make that data available to north side applications and systems. Data is collected from a sensor by a device service that speaks the protocol of that device. Example: a Modbus device service would communicate in Modbus to get a pressure reading from a Modbus pump. The device service translates the sensor data into an EdgeX event object and sends the event object to the core data service via REST communications (step 1). Core data persists the sensor data in the local edge database. Redis is used as the database by default (other databases can be used alternatively). In fact, persistence is not required and can be turned off. Data is persisted in EdgeX at the edge for two basics reasons: Edge nodes are not always connected. During periods of disconnected operations, the sensor data must be saved so that it can be transmitted northbound when connectivity is restored. This is referred to as store and forward capability. In some cases, analytics of sensor data needs to look back in history in order to understand the trend and to make the right decision based on that history. If a sensor reports that it is 72\u00b0 F right now, you might want to know what the temperature was ten minutes ago before you make a decision to adjust a heating or cooling system. If the temperature was 85\u00b0 F, you may decide that adjustments to lower the room temperature you made ten minutes ago were sufficient to cool the room. It is the context of historical data that are important to local analytic decisions. Core data puts sensor data events on a message topic destined for application services. Zero MQ is used as the messaging infrastructure by default (step 2). The application service transforms the data as needed and pushes the data to an endpoint. It can also filter, enrich, compress, encrypt or perform other functions on the event before sending it to the endpoint (step 3). The endpoint could be an HTTP/S endpoint, an MQTT topic, a cloud system (cloud topic), etc.","title":"Sensor Data Collection"},{"location":"#edge-analytics-and-actuation","text":"In edge computing, simply collecting sensor data is only part of the job of an edge platform like EdgeX. Another important job of an edge platform is to be able to: Analyze the incoming sensor data locally Act quickly on that analysis Edge or local analytics is the processing that performs an assessment of the sensor data collected at the edge (\u201clocally\u201d) and triggers actuations or actions based on what it sees. Why edge analytics ? Local analytics are important for two reasons: Some decisions cannot afford to wait for sensor collected data to be fed back to an enterprise or cloud system and have a response returned. Additionally, some edge systems are not always connected to the enterprise or cloud \u2013 they have intermittent periods of connectivity. Local analytics allows systems to operate independently, at least for some stretches of time. For example: a shipping container\u2019s cooling system must be able to make decisions locally without the benefit of Internet connectivity for long periods of time when the ship is at sea. Local analytics also allow a system to act quickly in a low latent fashion when critical to system operations. As an extreme case, imagine that your car\u2019s airbag fired on the basis of data being sent to the cloud and analyzed for collisions. Your car has local analytics to prevent such a potentially slow and error prone delivery of the safety actuation in your automobile. EdgeX is built to act locally on data it collects from the edge. In other words, events are processed by local analytics and can be used to trigger action back down on a sensor/device. Just as application services prepare data for consumption by north side cloud systems or applications, application services can process and get EdgeX events (and the sensor data they contain) to any analytics package (see step 4). By default, EdgeX ships with a simple rules engine (the default EdgeX rules engine is Kuiper \u2013 an open source rules engine by EMQ X). Your own analytics package (or ML agent) could replace or augment the local rules engine. The analytic package can explore the sensor event data and make a decision to trigger actuation of a device. For example, it could check that the pressure reading of an engine is greater than 60 PSI. When such a rule is determined to be true, the analytic package calls on the core command service to trigger some action, like \u201copen a valve\u201d on some controllable device (see step 5). The core command service gets the actuation request and determines which device it needs to act on with the request; then calling on the owning device service to do the actuation (see step 6). Core command allows developers to put additional security measures or checks in place before actuating. The device service receives the request for actuation, translates that into a protocol specific request and forwards the request to the desired device (see step 7).","title":"Edge Analytics and Actuation"},{"location":"#project-release-cadence","text":"Typically, EdgeX releases twice a year; once in the spring and once in the fall. Bug fix releases may occur more often. Each EdgeX release has a code name. The code name follows an alphabetic pattern similar to Android (code names sequentially follow the alphabet). The code name of each release is named after some geographical location in the world. The honor of naming an EdgeX release is given to a developer deemed to have contributed significantly to the project in the past six months. A release also has a version number. The release version follows sematic versioning to indicate the release is major or minor in scope. Major releases typically contain significant new features and functionality and are not always backward compatible with prior releases. Minor releases are backward compatible and usually contain bug fixes and fewer new features. See the project Wiki for more information on releases, versions and patches . Release Schedule Version Barcelona Oct 2017 0.5.0 California Jun 2017 0.6.0 Delhi Oct 2018 0.7.0 Edinburgh Jul 2019 1.0.0 Fuji Nov 2019 1.1.0 Geneva May 2020 1.2.0 Hanoi Fall 2020 TBD Ireland Spring 2021 TBD Jakarta Fall 2021 TBD Note : minor releases of the Device Services and Application Services (along with their associated SDKs) can be release independently. EdgeX community members convene in a face-to-face meeting right at the time of a release to plan the next release and roadmap future releases. See the Project Wiki for more detailed information on releases and roadmap .","title":"Project Release Cadence"},{"location":"#edgex-history-and-naming","text":"EdgeX Foundry began as a project chartered by Dell IoT Marketing and developed by the Dell Client Office of the CTO as an incubation project called Project Fuse in July 2015. It was initially created to run as the IoT software application on Dell\u2019s introductory line of IoT gateways. Dell entered the project into open source through the Linux Foundation on April 24, 2017. EdgeX was formally announced and demonstrated at Hanover Messe 2017. Hanover Messe is one of the world's largest industrial trade fairs. At the fair, the Linux Foundation also announced the association of 50 founding member organizations \u2013 the EdgeX ecosystem \u2013 to help further the project and the goals of creating a universal edge platform. The name \u2018foundry\u2019 was used to draw parallels to Cloud Foundry . EdgeX Foundry is meant to be a foundry for solutions at the edge just like Cloud Foundry is a foundry for solutions in the cloud. Cloud Foundry was originated by VMWare (Dell Technologies is a major shareholder of VMWare - recall that Dell Technologies was the original creator of EdgeX). The \u2018X\u2019 in EdgeX represents the transformational aspects of the platform and allows the project name to be trademarked and to be used in efforts such as certification and certification marks. The EdgeX Foundry Logo represents the nature of its role as transformation engine between the physical OT world and the digital IT world. The EdgeX community selected the octopus as the mascot or \u201cspirit animal\u201d of the project at its inception. Its eight arms and the suckers on the arms represent the sensors. The sensors bring the data into the octopus. Actually, the octopus has nine brains in a way. It has millions of neurons running down each arm; functioning as mini-brains in each of those arms. The arms of the octopus serve as \u201clocal analytics\u201d like that offered by EdgeX. The mascot is affectionately called \u201cEdgey\u201d by the community.","title":"EdgeX History and Naming"},{"location":"api/Ch-APIAppFunctionsSDK/","text":"Application Service - Application Service SDK Architecture Reference For a description of the architecture, see Application Functions SDK Introduction The SDK is provided to help build Application Services by assembling triggers, pre-existing functions and custom functions of your making into a pipeline. Application Service SDK V1 API Swagger Documentation","title":"Application Service - Application Service SDK"},{"location":"api/Ch-APIAppFunctionsSDK/#application-service-application-service-sdk","text":"","title":"Application Service - Application Service SDK"},{"location":"api/Ch-APIAppFunctionsSDK/#architecture-reference","text":"For a description of the architecture, see Application Functions SDK","title":"Architecture Reference"},{"location":"api/Ch-APIAppFunctionsSDK/#introduction","text":"The SDK is provided to help build Application Services by assembling triggers, pre-existing functions and custom functions of your making into a pipeline. Application Service SDK V1 API Swagger Documentation","title":"Introduction"},{"location":"api/Ch-APIDeviceSDK/","text":"Device Service - Device SDK Architecture Reference For a description of the architecture, see Device Services Introduction The EdgeX Foundry Device Service Software Development Kit (SDK) takes the Developer through the step-by-step process to create an EdgeX Foundry Device Service microservice. Then setup the SDK and execute the code to generate the Device Service scaffolding to get you started using EdgeX. The Device Service SDK supports: Synchronous read and write operations Asynchronous Device data Initialization and deconstruction of Driver Interface Initialization and destruction of Device Connection Framework for automated Provisioning Mechanism Support for multiple classes of Devices with Profiles Support for sets of actions triggered by a command Cached responses to queries Device SDK V1 API Swagger Documentation","title":"Device Service - Device SDK"},{"location":"api/Ch-APIDeviceSDK/#device-service-device-sdk","text":"","title":"Device Service - Device SDK"},{"location":"api/Ch-APIDeviceSDK/#architecture-reference","text":"For a description of the architecture, see Device Services","title":"Architecture Reference"},{"location":"api/Ch-APIDeviceSDK/#introduction","text":"The EdgeX Foundry Device Service Software Development Kit (SDK) takes the Developer through the step-by-step process to create an EdgeX Foundry Device Service microservice. Then setup the SDK and execute the code to generate the Device Service scaffolding to get you started using EdgeX. The Device Service SDK supports: Synchronous read and write operations Asynchronous Device data Initialization and deconstruction of Driver Interface Initialization and destruction of Device Connection Framework for automated Provisioning Mechanism Support for multiple classes of Devices with Profiles Support for sets of actions triggered by a command Cached responses to queries Device SDK V1 API Swagger Documentation","title":"Introduction"},{"location":"api/Ch-APISystemManagement/","text":"APIs - System Management - Agent Architecture Reference Coming Soon Introduction The EdgeX System Management Agent (SMA) exposes the EdgeX management service API to 3rd party systems. In other words, the Agent serves as a proxy for system management service API calls into each micro service. In the future, the SMA may also offer the management API in other remote management/control system protocols like LWM2M, OMA DM, etc. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/system-agent.raml System Management V1 API Swagger Documentation","title":"APIs - System Management - Agent"},{"location":"api/Ch-APISystemManagement/#apis-system-management-agent","text":"","title":"APIs - System Management - Agent"},{"location":"api/Ch-APISystemManagement/#architecture-reference","text":"Coming Soon","title":"Architecture Reference"},{"location":"api/Ch-APISystemManagement/#introduction","text":"The EdgeX System Management Agent (SMA) exposes the EdgeX management service API to 3rd party systems. In other words, the Agent serves as a proxy for system management service API calls into each micro service. In the future, the SMA may also offer the management API in other remote management/control system protocols like LWM2M, OMA DM, etc. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/system-agent.raml System Management V1 API Swagger Documentation","title":"Introduction"},{"location":"api/core/Ch-APICoreCommand/","text":"Core Command Architecture Reference For a description of the architecture, see Core-Command Introduction EdgeX Foundry's Command microservice is a conduit for other services to trigger action on devices and sensors through their managing Device Services. The service provides an API to get the list of commands that can be issued for all devices or a single device. Commands are divided into two groups for each device: GET commands are issued to a device or sensor to get a current value for a particular attribute on the device, such as the current temperature provided by a thermostat sensor, or the on/off status of a light. PUT commands are issued to a device or sensor to change the current state or status of a device or one of its attributes, such as setting the speed in RPMs of a motor, or setting the brightness of a dimmer light. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/core-command.raml Core Command V1 API Swagger Documentation","title":"Core Command"},{"location":"api/core/Ch-APICoreCommand/#core-command","text":"","title":"Core Command"},{"location":"api/core/Ch-APICoreCommand/#architecture-reference","text":"For a description of the architecture, see Core-Command","title":"Architecture Reference"},{"location":"api/core/Ch-APICoreCommand/#introduction","text":"EdgeX Foundry's Command microservice is a conduit for other services to trigger action on devices and sensors through their managing Device Services. The service provides an API to get the list of commands that can be issued for all devices or a single device. Commands are divided into two groups for each device: GET commands are issued to a device or sensor to get a current value for a particular attribute on the device, such as the current temperature provided by a thermostat sensor, or the on/off status of a light. PUT commands are issued to a device or sensor to change the current state or status of a device or one of its attributes, such as setting the speed in RPMs of a motor, or setting the brightness of a dimmer light. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/core-command.raml Core Command V1 API Swagger Documentation","title":"Introduction"},{"location":"api/core/Ch-APICoreData/","text":"Core Data Architecture Reference For a description of the architecture, see Core-Data Introduction EdgeX Foundry Core Data Service includes the device and sensor collected data database and APIs to expose the database to other services as well as north-bound integration. The database is secure. Direct access to the database is restricted to the Core Data service APIs. Core Data also provides the REST API to create and register a new device. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/core-data.raml Core Data V1 API Swagger Documentation","title":"Core Data"},{"location":"api/core/Ch-APICoreData/#core-data","text":"","title":"Core Data"},{"location":"api/core/Ch-APICoreData/#architecture-reference","text":"For a description of the architecture, see Core-Data","title":"Architecture Reference"},{"location":"api/core/Ch-APICoreData/#introduction","text":"EdgeX Foundry Core Data Service includes the device and sensor collected data database and APIs to expose the database to other services as well as north-bound integration. The database is secure. Direct access to the database is restricted to the Core Data service APIs. Core Data also provides the REST API to create and register a new device. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/core-data.raml Core Data V1 API Swagger Documentation","title":"Introduction"},{"location":"api/core/Ch-APICoreMetadata/","text":"Core Metadata Architecture Reference For a description of the architecture, see Core-Metadata Introduction The Metadata microservice includes the device/sensor metadata database and APIs to expose the database to other services. In particular, the device provisioning service deposits and manages device metadata through this service. This service may also hold and manage other configuration metadata used by other services on the gateway such as clean up schedules, hardware configuration (Wi-Fi connection info, MQTT queues, and so forth). Non-device metadata may need to be held in a different database and/or managed by another service--depending upon implementation. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/core-metadata.raml Core Metadata V1 API Swagger Documentation","title":"Core Metadata"},{"location":"api/core/Ch-APICoreMetadata/#core-metadata","text":"","title":"Core Metadata"},{"location":"api/core/Ch-APICoreMetadata/#architecture-reference","text":"For a description of the architecture, see Core-Metadata","title":"Architecture Reference"},{"location":"api/core/Ch-APICoreMetadata/#introduction","text":"The Metadata microservice includes the device/sensor metadata database and APIs to expose the database to other services. In particular, the device provisioning service deposits and manages device metadata through this service. This service may also hold and manage other configuration metadata used by other services on the gateway such as clean up schedules, hardware configuration (Wi-Fi connection info, MQTT queues, and so forth). Non-device metadata may need to be held in a different database and/or managed by another service--depending upon implementation. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/core-metadata.raml Core Metadata V1 API Swagger Documentation","title":"Introduction"},{"location":"api/core/Ch-APICoreServiceConfiguration/","text":"Configuration and Registry Architecture Reference For a description of the architecture, see Configuration Introduction The RESTful APIs are provided by Consul directly, and several communities supply Consul client libraries for different programming languages, including Go (official), Python, Java, PHP, Scala, Erlang/OTP, Ruby, Node.js, and C#. For the client libraries of different languages, please refer to the list of this page: https://www.consul.io/downloads_tools.html Configuration Management For the current API documentation, please refer to the official Consul web site: https://www.consul.io/intro/getting-started/kv.html https://www.consul.io/docs/agent/http/kv.html Service Registry For the current API documentation, please refer to the official Consul web site: https://www.consul.io/intro/getting-started/services.html https://www.consul.io/docs/agent/http/catalog.html https://www.consul.io/docs/agent/http/agent.html https://www.consul.io/docs/agent/checks.html https://www.consul.io/docs/agent/http/health.html Service Registration While each microservice is starting up, it should connect to Consul to register its endpoint information, including microservice ID, address, port number, and health checking method. After that, other microservices can locate its URL from Consul, and Consul has the ability to monitor its health status. The RESTful API of registration is described on the following Consul page: https://www.consul.io/docs/agent/http/agent.html#agent_service_register Service Deregistration Before microservices shut down, they have to deregister themselves from Consul. The RESTful API of deregistration is described on the following Consul page: https://www.consul.io/docs/agent/http/agent.html#agent_service_deregister Service Discovery Service Discovery feature allows client micro services to query the endpoint information of a particular microservice by its microservice IDor list all available services registered in Consul. The RESTful API of querying service by microservice IDis described on the following Consul page: https://www.consul.io/docs/agent/http/catalog.html#catalog_service The RESTful API of listing all available services is described on the following Consul page: https://www.consul.io/docs/agent/http/agent.html#agent_services Health Checking Health checking is a critical feature that prevents using services that are unhealthy. Consul provides a variety of methods to check the health of services, including Script + Interval, HTTP + Interval, TCP + Interval, Time to Live (TTL), and Docker + Interval. The detailed introduction and examples of each checking methods are described on the following Consul page: https://www.consul.io/docs/agent/checks.html The health checks should be established during service registration. Please see the paragraph on this page of Service Registration section.","title":"Configuration and Registry"},{"location":"api/core/Ch-APICoreServiceConfiguration/#configuration-and-registry","text":"","title":"Configuration and Registry"},{"location":"api/core/Ch-APICoreServiceConfiguration/#architecture-reference","text":"For a description of the architecture, see Configuration","title":"Architecture Reference"},{"location":"api/core/Ch-APICoreServiceConfiguration/#introduction","text":"The RESTful APIs are provided by Consul directly, and several communities supply Consul client libraries for different programming languages, including Go (official), Python, Java, PHP, Scala, Erlang/OTP, Ruby, Node.js, and C#. For the client libraries of different languages, please refer to the list of this page: https://www.consul.io/downloads_tools.html","title":"Introduction"},{"location":"api/core/Ch-APICoreServiceConfiguration/#configuration-management","text":"For the current API documentation, please refer to the official Consul web site: https://www.consul.io/intro/getting-started/kv.html https://www.consul.io/docs/agent/http/kv.html","title":"Configuration Management"},{"location":"api/core/Ch-APICoreServiceConfiguration/#service-registry","text":"For the current API documentation, please refer to the official Consul web site: https://www.consul.io/intro/getting-started/services.html https://www.consul.io/docs/agent/http/catalog.html https://www.consul.io/docs/agent/http/agent.html https://www.consul.io/docs/agent/checks.html https://www.consul.io/docs/agent/http/health.html Service Registration While each microservice is starting up, it should connect to Consul to register its endpoint information, including microservice ID, address, port number, and health checking method. After that, other microservices can locate its URL from Consul, and Consul has the ability to monitor its health status. The RESTful API of registration is described on the following Consul page: https://www.consul.io/docs/agent/http/agent.html#agent_service_register Service Deregistration Before microservices shut down, they have to deregister themselves from Consul. The RESTful API of deregistration is described on the following Consul page: https://www.consul.io/docs/agent/http/agent.html#agent_service_deregister Service Discovery Service Discovery feature allows client micro services to query the endpoint information of a particular microservice by its microservice IDor list all available services registered in Consul. The RESTful API of querying service by microservice IDis described on the following Consul page: https://www.consul.io/docs/agent/http/catalog.html#catalog_service The RESTful API of listing all available services is described on the following Consul page: https://www.consul.io/docs/agent/http/agent.html#agent_services Health Checking Health checking is a critical feature that prevents using services that are unhealthy. Consul provides a variety of methods to check the health of services, including Script + Interval, HTTP + Interval, TCP + Interval, Time to Live (TTL), and Docker + Interval. The detailed introduction and examples of each checking methods are described on the following Consul page: https://www.consul.io/docs/agent/checks.html The health checks should be established during service registration. Please see the paragraph on this page of Service Registration section.","title":"Service Registry"},{"location":"api/supporting/Ch-APISupportingServicesAlerts/","text":"Alerts & Notifications Architecture Reference For a description of the architecture, see Alerts and Notifications Introduction When a person or a system needs to be informed of something discovered on the node by another microservice on the node, EdgeX Foundry's Alerts and Notifications microservice delivers that information. Examples of Alerts and Notifications that other services might need to broadcast include sensor data detected outside of certain parameters, usually detected by a Rules Engine service, or a system or service malfunction, usually detected by system management services. https://github.com/edgexfoundry/support-notifications/blob/master/raml/support-notifications.raml Alerts and Notifications V1 API Swagger Documentation","title":"Alerts & Notifications"},{"location":"api/supporting/Ch-APISupportingServicesAlerts/#alerts-notifications","text":"","title":"Alerts &amp; Notifications"},{"location":"api/supporting/Ch-APISupportingServicesAlerts/#architecture-reference","text":"For a description of the architecture, see Alerts and Notifications","title":"Architecture Reference"},{"location":"api/supporting/Ch-APISupportingServicesAlerts/#introduction","text":"When a person or a system needs to be informed of something discovered on the node by another microservice on the node, EdgeX Foundry's Alerts and Notifications microservice delivers that information. Examples of Alerts and Notifications that other services might need to broadcast include sensor data detected outside of certain parameters, usually detected by a Rules Engine service, or a system or service malfunction, usually detected by system management services. https://github.com/edgexfoundry/support-notifications/blob/master/raml/support-notifications.raml Alerts and Notifications V1 API Swagger Documentation","title":"Introduction"},{"location":"api/supporting/Ch-APISupportingServicesLogging/","text":"Logging Architecture Reference For a description of the architecture, see Logging Introduction Logging Service is a microservice featuring REST API for other microservices to add, query, and delete logging requests. Two options of persistence--file or mongodb--are supported and configurable through the property file or remote consul configuration service. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/support-logging.raml Logging V1 API Swagger Documentation","title":"Logging"},{"location":"api/supporting/Ch-APISupportingServicesLogging/#logging","text":"","title":"Logging"},{"location":"api/supporting/Ch-APISupportingServicesLogging/#architecture-reference","text":"For a description of the architecture, see Logging","title":"Architecture Reference"},{"location":"api/supporting/Ch-APISupportingServicesLogging/#introduction","text":"Logging Service is a microservice featuring REST API for other microservices to add, query, and delete logging requests. Two options of persistence--file or mongodb--are supported and configurable through the property file or remote consul configuration service. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/support-logging.raml Logging V1 API Swagger Documentation","title":"Introduction"},{"location":"api/supporting/Ch-APISupportingServicesRulesEngine/","text":"Rules Engine Architecture Reference For a description of the architecture, see Rules Engine Introduction EdgeX Foundry Rules Engine Microservice receives data from the Export Service through 0MQ, and then triggers actuation based on event data it receives and analyzes. Built on Drools technology. https://github.com/edgexfoundry/support-rulesengine/blob/master/raml/support-rulesengine.raml","title":"Rules Engine"},{"location":"api/supporting/Ch-APISupportingServicesRulesEngine/#rules-engine","text":"","title":"Rules Engine"},{"location":"api/supporting/Ch-APISupportingServicesRulesEngine/#architecture-reference","text":"For a description of the architecture, see Rules Engine","title":"Architecture Reference"},{"location":"api/supporting/Ch-APISupportingServicesRulesEngine/#introduction","text":"EdgeX Foundry Rules Engine Microservice receives data from the Export Service through 0MQ, and then triggers actuation based on event data it receives and analyzes. Built on Drools technology. https://github.com/edgexfoundry/support-rulesengine/blob/master/raml/support-rulesengine.raml","title":"Introduction"},{"location":"api/supporting/Ch-APISupportingServicesScheduling/","text":"Scheduling Architecture Reference For a description of the architecture, see Scheduling Introduction The following API RESTful Web Service(s) for EdgeX Foundry. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/support-scheduling.raml Scheduling V1 API Swagger Documentation Description Scheduler Service - a service that can be used to schedule invocation of a URL. Requires the use of interval(s), and interval action(s). Interval(s) : - name - unique name of the service. - start - identifies when the operation starts. Expressed in ISO 8601 YYYYMMDD't'HHmmss format. Empty means now. - end - identifies when the operation ends. Expressed in ISO 8601 YYYYMMDD't'HHmmss format. Empty means never - frequency - identifies the interval between invocations. Expressed in ISO 8601 PxYxMxDTxHxMxS format. Empty means no frequency. Interval Action(s) : - name - unique name of the interval action. - interval - unique name of an existing interval. - target - the recipient of the interval action (ergo service or name). - protocol - the protocol type to be used to contact the target. (example HTTP). - httpMethod - HTTP protocol verb. - address - the endpoint server host. - port - the desired port. - path - the api path which will be acted on. - parameters - (optional) parameters which will be included in the BODY tag for HttpMethods. Any parameters that should be provided to the call, e.g. {\"milliseconds\":86400000} Examples Create an interval upon which the scheduler will operate : curl -X POST -H \"Content-Type: application/json\" -H \"Cache-Control: no-cache\" -d '{ : \"name\": \"midnight\", \"start\": \"20180101T000000\", \"frequency\": \"P1D\"}' \" http://localhost:48081/api/v1/interval \" Example of a second interval which will run every 20 seconds : curl -X POST -H \"Content-Type: application/json\" -H \"Cache-Control: no-cache\" -d '{ : \"name\": \"every20s\", \"start\":\"20000101T000000\", \"end\":\"\", \"frequency\":\"PT20S\"}' \" http://localhost:48081/api/v1/interval \" Create an interval action that will invoke the interval action (drive the scrubber) in core-data : curl -X POST -H \"Content-Type: application/json\" -H \"Cache-Control: no-cache\" -d '{ : \"name\": \"scrub-pushed-events\", \"interval\": \"midnight\", \"target\": \"core-data\", \"protocol\": \"http\", \"httpMethod\": \"DELETE\", \"address\": \"localhost\", \"port\": 48080, \"path\": \"/api/v1/event/scrub\"}' \" http://localhost:48085/api/v1/intervalaction \" This is a Random-Boolean-Device which created by edgex-device-virtual that connects every 20 seconds. : curl -X POST -H \"Content-Type: application/json\" -d '{ : \"name\": \"put-action\", \"interval\": \"every20s\", \"target\": \"edgex-device-modbus\", \"protocol\": \"http\", \"httpMethod\": \"PUT\", \"address\": \"localhost\", \"port\": 49990, \"path\":\"/api/v1/device/name/Random-Boolean-Device/RandomValue_Bool\", \"parameters\": \"{\"RandomValue_Bool\": \"true\",\"EnableRandomization_Bool\": \"true\"}\" }' \" http://localhost:48085/api/v1/intervalaction \"","title":"Scheduling"},{"location":"api/supporting/Ch-APISupportingServicesScheduling/#scheduling","text":"","title":"Scheduling"},{"location":"api/supporting/Ch-APISupportingServicesScheduling/#architecture-reference","text":"For a description of the architecture, see Scheduling","title":"Architecture Reference"},{"location":"api/supporting/Ch-APISupportingServicesScheduling/#introduction","text":"The following API RESTful Web Service(s) for EdgeX Foundry. https://github.com/edgexfoundry/edgex-go/blob/master/api/raml/support-scheduling.raml Scheduling V1 API Swagger Documentation","title":"Introduction"},{"location":"api/supporting/Ch-APISupportingServicesScheduling/#description","text":"Scheduler Service - a service that can be used to schedule invocation of a URL. Requires the use of interval(s), and interval action(s). Interval(s) : - name - unique name of the service. - start - identifies when the operation starts. Expressed in ISO 8601 YYYYMMDD't'HHmmss format. Empty means now. - end - identifies when the operation ends. Expressed in ISO 8601 YYYYMMDD't'HHmmss format. Empty means never - frequency - identifies the interval between invocations. Expressed in ISO 8601 PxYxMxDTxHxMxS format. Empty means no frequency. Interval Action(s) : - name - unique name of the interval action. - interval - unique name of an existing interval. - target - the recipient of the interval action (ergo service or name). - protocol - the protocol type to be used to contact the target. (example HTTP). - httpMethod - HTTP protocol verb. - address - the endpoint server host. - port - the desired port. - path - the api path which will be acted on. - parameters - (optional) parameters which will be included in the BODY tag for HttpMethods. Any parameters that should be provided to the call, e.g. {\"milliseconds\":86400000}","title":"Description"},{"location":"api/supporting/Ch-APISupportingServicesScheduling/#examples","text":"Create an interval upon which the scheduler will operate : curl -X POST -H \"Content-Type: application/json\" -H \"Cache-Control: no-cache\" -d '{ : \"name\": \"midnight\", \"start\": \"20180101T000000\", \"frequency\": \"P1D\"}' \" http://localhost:48081/api/v1/interval \" Example of a second interval which will run every 20 seconds : curl -X POST -H \"Content-Type: application/json\" -H \"Cache-Control: no-cache\" -d '{ : \"name\": \"every20s\", \"start\":\"20000101T000000\", \"end\":\"\", \"frequency\":\"PT20S\"}' \" http://localhost:48081/api/v1/interval \" Create an interval action that will invoke the interval action (drive the scrubber) in core-data : curl -X POST -H \"Content-Type: application/json\" -H \"Cache-Control: no-cache\" -d '{ : \"name\": \"scrub-pushed-events\", \"interval\": \"midnight\", \"target\": \"core-data\", \"protocol\": \"http\", \"httpMethod\": \"DELETE\", \"address\": \"localhost\", \"port\": 48080, \"path\": \"/api/v1/event/scrub\"}' \" http://localhost:48085/api/v1/intervalaction \" This is a Random-Boolean-Device which created by edgex-device-virtual that connects every 20 seconds. : curl -X POST -H \"Content-Type: application/json\" -d '{ : \"name\": \"put-action\", \"interval\": \"every20s\", \"target\": \"edgex-device-modbus\", \"protocol\": \"http\", \"httpMethod\": \"PUT\", \"address\": \"localhost\", \"port\": 49990, \"path\":\"/api/v1/device/name/Random-Boolean-Device/RandomValue_Bool\", \"parameters\": \"{\"RandomValue_Bool\": \"true\",\"EnableRandomization_Bool\": \"true\"}\" }' \" http://localhost:48085/api/v1/intervalaction \"","title":"Examples"},{"location":"design/","text":"Architecture Decision Records Folder This folder contains EdgeX Foundry decision records (ADR) and legacy design / requirement documents. /design /adr (architecture decision Records) /legacy-design (legacy design documents) /legacy-requirements (legacy requirement documents) At the root of the ADR folder (/design/adr) are decisions that are relevant to multiple parts of the project (aka \ufffd cross cutting concerns ). Sub folders under the ADR folder contain decisions relevant to the specific area of the project and essentially set up along working group lines (security, core, application, etc.). Naming and Formatting ADR documents are requested to follow RFC (request for comments) naming standard. Specifically, authors should name their documents with a sequentially increasing integer (or serial number) and then the architectural design topic: (sequence number - topic). Example: 0001-SeparateConfigurationInterface. The sequence is a global sequence for all EdgeX ADR. Per RFC and Michael Nygard suggestions the makeup of the ADR document should generally include: Title Status (proposed, accepted, rejected, deprecated, superseded, etc.) Context and Proposed Design Decision Consequences/considerations References Document history is maintained via Github history. Ownership EdgeX WG chairman own the sub folder and included documents associated to their work group. The EdgeX TSC chair/vice chair are responsible for the root level, cross cutting concern documents. Review and Approval ADR\u2019s shall be submitted as PRs to the appropriate edgex-docs folder based on the Architecture Decision Records Folder section above. The status of the PR (inside the document) shall be listed as proposed during this period. The PRs shall be left open (not merged) so that comments against the PR can be collected during the proposal period. The PRs can be approved and merged only after a formal vote of approval is conducted by the TSC. On approval of the ADR by the TSC, the status of the ADR should be changed to accepted . If the ADR is not approved by the TSC, the status in the document should be changed to rejected and the PR closed. Legacy A separate folder (/design/legacy-design) is used for legacy design/architecture decisions. A separate folder (/design/legacy-requirements) is used for legacy requirements documents. WG chairman take the responsibility for posting legacy material in to the applicable folders. Table of Contents A README with a table of contents for current documents is located here . Legacy Design and Requirements have their own Table of Contents as well and are located in their respective directories at /legacy-design and /legacy-requirements . Document authors are asked to keep the TOC updated with each new document entry.","title":"Architecture Decision Records Folder"},{"location":"design/#architecture-decision-records-folder","text":"This folder contains EdgeX Foundry decision records (ADR) and legacy design / requirement documents. /design /adr (architecture decision Records) /legacy-design (legacy design documents) /legacy-requirements (legacy requirement documents) At the root of the ADR folder (/design/adr) are decisions that are relevant to multiple parts of the project (aka \ufffd cross cutting concerns ). Sub folders under the ADR folder contain decisions relevant to the specific area of the project and essentially set up along working group lines (security, core, application, etc.).","title":"Architecture Decision Records Folder"},{"location":"design/#naming-and-formatting","text":"ADR documents are requested to follow RFC (request for comments) naming standard. Specifically, authors should name their documents with a sequentially increasing integer (or serial number) and then the architectural design topic: (sequence number - topic). Example: 0001-SeparateConfigurationInterface. The sequence is a global sequence for all EdgeX ADR. Per RFC and Michael Nygard suggestions the makeup of the ADR document should generally include: Title Status (proposed, accepted, rejected, deprecated, superseded, etc.) Context and Proposed Design Decision Consequences/considerations References Document history is maintained via Github history.","title":"Naming and Formatting"},{"location":"design/#ownership","text":"EdgeX WG chairman own the sub folder and included documents associated to their work group. The EdgeX TSC chair/vice chair are responsible for the root level, cross cutting concern documents.","title":"Ownership"},{"location":"design/#review-and-approval","text":"ADR\u2019s shall be submitted as PRs to the appropriate edgex-docs folder based on the Architecture Decision Records Folder section above. The status of the PR (inside the document) shall be listed as proposed during this period. The PRs shall be left open (not merged) so that comments against the PR can be collected during the proposal period. The PRs can be approved and merged only after a formal vote of approval is conducted by the TSC. On approval of the ADR by the TSC, the status of the ADR should be changed to accepted . If the ADR is not approved by the TSC, the status in the document should be changed to rejected and the PR closed.","title":"Review and Approval"},{"location":"design/#legacy","text":"A separate folder (/design/legacy-design) is used for legacy design/architecture decisions. A separate folder (/design/legacy-requirements) is used for legacy requirements documents. WG chairman take the responsibility for posting legacy material in to the applicable folders.","title":"Legacy"},{"location":"design/#table-of-contents","text":"A README with a table of contents for current documents is located here . Legacy Design and Requirements have their own Table of Contents as well and are located in their respective directories at /legacy-design and /legacy-requirements . Document authors are asked to keep the TOC updated with each new document entry.","title":"Table of Contents"},{"location":"design/TOC/","text":"ADR Table of Contents Name/Link Short Description 0001 Registry Refactor Separate out Registry and Configuration APIs 0002 Array Datatypes Allow Arrays to be held in Readings 0003 V2 API Principles Principles and Goals of V2 API Design 0004 Feature Flags Feature Flag Implementation 0005 Service Self Config Init Service Self Config Init & Config Seed Removal 0007 Release Automation Overview of Release Automation Flow for EdgeX 0011 Device Service REST API The REST API for Device Services in EdgeX v2.x","title":"ADR Table of Contents"},{"location":"design/TOC/#adr-table-of-contents","text":"Name/Link Short Description 0001 Registry Refactor Separate out Registry and Configuration APIs 0002 Array Datatypes Allow Arrays to be held in Readings 0003 V2 API Principles Principles and Goals of V2 API Design 0004 Feature Flags Feature Flag Implementation 0005 Service Self Config Init Service Self Config Init & Config Seed Removal 0007 Release Automation Overview of Release Automation Flow for EdgeX 0011 Device Service REST API The REST API for Device Services in EdgeX v2.x","title":"ADR Table of Contents"},{"location":"design/adr/0001-Registy-Refactor/","text":"Registry Refactoring Design Status Context Proposed Design Decision Consequences References Status Approved Context Currently the Registry Client in go-mod-registry module provides Service Configuration and Service Registration functionality. The goal of this design is to refactor the go-mod-registry module for separation of concerns. The Service Registry functionality will stay in the go-mod-registry module and the Service Configuration functionality will be separated out into a new go-mod-configuration module. This allows for implementations for deferent providers for each, another aspect of separation of concerns. Proposed Design Provider Connection information An aspect of using the current Registry Client is \" Where do the services get the Registry Provider connection information? \" Currently all services either pull this connection information from the local configuration file or from the edgex_registry environment variable. Device Services also have the option to specify this connection information on the command line. With the refactoring for separation of concerns, this issue changes to \" Where do the services get the Configuration Provider connection information? \" There have been concerns voiced by some in the EdgeX community that storing this Configuration Provider connection information in the configuration which ultimately is provided by that provider is not the right design. This design proposes that all services will use the command line option approach with the ability to override with an environment variable. The Configuration Provider information will not be stored in each service's local configuration file. The edgex_registry environment variable will be deprecated. The Registry Provider connection information will continue to be stored in each service's configuration either locally or from the Configuration Provider same as all other EdgeX Client and Database connection information. Command line option changes The new -cp/-configProvider command line option will be added to each service which will have a value specified using the format {type}.{protocol}://{host}:{port} e.g consul.http://localhost:8500 . This new command line option will be overridden by the edgex_configuration_provider environment variable when it is set. This environment variable's value has the same format as the command line option value. If no value is provided to the -cp/-configProvider option, i.e. just -cp , and no environment variable override is specified, the default value of consul.http://localhost:8500 will be used. if -cp/-configProvider not used and no environment variable override is specified the local configuration file is used, as is it now. All services will log the Configuration Provider connection information that is used. The existing -r/-registry command line option will be retained as a Boolean flag to indicate to use the Registry. Bootstrap Changes All services in the edgex-go mono repo use the new common bootstrap functionality. The plan is to move this code to a go module for the Device Service and App Functions SDKs to also use. The current bootstrap modules pkg/bootstrap/configuration/registry.go and pkg/bootstrap/container/registry.go will be refactored to use the new Configuration Client and be renamed appropriately. New bootstrap modules will be created for using the revised version of Registry Client . The current use of useRegistry and registryClient for service configuration will be change to appropriate names for using the new Configuration Client . The current use of useRegistry and registryClient for service registration will be retained for service registration. Call to the new Unregister() API will be added to shutdown code for all services. Config-Seed Changes The conf-seed service will have similar changes for specifying the Configuration Provider connection information since it doesn't use the common bootstrap package. Beyond that it will have minor changes for switching to using the Configuration Client interface, which will just be imports and appropriate name refactoring. Config Endpoint Changes Since the Configuration Provider connection information will no longer be in the service's configuration struct, the config endpoint processing will be modified to add the Configuration Provider connection information to the resulting JSON create from service's configuration. Client Interfaces changes Current Registry Client This following is the current Registry Client Interface type Client interface { Register () error HasConfiguration () ( bool , error ) PutConfigurationToml ( configuration * toml . Tree , overwrite bool ) error PutConfiguration ( configStruct interface {}, overwrite bool ) error GetConfiguration ( configStruct interface {}) ( interface {}, error ) WatchForChanges ( updateChannel chan <- interface {}, errorChannel chan <- error , configuration interface {}, waitKey string ) IsAlive () bool ConfigurationValueExists ( name string ) ( bool , error ) GetConfigurationValue ( name string ) ([] byte , error ) PutConfigurationValue ( name string , value [] byte ) error GetServiceEndpoint ( serviceId string ) ( types . ServiceEndpoint , error ) IsServiceAvailable ( serviceId string ) error } New Configuration Client This following is the new Configuration Client Interface which contains the Service Configuration specific portion from the above current Registry Client . type Client interface { HasConfiguration () ( bool , error ) PutConfigurationFromToml ( configuration * toml . Tree , overwrite bool ) error PutConfiguration ( configStruct interface {}, overwrite bool ) error GetConfiguration ( configStruct interface {}) ( interface {}, error ) WatchForChanges ( updateChannel chan <- interface {}, errorChannel chan <- error , configuration interface {}, waitKey string ) IsAlive () bool ConfigurationValueExists ( name string ) ( bool , error ) GetConfigurationValue ( name string ) ([] byte , error ) PutConfigurationValue ( name string , value [] byte ) error } Revised Registry Client This following is the revised Registry Client Interface, which contains the Service Registry specific portion from the above current Registry Client . The UnRegister() API has been added per issue #20 type Client interface { Register () error UnRegister () error IsAlive () bool GetServiceEndpoint ( serviceId string ) ( types . ServiceEndpoint , error ) IsServiceAvailable ( serviceId string ) error } Client Configuration Structs Current Registry Client Config The following is the current struct used to configure the current Registry Client type Config struct { Protocol string Host string Port int Type string Stem string ServiceKey string ServiceHost string ServicePort int ServiceProtocol string CheckRoute string CheckInterval string } New Configuration Client Config The following is the new struct the will be used to configure the new Configuration Client from the command line option or environment variable values. The Service Registry portion has been removed from the above existing Registry Client Config type Config struct { Protocol string Host string Port int Type string BasePath string ServiceKey string } New Registry Client Config The following is the revised struct the will be used to configure the new Registry Client from the information in the service's configuration. This is mostly unchanged from the existing Registry Client Config , except that the Stem for configuration has been removed type Config struct { Protocol string Host string Port int Type string ServiceKey string ServiceHost string ServicePort int ServiceProtocol string CheckRoute string CheckInterval string } Provider Implementations The current Consul implementation of the Registry Client will be split up into implementations for the new Configuration Client in the new go-mod-configuration module and the revised Registry Client in the existing go-mod-registry module. Decision It was decided to move forward with the above design After initial ADR was approved, it was decided to retain the -r/--registry command-line flag and not add the Enabled field in the Registry provider configuration. Consequences Once the refactoring of go-mod-registry and go-mod-configuration are complete, they will need to be integrated into the new go-mod-bootstrap. Part of this integration will be the Command line option changes above. At this point the edgex-go services will be integrated with the new Registry and Configuration providers. The App Services SDK and Device Services SDK will then need to integrate go-mod-bootstrap to take advantage of these new providers. References Registry Abstraction - Decouple EdgeX services from Consul (Previous design)","title":"Registry Refactoring Design"},{"location":"design/adr/0001-Registy-Refactor/#registry-refactoring-design","text":"Status Context Proposed Design Decision Consequences References","title":"Registry Refactoring Design"},{"location":"design/adr/0001-Registy-Refactor/#status","text":"Approved","title":"Status"},{"location":"design/adr/0001-Registy-Refactor/#context","text":"Currently the Registry Client in go-mod-registry module provides Service Configuration and Service Registration functionality. The goal of this design is to refactor the go-mod-registry module for separation of concerns. The Service Registry functionality will stay in the go-mod-registry module and the Service Configuration functionality will be separated out into a new go-mod-configuration module. This allows for implementations for deferent providers for each, another aspect of separation of concerns.","title":"Context"},{"location":"design/adr/0001-Registy-Refactor/#proposed-design","text":"","title":"Proposed Design"},{"location":"design/adr/0001-Registy-Refactor/#provider-connection-information","text":"An aspect of using the current Registry Client is \" Where do the services get the Registry Provider connection information? \" Currently all services either pull this connection information from the local configuration file or from the edgex_registry environment variable. Device Services also have the option to specify this connection information on the command line. With the refactoring for separation of concerns, this issue changes to \" Where do the services get the Configuration Provider connection information? \" There have been concerns voiced by some in the EdgeX community that storing this Configuration Provider connection information in the configuration which ultimately is provided by that provider is not the right design. This design proposes that all services will use the command line option approach with the ability to override with an environment variable. The Configuration Provider information will not be stored in each service's local configuration file. The edgex_registry environment variable will be deprecated. The Registry Provider connection information will continue to be stored in each service's configuration either locally or from the Configuration Provider same as all other EdgeX Client and Database connection information.","title":"Provider Connection information"},{"location":"design/adr/0001-Registy-Refactor/#command-line-option-changes","text":"The new -cp/-configProvider command line option will be added to each service which will have a value specified using the format {type}.{protocol}://{host}:{port} e.g consul.http://localhost:8500 . This new command line option will be overridden by the edgex_configuration_provider environment variable when it is set. This environment variable's value has the same format as the command line option value. If no value is provided to the -cp/-configProvider option, i.e. just -cp , and no environment variable override is specified, the default value of consul.http://localhost:8500 will be used. if -cp/-configProvider not used and no environment variable override is specified the local configuration file is used, as is it now. All services will log the Configuration Provider connection information that is used. The existing -r/-registry command line option will be retained as a Boolean flag to indicate to use the Registry.","title":"Command line option changes"},{"location":"design/adr/0001-Registy-Refactor/#bootstrap-changes","text":"All services in the edgex-go mono repo use the new common bootstrap functionality. The plan is to move this code to a go module for the Device Service and App Functions SDKs to also use. The current bootstrap modules pkg/bootstrap/configuration/registry.go and pkg/bootstrap/container/registry.go will be refactored to use the new Configuration Client and be renamed appropriately. New bootstrap modules will be created for using the revised version of Registry Client . The current use of useRegistry and registryClient for service configuration will be change to appropriate names for using the new Configuration Client . The current use of useRegistry and registryClient for service registration will be retained for service registration. Call to the new Unregister() API will be added to shutdown code for all services.","title":"Bootstrap Changes"},{"location":"design/adr/0001-Registy-Refactor/#config-seed-changes","text":"The conf-seed service will have similar changes for specifying the Configuration Provider connection information since it doesn't use the common bootstrap package. Beyond that it will have minor changes for switching to using the Configuration Client interface, which will just be imports and appropriate name refactoring.","title":"Config-Seed Changes"},{"location":"design/adr/0001-Registy-Refactor/#config-endpoint-changes","text":"Since the Configuration Provider connection information will no longer be in the service's configuration struct, the config endpoint processing will be modified to add the Configuration Provider connection information to the resulting JSON create from service's configuration.","title":"Config Endpoint Changes"},{"location":"design/adr/0001-Registy-Refactor/#client-interfaces-changes","text":"","title":"Client Interfaces changes"},{"location":"design/adr/0001-Registy-Refactor/#current-registry-client","text":"This following is the current Registry Client Interface type Client interface { Register () error HasConfiguration () ( bool , error ) PutConfigurationToml ( configuration * toml . Tree , overwrite bool ) error PutConfiguration ( configStruct interface {}, overwrite bool ) error GetConfiguration ( configStruct interface {}) ( interface {}, error ) WatchForChanges ( updateChannel chan <- interface {}, errorChannel chan <- error , configuration interface {}, waitKey string ) IsAlive () bool ConfigurationValueExists ( name string ) ( bool , error ) GetConfigurationValue ( name string ) ([] byte , error ) PutConfigurationValue ( name string , value [] byte ) error GetServiceEndpoint ( serviceId string ) ( types . ServiceEndpoint , error ) IsServiceAvailable ( serviceId string ) error }","title":"Current Registry Client"},{"location":"design/adr/0001-Registy-Refactor/#new-configuration-client","text":"This following is the new Configuration Client Interface which contains the Service Configuration specific portion from the above current Registry Client . type Client interface { HasConfiguration () ( bool , error ) PutConfigurationFromToml ( configuration * toml . Tree , overwrite bool ) error PutConfiguration ( configStruct interface {}, overwrite bool ) error GetConfiguration ( configStruct interface {}) ( interface {}, error ) WatchForChanges ( updateChannel chan <- interface {}, errorChannel chan <- error , configuration interface {}, waitKey string ) IsAlive () bool ConfigurationValueExists ( name string ) ( bool , error ) GetConfigurationValue ( name string ) ([] byte , error ) PutConfigurationValue ( name string , value [] byte ) error }","title":"New Configuration Client"},{"location":"design/adr/0001-Registy-Refactor/#revised-registry-client","text":"This following is the revised Registry Client Interface, which contains the Service Registry specific portion from the above current Registry Client . The UnRegister() API has been added per issue #20 type Client interface { Register () error UnRegister () error IsAlive () bool GetServiceEndpoint ( serviceId string ) ( types . ServiceEndpoint , error ) IsServiceAvailable ( serviceId string ) error }","title":"Revised Registry Client"},{"location":"design/adr/0001-Registy-Refactor/#client-configuration-structs","text":"","title":"Client Configuration Structs"},{"location":"design/adr/0001-Registy-Refactor/#current-registry-client-config","text":"The following is the current struct used to configure the current Registry Client type Config struct { Protocol string Host string Port int Type string Stem string ServiceKey string ServiceHost string ServicePort int ServiceProtocol string CheckRoute string CheckInterval string }","title":"Current Registry Client Config"},{"location":"design/adr/0001-Registy-Refactor/#new-configuration-client-config","text":"The following is the new struct the will be used to configure the new Configuration Client from the command line option or environment variable values. The Service Registry portion has been removed from the above existing Registry Client Config type Config struct { Protocol string Host string Port int Type string BasePath string ServiceKey string }","title":"New Configuration Client Config"},{"location":"design/adr/0001-Registy-Refactor/#new-registry-client-config","text":"The following is the revised struct the will be used to configure the new Registry Client from the information in the service's configuration. This is mostly unchanged from the existing Registry Client Config , except that the Stem for configuration has been removed type Config struct { Protocol string Host string Port int Type string ServiceKey string ServiceHost string ServicePort int ServiceProtocol string CheckRoute string CheckInterval string }","title":"New Registry Client Config"},{"location":"design/adr/0001-Registy-Refactor/#provider-implementations","text":"The current Consul implementation of the Registry Client will be split up into implementations for the new Configuration Client in the new go-mod-configuration module and the revised Registry Client in the existing go-mod-registry module.","title":"Provider Implementations"},{"location":"design/adr/0001-Registy-Refactor/#decision","text":"It was decided to move forward with the above design After initial ADR was approved, it was decided to retain the -r/--registry command-line flag and not add the Enabled field in the Registry provider configuration.","title":"Decision"},{"location":"design/adr/0001-Registy-Refactor/#consequences","text":"Once the refactoring of go-mod-registry and go-mod-configuration are complete, they will need to be integrated into the new go-mod-bootstrap. Part of this integration will be the Command line option changes above. At this point the edgex-go services will be integrated with the new Registry and Configuration providers. The App Services SDK and Device Services SDK will then need to integrate go-mod-bootstrap to take advantage of these new providers.","title":"Consequences"},{"location":"design/adr/0001-Registy-Refactor/#references","text":"Registry Abstraction - Decouple EdgeX services from Consul (Previous design)","title":"References"},{"location":"design/adr/0004-Feature-Flags/","text":"Feature Flag Proposal Status Accepted Context Out of the proposal for releasing on time, the community suggested that we take a closer look at feature-flags. Feature-flags are typically intended for users of an application to turn on or off new or unused features. This gives user more control to adopt a feature-set at their own pace \u2013 i.e disabling store and forward in App Functions SDK without breaking backward compatibility. It can also be used to indicate to developers the features that are more often used than others and can provided valuable feedback to enhance and continue a given feature. To gain that insight of the use of any given feature, we would require not only instrumentation of the code but a central location in the cloud (i.e a TIG stack) for the telemetry to be ingested and in turn reported in order to provide the feedback to the developers. This becomes infeasible primarily because the cloud infrastructure costs, privacy concerns, and other unforeseen legal reasons for sending \u201cUsage Metrics\u201d of an EdgeX installation back to a central entity such as the Linux Foundation, among many others. Without the valuable feedback loop, feature-flags don\u2019t provide much value on their own and they certainly don\u2019t assist in increasing velocity to help us deliver on time. Putting aside one of the major value propositions listed above, feasibility of a feature flag \u201cmodule\u201d was still evaluated. The simplest approach would be to leverage configuration following a certain format such as FF_[NewFeatureName]=true/false. This is similar to what is done today. Turning on/off security is an example, turning on/off the registry is another. Expanding this further with a module could offer standardization of controlling a given feature such as featurepkg.Register(\u201cMyNewFeature\u201d) or featurepkg.IsOn(\u201cMyNewFeature\u201d) . However, this really is just adding complexity on top of the underlying configuration that is already implemented. If we were to consider doing something like this, it lends it self to a central management of features within the EdgeX framework\u2014either its own service or possibly added as part of the SMA. This could help address concerns around feature dependencies and compatibility. Feature A on Service X requires Feature B and Feature C on Service Y. Continuing down this path starts to beget a fairly large impact to EdgeX for value that cannot be fully realized. Decision The community should NOT pursue a full-fledged feature flag implementation either homegrown or off-the-shelf. However, it should be encouraged to develop features with a wholistic perspective and consider leveraging configuration options to turn them on/off. In other words, once a feature compiles, can work under common scenarios, but perhaps isn\u2019t fully tested with edge cases, but doesn\u2019t impact any other functionality, should be encouraged. Consequences Allows more focus on the many more competing priorities for this release. Minimal impact to development cycles and release schedule","title":"Feature Flag Proposal"},{"location":"design/adr/0004-Feature-Flags/#feature-flag-proposal","text":"","title":"Feature Flag Proposal"},{"location":"design/adr/0004-Feature-Flags/#status","text":"Accepted","title":"Status"},{"location":"design/adr/0004-Feature-Flags/#context","text":"Out of the proposal for releasing on time, the community suggested that we take a closer look at feature-flags. Feature-flags are typically intended for users of an application to turn on or off new or unused features. This gives user more control to adopt a feature-set at their own pace \u2013 i.e disabling store and forward in App Functions SDK without breaking backward compatibility. It can also be used to indicate to developers the features that are more often used than others and can provided valuable feedback to enhance and continue a given feature. To gain that insight of the use of any given feature, we would require not only instrumentation of the code but a central location in the cloud (i.e a TIG stack) for the telemetry to be ingested and in turn reported in order to provide the feedback to the developers. This becomes infeasible primarily because the cloud infrastructure costs, privacy concerns, and other unforeseen legal reasons for sending \u201cUsage Metrics\u201d of an EdgeX installation back to a central entity such as the Linux Foundation, among many others. Without the valuable feedback loop, feature-flags don\u2019t provide much value on their own and they certainly don\u2019t assist in increasing velocity to help us deliver on time. Putting aside one of the major value propositions listed above, feasibility of a feature flag \u201cmodule\u201d was still evaluated. The simplest approach would be to leverage configuration following a certain format such as FF_[NewFeatureName]=true/false. This is similar to what is done today. Turning on/off security is an example, turning on/off the registry is another. Expanding this further with a module could offer standardization of controlling a given feature such as featurepkg.Register(\u201cMyNewFeature\u201d) or featurepkg.IsOn(\u201cMyNewFeature\u201d) . However, this really is just adding complexity on top of the underlying configuration that is already implemented. If we were to consider doing something like this, it lends it self to a central management of features within the EdgeX framework\u2014either its own service or possibly added as part of the SMA. This could help address concerns around feature dependencies and compatibility. Feature A on Service X requires Feature B and Feature C on Service Y. Continuing down this path starts to beget a fairly large impact to EdgeX for value that cannot be fully realized.","title":"Context"},{"location":"design/adr/0004-Feature-Flags/#decision","text":"The community should NOT pursue a full-fledged feature flag implementation either homegrown or off-the-shelf. However, it should be encouraged to develop features with a wholistic perspective and consider leveraging configuration options to turn them on/off. In other words, once a feature compiles, can work under common scenarios, but perhaps isn\u2019t fully tested with edge cases, but doesn\u2019t impact any other functionality, should be encouraged.","title":"Decision"},{"location":"design/adr/0004-Feature-Flags/#consequences","text":"Allows more focus on the many more competing priorities for this release. Minimal impact to development cycles and release schedule","title":"Consequences"},{"location":"design/adr/0005-Service-Self-Config/","text":"Service Self Config Init & Config Seed Removal Status approved - TSC vote on 3/25/20 for Geneva release NOTE: this ADR does not address high availability considerations and concerns. EdgeX, in general, has a number of unanswered questions with regard to HA architecture and this design adds to those considerations. Context Since its debut, EdgeX has had a configuration seed service (config-seed) that, on start of EdgeX, deposits configuration for all the services into Consul (our configuration/registry service). For development purposes, or on resource constrained platforms, EdgeX can be run without Consul with services simply reading configuration from the filesystem. While this process has nominally worked for several releases of EdgeX, there has always been some issues with this extra initialization process (config-seed), not least of which are: - race conditions on the part of the services, as they bootstrap, coming up before the config-seed completes its deposit of configuration into Consul - how to deal with \"overrides\" such as environmental variable provided configuration overrides. As the override is often specific to a service but has to be in place for config-seed in order to take effect. - need for an additional service that is only there for init and then dies (confusing to users) NOTE - for historical purposes, it should be noted that config-seed only writes configuration into the configuration/registry service (Consul) once on the first start of EdgeX. On subsequent starts of EdgeX, config-seed checks to see if it has already populated the configuration/registry service and will not rewrite configuration again (unless the --overwrite flag is used). The design/architectural proposal, therefore, is: - removal of the config-seed service (removing cmd/config-seed from the edgex-go repository) - have each EdgeX micro service \"self seed\" - that is seed Consul with their own required configuration on bootstrap of the service. Details of that bootstrapping process are below. Command Line Options All EdgeX services support a common set of command-line options, some combination of which are required on startup for a service to interact with the rest of EdgeX. Command line options are not set by any configuration. Command line options include: --configProvider or -cp (the configuration provider location URL) --overwrite or -o (overwrite the configuration in the configuration provider) --file or -f (the configuration filename - configuration.toml is used by default if the configuration filename is not provided) --profile or -p (the name of a sub directory in the configuration directory in which a profile-specific configuration file is found. This has no default. If not specified, the configuration file is read from the configuration directory) --confdir or -c (the directory where the configuration file is found - ./res is used by default if the confdir is not specified, where \".\" is the convention on Linux/Unix/MacOS which means current directory) --registry or -r (string indicating use of the registry) The distinction of command line options versus configuration will be important later in this ADR. Two command line options (-o for overwrite and -r for registry) are not overridable by environmental variables. NOTES: Use of the --overwrite command line option should be used sparingly and with expert knowledge of EdgeX; in particular knowledge of how it operates and where/how it gets its configuration on restarts, etc. Ordinarily, --overwrite is provided as a means to support development needs. Use of --overwrite permanently in production enviroments is highly discouraged. Configuration Initialization Each service has (or shall have if not providing it already) a local configuration file. The service may use the local configuration file on initialization of the service (aka bootstrap of the service) depending on command line options and environmental variables (see below) provided at startup. Using a configuration provider When the configuration provider is specified, the service will call on the configuration provider (Consul) and check if the top-level (root) namespace for the service exists. If configuratation at the top-level (root) namespace exists, it indicates that the service has already populated its configuration into the configuration provider in a prior startup. If the service finds the top-level (root) namespace is already populated with configuration information it will then read that configuration information from the configuration provider under namespace for that service (and ignore what is in the local configuration file). If the service finds the top-level (root) namespace is not populated with configuration information, it will read its local configuration file and populate the configuration provider (under the namespace for the service) with configuration read from the local configuration file. A configuration provider can be specified with a command line argument (the -cp / --configProvider) or environment variable (the EDGEX_CONFIGURATION_PROVIDER environmental variable which overrides the command line argument). NOTE: the environmental variables are typically uppercase but there have been inconsistencies in environmental variable casing (example: edgex_registry). This should be considered and made consistent in a future major release. Using the local configuration file When a configuration provider isn't specified, the service just uses the configuration in its local configuration file. That is the service uses the configuration in the file associated with the profile, config filename and config file directory command line options or environmental variables. In this case, the service does not contact the configuration service (Consul) for any configuration information. NOTE: As the services now self seed and deployment specific changes can be made via environment overrides, it will no longer be necessary to have a Docker profile configuration file in each of the service directories (example: https://github.com/edgexfoundry/edgex-go/blob/master/cmd/core-data/res/docker/configuration.toml). See Consequences below. It will still be possible for users to use the profile mechanism to specify a Docker configuration, but it will no longer be required and not the recommended approach to providing Docker container specific configuration. Overrides Environment variables used to override configuration always take precedence whether configuration is being sourced locally or read from the config provider/Consul. Note - this means that a configuration value that is being overridden by an environment variable will always be the source of truth, even if the same configuration is changed directly in Consul. The name of the environmental variable must match the path names in Consul. NOTES: - Environmental variables overrides remove the need to change the \"docker\" profile in the res/docker/configuration.toml files - Allowing removal of 50% of the existing configuration.toml files. - The override rules in EdgeX between environmental variables and command line options may be counter intuitive compared to other systems. There appears to be no standard practice. Indeed, web searching \"Reddit & Starting Fights Env Variables vs Command Line Args\" will layout the prevailing differences. - Environment variables used for configuration overrides are named by prepending the the configuration element with the configuration section inclusive of sub-path, where sub-path's \".\"s are replaced with underscores. These configuration environment variable overrides must be specified using camel case. Here are two examples: Registry_Host for [Registry] Host = 'localhost' Clients_CoreData_Host for [Clients] [Clients.CoreData] Host = 'localhost' - Going forward, environmental variables that override command line options should be all uppercase. All values overriden get logged (indicating which configuration value or op param and the new value). Decision These features have been implemented (with some minor changes to be done) for consideration here: https://github.com/edgexfoundry/go-mod-bootstrap/compare/master...lenny-intel:SelfSeed2. This code branch will be removed once this ADR is approved and implemented on master. The implementation for self-seeding services and environmental overrides is already implemented (for Fuji) per this document in the application services and device services (and instituted in the SDKs of each). Backward compatibility Several aspects of this ADR contain backward compatibility issues for the device service and application service SDKs. Therefore, for the upcoming minor release, the following guidelines and expections are added to provide for backward compatibility. --registry= for Device SDKs As earlier versions of the device service SDKs accepted a URI for --registry, if specified on the command line, use the given URI as the address of the configuration provider. If both --configProvider and --registry specify URIs, then the service should log an error and exit. --registry (no \u2018=\u2019) and w/o --configProvider for both SDKs If a configProvider URI isn't specified, but --registry (w/out a URI) is specified, then the service will use the Registry provider information from its local configuration file for both configuration and registry providers. Env Var: edgex_registry= for all services (currently has been removed) Add it back and use value as if it was EDGEX_CONFIGURATION_PROVIDER and enable use of registry with same settings in URL. Default to http as it is in Fuji. Consequences Docker compose files will need to be changed to remove config seed. The main Snap will need to be changed to remove config seed. Config seed code (currently in edgex-go repo) is to be removed. Any service specific environmental overrides currently on config seed need to be moved to the specific service(s). The Docker configuration files and directory (example: https://github.com/edgexfoundry/edgex-go/blob/master/cmd/core-data/res/docker/configuration.toml) that are used to populate the config seed for Docker containers can be eliminated from all the services. In cmd/security-secretstore-setup, there is only a docker configuration.toml. This file will be moved rather than deleted. Documentation would need to reflect removal of config seed and \"self seeding\" process. Removes any potential issue with past race conditions (as experienced with the Edinburgh release) as each service is now responsible for its own configuration. There are still high availability concerns that need to be considered and not covered in this ADR at this time. Removes some confusion on the part of users as to why a service (config-seed) starts and immediately exits. Minimal impact to development cycles and release schedule Configuration endpoints in all services need to ensure the environmental variables are reflected in the configuration data returned (this is a system management impact). Docker files will need to be modified to remove setting profile=docker Docker compose files will need to be changed to add environmental overrides for removal of docker profiles. These should go in the global environment section of the compose files for those overrides that apply to all services. Example: # all common shared environment variables defined here: x-common-env-variables: &common-variables EDGEX_SECURITY_SECRET_STORE: \"false\" EDGEX_CONFIGURATION_PROVIDER: consul.http://edgex-core-consul:8500 Clients_CoreData_Host: edgex-core-data Clients_Logging_Host: edgex-support-logging Logging_EnableRemote: \"true\"","title":"Service Self Config Init & Config Seed Removal"},{"location":"design/adr/0005-Service-Self-Config/#service-self-config-init-config-seed-removal","text":"","title":"Service Self Config Init &amp; Config Seed Removal"},{"location":"design/adr/0005-Service-Self-Config/#status","text":"approved - TSC vote on 3/25/20 for Geneva release NOTE: this ADR does not address high availability considerations and concerns. EdgeX, in general, has a number of unanswered questions with regard to HA architecture and this design adds to those considerations.","title":"Status"},{"location":"design/adr/0005-Service-Self-Config/#context","text":"Since its debut, EdgeX has had a configuration seed service (config-seed) that, on start of EdgeX, deposits configuration for all the services into Consul (our configuration/registry service). For development purposes, or on resource constrained platforms, EdgeX can be run without Consul with services simply reading configuration from the filesystem. While this process has nominally worked for several releases of EdgeX, there has always been some issues with this extra initialization process (config-seed), not least of which are: - race conditions on the part of the services, as they bootstrap, coming up before the config-seed completes its deposit of configuration into Consul - how to deal with \"overrides\" such as environmental variable provided configuration overrides. As the override is often specific to a service but has to be in place for config-seed in order to take effect. - need for an additional service that is only there for init and then dies (confusing to users) NOTE - for historical purposes, it should be noted that config-seed only writes configuration into the configuration/registry service (Consul) once on the first start of EdgeX. On subsequent starts of EdgeX, config-seed checks to see if it has already populated the configuration/registry service and will not rewrite configuration again (unless the --overwrite flag is used). The design/architectural proposal, therefore, is: - removal of the config-seed service (removing cmd/config-seed from the edgex-go repository) - have each EdgeX micro service \"self seed\" - that is seed Consul with their own required configuration on bootstrap of the service. Details of that bootstrapping process are below.","title":"Context"},{"location":"design/adr/0005-Service-Self-Config/#command-line-options","text":"All EdgeX services support a common set of command-line options, some combination of which are required on startup for a service to interact with the rest of EdgeX. Command line options are not set by any configuration. Command line options include: --configProvider or -cp (the configuration provider location URL) --overwrite or -o (overwrite the configuration in the configuration provider) --file or -f (the configuration filename - configuration.toml is used by default if the configuration filename is not provided) --profile or -p (the name of a sub directory in the configuration directory in which a profile-specific configuration file is found. This has no default. If not specified, the configuration file is read from the configuration directory) --confdir or -c (the directory where the configuration file is found - ./res is used by default if the confdir is not specified, where \".\" is the convention on Linux/Unix/MacOS which means current directory) --registry or -r (string indicating use of the registry) The distinction of command line options versus configuration will be important later in this ADR. Two command line options (-o for overwrite and -r for registry) are not overridable by environmental variables. NOTES: Use of the --overwrite command line option should be used sparingly and with expert knowledge of EdgeX; in particular knowledge of how it operates and where/how it gets its configuration on restarts, etc. Ordinarily, --overwrite is provided as a means to support development needs. Use of --overwrite permanently in production enviroments is highly discouraged.","title":"Command Line Options"},{"location":"design/adr/0005-Service-Self-Config/#configuration-initialization","text":"Each service has (or shall have if not providing it already) a local configuration file. The service may use the local configuration file on initialization of the service (aka bootstrap of the service) depending on command line options and environmental variables (see below) provided at startup. Using a configuration provider When the configuration provider is specified, the service will call on the configuration provider (Consul) and check if the top-level (root) namespace for the service exists. If configuratation at the top-level (root) namespace exists, it indicates that the service has already populated its configuration into the configuration provider in a prior startup. If the service finds the top-level (root) namespace is already populated with configuration information it will then read that configuration information from the configuration provider under namespace for that service (and ignore what is in the local configuration file). If the service finds the top-level (root) namespace is not populated with configuration information, it will read its local configuration file and populate the configuration provider (under the namespace for the service) with configuration read from the local configuration file. A configuration provider can be specified with a command line argument (the -cp / --configProvider) or environment variable (the EDGEX_CONFIGURATION_PROVIDER environmental variable which overrides the command line argument). NOTE: the environmental variables are typically uppercase but there have been inconsistencies in environmental variable casing (example: edgex_registry). This should be considered and made consistent in a future major release. Using the local configuration file When a configuration provider isn't specified, the service just uses the configuration in its local configuration file. That is the service uses the configuration in the file associated with the profile, config filename and config file directory command line options or environmental variables. In this case, the service does not contact the configuration service (Consul) for any configuration information. NOTE: As the services now self seed and deployment specific changes can be made via environment overrides, it will no longer be necessary to have a Docker profile configuration file in each of the service directories (example: https://github.com/edgexfoundry/edgex-go/blob/master/cmd/core-data/res/docker/configuration.toml). See Consequences below. It will still be possible for users to use the profile mechanism to specify a Docker configuration, but it will no longer be required and not the recommended approach to providing Docker container specific configuration.","title":"Configuration Initialization"},{"location":"design/adr/0005-Service-Self-Config/#overrides","text":"Environment variables used to override configuration always take precedence whether configuration is being sourced locally or read from the config provider/Consul. Note - this means that a configuration value that is being overridden by an environment variable will always be the source of truth, even if the same configuration is changed directly in Consul. The name of the environmental variable must match the path names in Consul. NOTES: - Environmental variables overrides remove the need to change the \"docker\" profile in the res/docker/configuration.toml files - Allowing removal of 50% of the existing configuration.toml files. - The override rules in EdgeX between environmental variables and command line options may be counter intuitive compared to other systems. There appears to be no standard practice. Indeed, web searching \"Reddit & Starting Fights Env Variables vs Command Line Args\" will layout the prevailing differences. - Environment variables used for configuration overrides are named by prepending the the configuration element with the configuration section inclusive of sub-path, where sub-path's \".\"s are replaced with underscores. These configuration environment variable overrides must be specified using camel case. Here are two examples: Registry_Host for [Registry] Host = 'localhost' Clients_CoreData_Host for [Clients] [Clients.CoreData] Host = 'localhost' - Going forward, environmental variables that override command line options should be all uppercase. All values overriden get logged (indicating which configuration value or op param and the new value).","title":"Overrides"},{"location":"design/adr/0005-Service-Self-Config/#decision","text":"These features have been implemented (with some minor changes to be done) for consideration here: https://github.com/edgexfoundry/go-mod-bootstrap/compare/master...lenny-intel:SelfSeed2. This code branch will be removed once this ADR is approved and implemented on master. The implementation for self-seeding services and environmental overrides is already implemented (for Fuji) per this document in the application services and device services (and instituted in the SDKs of each).","title":"Decision"},{"location":"design/adr/0005-Service-Self-Config/#backward-compatibility","text":"Several aspects of this ADR contain backward compatibility issues for the device service and application service SDKs. Therefore, for the upcoming minor release, the following guidelines and expections are added to provide for backward compatibility. --registry= for Device SDKs As earlier versions of the device service SDKs accepted a URI for --registry, if specified on the command line, use the given URI as the address of the configuration provider. If both --configProvider and --registry specify URIs, then the service should log an error and exit. --registry (no \u2018=\u2019) and w/o --configProvider for both SDKs If a configProvider URI isn't specified, but --registry (w/out a URI) is specified, then the service will use the Registry provider information from its local configuration file for both configuration and registry providers. Env Var: edgex_registry= for all services (currently has been removed) Add it back and use value as if it was EDGEX_CONFIGURATION_PROVIDER and enable use of registry with same settings in URL. Default to http as it is in Fuji.","title":"Backward compatibility"},{"location":"design/adr/0005-Service-Self-Config/#consequences","text":"Docker compose files will need to be changed to remove config seed. The main Snap will need to be changed to remove config seed. Config seed code (currently in edgex-go repo) is to be removed. Any service specific environmental overrides currently on config seed need to be moved to the specific service(s). The Docker configuration files and directory (example: https://github.com/edgexfoundry/edgex-go/blob/master/cmd/core-data/res/docker/configuration.toml) that are used to populate the config seed for Docker containers can be eliminated from all the services. In cmd/security-secretstore-setup, there is only a docker configuration.toml. This file will be moved rather than deleted. Documentation would need to reflect removal of config seed and \"self seeding\" process. Removes any potential issue with past race conditions (as experienced with the Edinburgh release) as each service is now responsible for its own configuration. There are still high availability concerns that need to be considered and not covered in this ADR at this time. Removes some confusion on the part of users as to why a service (config-seed) starts and immediately exits. Minimal impact to development cycles and release schedule Configuration endpoints in all services need to ensure the environmental variables are reflected in the configuration data returned (this is a system management impact). Docker files will need to be modified to remove setting profile=docker Docker compose files will need to be changed to add environmental overrides for removal of docker profiles. These should go in the global environment section of the compose files for those overrides that apply to all services. Example: # all common shared environment variables defined here: x-common-env-variables: &common-variables EDGEX_SECURITY_SECRET_STORE: \"false\" EDGEX_CONFIGURATION_PROVIDER: consul.http://edgex-core-consul:8500 Clients_CoreData_Host: edgex-core-data Clients_Logging_Host: edgex-support-logging Logging_EnableRemote: \"true\"","title":"Consequences"},{"location":"design/adr/core/0003-V2-API-Principles/","text":"Geneva API Guiding Principles Status Accepted by EdgeX Foundry working groups as of Core Working Group meeting 16-Jan-2020 Context A redesign of the EdgeX Foundry API is proposed for the Geneva release. This is understood by the community to warrant a 2.0 release that will not be backward compatible. The goal is to rework the API using solid principles that will allow for extension over the course of several release cycles, avoiding the necessity of yet another major release version in a short period of time. Briefly, this effort grew from the acknowledgement that the current models used to facilitate requests and responses via the EdgeX Foundry API were legacy definitions that were once used as internal representations of state within the EdgeX services themselves. Thus if you want to add or update a device, you populate a full device model rather than a specific Add/UpdateDeviceRequest. Currently, your request model has the same definition, and thus validation constraints, as the response model because they are one and the same! It is desirable to separate and be specific about what is required for a given request, as well as its state validity, and the bare minimum that must be returned within a response. Following from that central need, other considerations have been used when designing this proposed API. These will be enumerated and briefly explained below. 1.) Transport-agnostic Define the request/response data transfer objects (DTO) in a manner whereby they can be used independent of transport. For example, although an OpenAPI doc is implicitly coupled to HTTP/REST, define the DTOs in such a way that they could also be used if the platform were to evolve to a pub/sub architecture. 2.) Support partial updates via PATCH Given a request to, for example, update a device the user should be able to update only some properties of the device. Previously this would require an endpoint for each individual property to be updated since the \"update device\" endpoint, facilitated by a PUT, would perform a complete replacement of the device's data. If you only wanted to update the LastConnected timestamp, then a separate endpoint for that property was required. We will leverage PATCH in order to update an entity and only those properties populated on the request will be considered. Properties that are missing or left blank will not be touched. 3.) Support multiple requests at once Endpoints for the addition or updating of data (POST/PATCH) should accept multiple requests at once. If it were desirable to add or update multiple devices with one request, for example, the API should facilitate this. 4.) Support multiple correlated responses at once Following from #3 above, each request sent to the endpoint must result in a corresponding response. In the case of HTTP/REST, this means if four requests are sent to a POST operation, the return payload will have four responses. Each response must expose a \"code\" property containing a numeric result for what occurred. These could be equivalent to HTTP status codes, for example. So while the overall call might succeed, one or more of the child requests may not have. It is up to the caller to examine each response and handle accordingly. In order to correlate each response to its original request, each request must be assigned its own ID (in GUID format). The caller can then tie a response to an individual request and handle the result accordingly, or otherwise track that a response to a given request was not received. 5.) Use of 207 HTTP Status (Multi-Result) In the case where an endpoint can support multiple responses, the returned HTTP code from a REST API will be 207 (Multi-status) 6.) Each service should provide a \"batch\" request endpoint In addition to use-case specific endpoints that you'd find in any REST API, each service should provide a \"batch\" endpoint that can take any kind of request. This is a generic endpoint that allows you to group requests of different types within a single call. For example, instead of having to call two endpoints to get two jobs done, you can call a single endpoint passing the specific requests and have them routed appropriately within the service. Also, when considering agnostic transport, the batch endpoint would allow for the definition and handling of \"GET\" equivalent DTOs which are now implicit in the format of a URL. 7.) GET endpoints returning a list of items must support pagination URL parameters must be supported for every GET endpoint to support pagination. These parameters should indicate the current page of results and the number of results on a page. Decision Commnunity has accepted the reasoning for the new API and the design principles outlined above. The approach will be to gradually implement the V2 API side-by-side with the current V1 APIs. We believe it will take more than a single release cycle to implement the new specification. Releases of that occur prior to the V2 API implementation completion will continue to be major versioned as 1.x. Subsequent to completion, releases will be major versioned as 2.x. Consequences Backward incompatibility with EdgeX Foundry's V1 API requires a major version increment (e.g. v2.x). Service-level testing (e.g. blackbox tests) needs to be rewritten. Specification-first development allows for different implementations of EdgeX services to be certified as \"EdgeX Compliant\" in reference to an objective standard. Transport-agnostic focus enables different architectural patterns (pub/sub versus REST) using the same data representation.","title":"Geneva API Guiding Principles"},{"location":"design/adr/core/0003-V2-API-Principles/#geneva-api-guiding-principles","text":"","title":"Geneva API Guiding Principles"},{"location":"design/adr/core/0003-V2-API-Principles/#status","text":"Accepted by EdgeX Foundry working groups as of Core Working Group meeting 16-Jan-2020","title":"Status"},{"location":"design/adr/core/0003-V2-API-Principles/#context","text":"A redesign of the EdgeX Foundry API is proposed for the Geneva release. This is understood by the community to warrant a 2.0 release that will not be backward compatible. The goal is to rework the API using solid principles that will allow for extension over the course of several release cycles, avoiding the necessity of yet another major release version in a short period of time. Briefly, this effort grew from the acknowledgement that the current models used to facilitate requests and responses via the EdgeX Foundry API were legacy definitions that were once used as internal representations of state within the EdgeX services themselves. Thus if you want to add or update a device, you populate a full device model rather than a specific Add/UpdateDeviceRequest. Currently, your request model has the same definition, and thus validation constraints, as the response model because they are one and the same! It is desirable to separate and be specific about what is required for a given request, as well as its state validity, and the bare minimum that must be returned within a response. Following from that central need, other considerations have been used when designing this proposed API. These will be enumerated and briefly explained below. 1.) Transport-agnostic Define the request/response data transfer objects (DTO) in a manner whereby they can be used independent of transport. For example, although an OpenAPI doc is implicitly coupled to HTTP/REST, define the DTOs in such a way that they could also be used if the platform were to evolve to a pub/sub architecture. 2.) Support partial updates via PATCH Given a request to, for example, update a device the user should be able to update only some properties of the device. Previously this would require an endpoint for each individual property to be updated since the \"update device\" endpoint, facilitated by a PUT, would perform a complete replacement of the device's data. If you only wanted to update the LastConnected timestamp, then a separate endpoint for that property was required. We will leverage PATCH in order to update an entity and only those properties populated on the request will be considered. Properties that are missing or left blank will not be touched. 3.) Support multiple requests at once Endpoints for the addition or updating of data (POST/PATCH) should accept multiple requests at once. If it were desirable to add or update multiple devices with one request, for example, the API should facilitate this. 4.) Support multiple correlated responses at once Following from #3 above, each request sent to the endpoint must result in a corresponding response. In the case of HTTP/REST, this means if four requests are sent to a POST operation, the return payload will have four responses. Each response must expose a \"code\" property containing a numeric result for what occurred. These could be equivalent to HTTP status codes, for example. So while the overall call might succeed, one or more of the child requests may not have. It is up to the caller to examine each response and handle accordingly. In order to correlate each response to its original request, each request must be assigned its own ID (in GUID format). The caller can then tie a response to an individual request and handle the result accordingly, or otherwise track that a response to a given request was not received. 5.) Use of 207 HTTP Status (Multi-Result) In the case where an endpoint can support multiple responses, the returned HTTP code from a REST API will be 207 (Multi-status) 6.) Each service should provide a \"batch\" request endpoint In addition to use-case specific endpoints that you'd find in any REST API, each service should provide a \"batch\" endpoint that can take any kind of request. This is a generic endpoint that allows you to group requests of different types within a single call. For example, instead of having to call two endpoints to get two jobs done, you can call a single endpoint passing the specific requests and have them routed appropriately within the service. Also, when considering agnostic transport, the batch endpoint would allow for the definition and handling of \"GET\" equivalent DTOs which are now implicit in the format of a URL. 7.) GET endpoints returning a list of items must support pagination URL parameters must be supported for every GET endpoint to support pagination. These parameters should indicate the current page of results and the number of results on a page.","title":"Context"},{"location":"design/adr/core/0003-V2-API-Principles/#decision","text":"Commnunity has accepted the reasoning for the new API and the design principles outlined above. The approach will be to gradually implement the V2 API side-by-side with the current V1 APIs. We believe it will take more than a single release cycle to implement the new specification. Releases of that occur prior to the V2 API implementation completion will continue to be major versioned as 1.x. Subsequent to completion, releases will be major versioned as 2.x.","title":"Decision"},{"location":"design/adr/core/0003-V2-API-Principles/#consequences","text":"Backward incompatibility with EdgeX Foundry's V1 API requires a major version increment (e.g. v2.x). Service-level testing (e.g. blackbox tests) needs to be rewritten. Specification-first development allows for different implementations of EdgeX services to be certified as \"EdgeX Compliant\" in reference to an objective standard. Transport-agnostic focus enables different architectural patterns (pub/sub versus REST) using the same data representation.","title":"Consequences"},{"location":"design/adr/device-service/0002-Array-Datatypes/","text":"Array Datatypes Design Status Context Decision Consequences Status Proposed Context The current data model does not directly provide for devices which provide array data. Small fixed-length arrays may be handled by defining multiple device resources - one for each element - and aggregating them via a resource command. Other array data may be passed using the Binary type. Neither of these approaches is ideal: the binary data is opaque and any service processing it would need specific knowledge to do so, and aggregation presents the device service implementation with a multiple-read request that could in many cases be better handled by a single request. This design adds arrays of primitives to the range of supported types in EdgeX. It comprises an extension of the DeviceProfile model, and an update to the definition of Reading. Decision DeviceProfile extension The permitted values of the Type field in PropertyValue are extended to include: \"BoolArray\", \"Uint8Array\", \"Uint16Array\", \"Uint32Array\", \"Uint64Array\", \"Int8Array\", Int16Array\", \"Int32Array\", \"Int64Array\", \"Float32Array\", \"Float64Array\" Readings Implementation in v2 API The value field of SimpleReading becomes an array of strings. For non-array types, an array of length 1 is created. Fallback position for v1 API In the v1 API, Reading.Value is a string representation of the data. If this is maintained, the representation for Array types will follow the JSON array syntax, ie [\"value1\", \"value2\", ...] Consequences Any service which processes Readings will need to be reworked to account for the new Reading type. Device Service considerations The API used for interfacing between device SDKs and devices service implementations contains a local representation of reading values. This will need to be updated in line with the changes outlined here. For C, this will involve an extension of the existing union type. For Go, additional fields may be added to the CommandValue structure. Processing of numeric data in the device service, ie offset , scale etc will not be applied to the values in an array.","title":"Array Datatypes Design"},{"location":"design/adr/device-service/0002-Array-Datatypes/#array-datatypes-design","text":"Status Context Decision Consequences","title":"Array Datatypes Design"},{"location":"design/adr/device-service/0002-Array-Datatypes/#status","text":"Proposed","title":"Status"},{"location":"design/adr/device-service/0002-Array-Datatypes/#context","text":"The current data model does not directly provide for devices which provide array data. Small fixed-length arrays may be handled by defining multiple device resources - one for each element - and aggregating them via a resource command. Other array data may be passed using the Binary type. Neither of these approaches is ideal: the binary data is opaque and any service processing it would need specific knowledge to do so, and aggregation presents the device service implementation with a multiple-read request that could in many cases be better handled by a single request. This design adds arrays of primitives to the range of supported types in EdgeX. It comprises an extension of the DeviceProfile model, and an update to the definition of Reading.","title":"Context"},{"location":"design/adr/device-service/0002-Array-Datatypes/#decision","text":"","title":"Decision"},{"location":"design/adr/device-service/0002-Array-Datatypes/#deviceprofile-extension","text":"The permitted values of the Type field in PropertyValue are extended to include: \"BoolArray\", \"Uint8Array\", \"Uint16Array\", \"Uint32Array\", \"Uint64Array\", \"Int8Array\", Int16Array\", \"Int32Array\", \"Int64Array\", \"Float32Array\", \"Float64Array\"","title":"DeviceProfile extension"},{"location":"design/adr/device-service/0002-Array-Datatypes/#readings","text":"","title":"Readings"},{"location":"design/adr/device-service/0002-Array-Datatypes/#implementation-in-v2-api","text":"The value field of SimpleReading becomes an array of strings. For non-array types, an array of length 1 is created.","title":"Implementation in v2 API"},{"location":"design/adr/device-service/0002-Array-Datatypes/#fallback-position-for-v1-api","text":"In the v1 API, Reading.Value is a string representation of the data. If this is maintained, the representation for Array types will follow the JSON array syntax, ie [\"value1\", \"value2\", ...]","title":"Fallback position for v1 API"},{"location":"design/adr/device-service/0002-Array-Datatypes/#consequences","text":"Any service which processes Readings will need to be reworked to account for the new Reading type.","title":"Consequences"},{"location":"design/adr/device-service/0002-Array-Datatypes/#device-service-considerations","text":"The API used for interfacing between device SDKs and devices service implementations contains a local representation of reading values. This will need to be updated in line with the changes outlined here. For C, this will involve an extension of the existing union type. For Go, additional fields may be added to the CommandValue structure. Processing of numeric data in the device service, ie offset , scale etc will not be applied to the values in an array.","title":"Device Service considerations"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/","text":"Device Service REST API Status proposed Context This ADR details the REST API to be provided by Device Service implementations in EdgeX version 2.x. As such, it supercedes the equivalent sections of the earlier \"Device Service Functional Requirements\" document. These requirements should be implemented as far as possible within the Device Service SDKs, but they also apply to any Device Service implementation. Decision Common endpoints The DS should provide the REST endpoints that are expected of all EdgeX microservices, specifically: config metrics ping version Callback Endpoint Methods callback/device PUT and POST callback/device/id/{id} DELETE callback/profile PUT and POST callback/profile/id/{id} DELETE callback/watcher PUT and POST callback/watcher/id/{id} DELETE parameter meaning {id} a database-generated object id These endpoints are used by the Core Metadata service to inform the device service of metadata updates. Endpoints are defined for each of the objects of interest to a device service, ie Devices, Device Profiles and Provision Watchers. On receipt of calls to these endpoints the device service should update its internal state accordingly. Object deletion When an object is deleted, the Metadata service makes a DELETE request to the relevant callback/{type}/id/{id} endpoint. Object creation and updates When an object is created or updated, the Metadata service makes a POST or PUT request respectively to the relevant callback/{type} endpoint. The payload of the request is the new or updated object. Device Endpoint Methods device/name/{name}/{command} GET and PUT device/{id}/{command} GET and PUT parameter meaning {id} a database-generated device id or {name} the name of the device {command} the command name The command specified must match a deviceCommand or deviceResource name in the device's profile body (for PUT ): An application/json SettingRequest, which is a set of key/value pairs where the keys are valid deviceResource names, and the values provide the command argument for that resource. Example: {\"AHU-TargetTemperature\": \"28.5\", \"AHU-TargetBand\": \"4.0\"} Return code Meaning 200 the command was successful 404 the specified device does not exist, or the command/resource is unknown 405 attempted write to a read-only resource 423 the specified device is locked (admin state) or disabled (operating state) 500 the device driver is unable to process the request response body : A successful GET operation will return a JSON-encoded Event, which contains one or more Readings. Example: {\"device\":\"Gyro\",\"origin\":1592405201763915855,\"readings\":[{\"name\":\"Xrotation\",\"value\":\"124\",\"origin\":1592405201763915855,\"valueType\":\"int32\"},{\"name\":\"Yrotation\",\"value\":\"-54\",\"origin\":1592405201763915855,\"valueType\":\"int32\"},{\"name\":\"Zrotation\",\"value\":\"122\",\"origin\":1592405201763915855,\"valueType\":\"int32\"}]} This endpoint is used for obtaining readings from a device, and for writing settings to a device. Data formats The values obtained when readings are taken, or used to make settings, are expressed as strings. Type EdgeX types Representation Boolean bool \"true\" or \"false\" Integer uint8-uint64 , int8-int64 Numeric string, eg \"-132\" Float float32 , float64 base64 encoded little-endian binary or decimal with exponent, eg \"1.234e-5\" String string string Binary bytes octet array Array boolarray , uint8array-uint64array , int8array-int64array , float32array , float64array JSON Array, eg \"[\"1\", \"34\", \"-5\"]\" Notes: - The presence of a Binary reading will cause the entire Event to be encoded using CBOR rather than JSON - The representation of a float is determined by the \"floatEncoding\" attribute of the device resource. \"base64\" is the default, \"eNotation\" will cause the float to be written as a decimal with exponent. Readings and Events A Reading represents a value obtained from a deviceResource. It contains the following fields Field name Description name The name of the deviceResource valueType The type of the data origin A timestamp indicating when the reading was taken value The reading value mediaType (Only for Binary readings) The MIME type of the data floatEncoding (Only for floats and arrays of floats) The float representation in use An Event represents the result of a GET command. If the command names a deviceResource, the Event will contain a single Reading. If the command names a deviceCommand, the Event will contain as many Readings as there are deviceResources listed in the deviceCommand. The fields of an Event are as follows: Field name Description device The name of the Device from which the Readings are taken origin The time at which the Event was created readings An array of Readings Query Parameters Calls to the device endpoints may include a Query String in the URL. This may be used to pass parameters relating to the request to the device service. Individual device services may define their own parameters to control specific behaviors. Parameters beginning with the prefix ds- are reserved to the Device SDKs and the following parameters are defined for GET requests: Parameter Valid Values Default Meaning ds-postevent \"yes\" or \"no\" \"no\" If set to yes, a successful GET will result in an event being posted to core-data ds-returnevent \"yes\" or \"no\" \"yes\" If set to no, there will be no Event returned in the http response Device States A Device in EdgeX has two states associated with it: the Administrative state and the Operational state. The Administrative state may be set to LOCKED (normally UNLOCKED ) to block access to the device for administrative reasons. The Operational state may be set to DISABLED (normally ENABLED ) to indicate that the device is not currently working. In either case access to the device via this endpoint will be denied and HTTP 423 (\"Locked\") will be returned. Data Transformations A number of simple data transformations may be defined in the deviceResource. The table below shows these transformations in the order in which they are applied to outgoing data, ie Readings. The transformations are inverted and applied in reverse order for incoming data. Transform Applicable reading types Effect mask Integers The reading is bitwise masked with the specified value. shift Integers The reading is bit-shifted by the specified value. Positive values indicate right-shift, negative for left. base Integers and Floats The reading is replaced by the specified value raised to the power of the reading. scale Integers and Floats The reading is multiplied by the specified value. offset Integers and Floats The reading is increased by the specified value. The operation of the mask transform on incoming data (a setting) is that the value to be set on the resource is the existing value bitwise-anded with the complement of the mask, bitwise-ored with the value specified in the request. ie, new-value = (current-value & !mask) | request-value The combination of mask and shift can therefore be used to access data contained in a subdivision of an octet. Assertions and Mappings Assertions are another attribute in a device resource's PropertyValue which specify a string value which the result is compared against. If the comparison fails, then the result is set to a string of the form \"Assertion failed for device resource: \\ , with value: \\ \" , this also has a side-effect of setting the device operatingstate to DISABLED . A 500 status code is also returned. Mappings may be defined in a deviceCommand. These allow Readings of string type to be remapped. Mappings are applied after assertions are checked, and are the final transformation before Readings are created. Mappings are also applied, but in reverse, to settings ( PUT request data). lastConnected timestamp Each Device has as part of its metadata a timestamp named lastConnected , this indicates the most recent occasion when the device was successfully interacted with. The device service should update this timestamp every time a GET or PUT operation succeeds, unless it has been configured not to do so (eg for performance reasons). Discovery Endpoint Methods discovery POST A call to this endpoint triggers the device discovery process, if enabled. See Discovery Design for details. Consequences Changes from v1.x API The callback endpoint is split according to the type of object being updated Callbacks for new and updated objects take the object in the request body The device/all form is removed GET requests take parameters controlling what is to be done with resulting Events, and the default behavior does not send the Event to core-data References OpenAPI definition of v2 API : https://github.com/edgexfoundry/device-sdk-go/blob/master/api/oas3.0/v2/device-sdk.yaml Device Service Functional Requirements (Geneva) : https://wiki.edgexfoundry.org/download/attachments/329488/edgex-device-service-requirements-v11.pdf?version=1&modificationDate=1591621033000&api=v2","title":"Device Service REST API"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#device-service-rest-api","text":"","title":"Device Service REST API"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#status","text":"proposed","title":"Status"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#context","text":"This ADR details the REST API to be provided by Device Service implementations in EdgeX version 2.x. As such, it supercedes the equivalent sections of the earlier \"Device Service Functional Requirements\" document. These requirements should be implemented as far as possible within the Device Service SDKs, but they also apply to any Device Service implementation.","title":"Context"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#decision","text":"","title":"Decision"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#common-endpoints","text":"The DS should provide the REST endpoints that are expected of all EdgeX microservices, specifically: config metrics ping version","title":"Common endpoints"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#callback","text":"Endpoint Methods callback/device PUT and POST callback/device/id/{id} DELETE callback/profile PUT and POST callback/profile/id/{id} DELETE callback/watcher PUT and POST callback/watcher/id/{id} DELETE parameter meaning {id} a database-generated object id These endpoints are used by the Core Metadata service to inform the device service of metadata updates. Endpoints are defined for each of the objects of interest to a device service, ie Devices, Device Profiles and Provision Watchers. On receipt of calls to these endpoints the device service should update its internal state accordingly.","title":"Callback"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#object-deletion","text":"When an object is deleted, the Metadata service makes a DELETE request to the relevant callback/{type}/id/{id} endpoint.","title":"Object deletion"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#object-creation-and-updates","text":"When an object is created or updated, the Metadata service makes a POST or PUT request respectively to the relevant callback/{type} endpoint. The payload of the request is the new or updated object.","title":"Object creation and updates"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#device","text":"Endpoint Methods device/name/{name}/{command} GET and PUT device/{id}/{command} GET and PUT parameter meaning {id} a database-generated device id or {name} the name of the device {command} the command name The command specified must match a deviceCommand or deviceResource name in the device's profile body (for PUT ): An application/json SettingRequest, which is a set of key/value pairs where the keys are valid deviceResource names, and the values provide the command argument for that resource. Example: {\"AHU-TargetTemperature\": \"28.5\", \"AHU-TargetBand\": \"4.0\"} Return code Meaning 200 the command was successful 404 the specified device does not exist, or the command/resource is unknown 405 attempted write to a read-only resource 423 the specified device is locked (admin state) or disabled (operating state) 500 the device driver is unable to process the request response body : A successful GET operation will return a JSON-encoded Event, which contains one or more Readings. Example: {\"device\":\"Gyro\",\"origin\":1592405201763915855,\"readings\":[{\"name\":\"Xrotation\",\"value\":\"124\",\"origin\":1592405201763915855,\"valueType\":\"int32\"},{\"name\":\"Yrotation\",\"value\":\"-54\",\"origin\":1592405201763915855,\"valueType\":\"int32\"},{\"name\":\"Zrotation\",\"value\":\"122\",\"origin\":1592405201763915855,\"valueType\":\"int32\"}]} This endpoint is used for obtaining readings from a device, and for writing settings to a device.","title":"Device"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#data-formats","text":"The values obtained when readings are taken, or used to make settings, are expressed as strings. Type EdgeX types Representation Boolean bool \"true\" or \"false\" Integer uint8-uint64 , int8-int64 Numeric string, eg \"-132\" Float float32 , float64 base64 encoded little-endian binary or decimal with exponent, eg \"1.234e-5\" String string string Binary bytes octet array Array boolarray , uint8array-uint64array , int8array-int64array , float32array , float64array JSON Array, eg \"[\"1\", \"34\", \"-5\"]\" Notes: - The presence of a Binary reading will cause the entire Event to be encoded using CBOR rather than JSON - The representation of a float is determined by the \"floatEncoding\" attribute of the device resource. \"base64\" is the default, \"eNotation\" will cause the float to be written as a decimal with exponent.","title":"Data formats"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#readings-and-events","text":"A Reading represents a value obtained from a deviceResource. It contains the following fields Field name Description name The name of the deviceResource valueType The type of the data origin A timestamp indicating when the reading was taken value The reading value mediaType (Only for Binary readings) The MIME type of the data floatEncoding (Only for floats and arrays of floats) The float representation in use An Event represents the result of a GET command. If the command names a deviceResource, the Event will contain a single Reading. If the command names a deviceCommand, the Event will contain as many Readings as there are deviceResources listed in the deviceCommand. The fields of an Event are as follows: Field name Description device The name of the Device from which the Readings are taken origin The time at which the Event was created readings An array of Readings","title":"Readings and Events"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#query-parameters","text":"Calls to the device endpoints may include a Query String in the URL. This may be used to pass parameters relating to the request to the device service. Individual device services may define their own parameters to control specific behaviors. Parameters beginning with the prefix ds- are reserved to the Device SDKs and the following parameters are defined for GET requests: Parameter Valid Values Default Meaning ds-postevent \"yes\" or \"no\" \"no\" If set to yes, a successful GET will result in an event being posted to core-data ds-returnevent \"yes\" or \"no\" \"yes\" If set to no, there will be no Event returned in the http response","title":"Query Parameters"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#device-states","text":"A Device in EdgeX has two states associated with it: the Administrative state and the Operational state. The Administrative state may be set to LOCKED (normally UNLOCKED ) to block access to the device for administrative reasons. The Operational state may be set to DISABLED (normally ENABLED ) to indicate that the device is not currently working. In either case access to the device via this endpoint will be denied and HTTP 423 (\"Locked\") will be returned.","title":"Device States"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#data-transformations","text":"A number of simple data transformations may be defined in the deviceResource. The table below shows these transformations in the order in which they are applied to outgoing data, ie Readings. The transformations are inverted and applied in reverse order for incoming data. Transform Applicable reading types Effect mask Integers The reading is bitwise masked with the specified value. shift Integers The reading is bit-shifted by the specified value. Positive values indicate right-shift, negative for left. base Integers and Floats The reading is replaced by the specified value raised to the power of the reading. scale Integers and Floats The reading is multiplied by the specified value. offset Integers and Floats The reading is increased by the specified value. The operation of the mask transform on incoming data (a setting) is that the value to be set on the resource is the existing value bitwise-anded with the complement of the mask, bitwise-ored with the value specified in the request. ie, new-value = (current-value & !mask) | request-value The combination of mask and shift can therefore be used to access data contained in a subdivision of an octet.","title":"Data Transformations"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#assertions-and-mappings","text":"Assertions are another attribute in a device resource's PropertyValue which specify a string value which the result is compared against. If the comparison fails, then the result is set to a string of the form \"Assertion failed for device resource: \\ , with value: \\ \" , this also has a side-effect of setting the device operatingstate to DISABLED . A 500 status code is also returned. Mappings may be defined in a deviceCommand. These allow Readings of string type to be remapped. Mappings are applied after assertions are checked, and are the final transformation before Readings are created. Mappings are also applied, but in reverse, to settings ( PUT request data).","title":"Assertions and Mappings"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#lastconnected-timestamp","text":"Each Device has as part of its metadata a timestamp named lastConnected , this indicates the most recent occasion when the device was successfully interacted with. The device service should update this timestamp every time a GET or PUT operation succeeds, unless it has been configured not to do so (eg for performance reasons).","title":"lastConnected timestamp"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#discovery","text":"Endpoint Methods discovery POST A call to this endpoint triggers the device discovery process, if enabled. See Discovery Design for details.","title":"Discovery"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#consequences","text":"","title":"Consequences"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#changes-from-v1x-api","text":"The callback endpoint is split according to the type of object being updated Callbacks for new and updated objects take the object in the request body The device/all form is removed GET requests take parameters controlling what is to be done with resulting Events, and the default behavior does not send the Event to core-data","title":"Changes from v1.x API"},{"location":"design/adr/device-service/0011-DeviceService-Rest-API/#references","text":"OpenAPI definition of v2 API : https://github.com/edgexfoundry/device-sdk-go/blob/master/api/oas3.0/v2/device-sdk.yaml Device Service Functional Requirements (Geneva) : https://wiki.edgexfoundry.org/download/attachments/329488/edgex-device-service-requirements-v11.pdf?version=1&modificationDate=1591621033000&api=v2","title":"References"},{"location":"design/adr/devops/0007-Release-Automation/","text":"Release Automation Status Approved by TSC 04/08/2020 Context EdgeX Foundry is a framework composed of microservices to ease development of IoT/Edge solutions. With the framework getting richer, project growth, the number of artifacts to be released has increased. This proposal outlines a method for automating the release process for the base artifacts. Requirements Release Artifact Definition For the scope of Geneva release artifact types are defined as: GitHub tags in the repositories. Docker images in our Nexus repository and Docker hub. Snaps in the Snapcraft store. This list is likely to expand in future releases. General Requirements As the EdgeX Release Czar I gathered the following requirements for automating this part of the release. The release automation needs a manual trigger to be triggered by the EdgeX Release Czar or the Linux Foundation Release Engineers. The goal of this automation is to have a \"push button\" release mechanism to reduce human error in our release process. Release artifacts can come from one or more GitHub repositories at a time. GitHub repositories can have one or more release artifact types to release. GitHub repositories can have one or more artifacts of a specific type to release. (For example: The mono repository, edgex-go, has more than 20 docker images to release.) GitHub repositories may be released at different times. (For example: Application and Device service repositories can be released on a different day than the Core services in the mono repository.) Ability to track multiple release streams for the project. An audit trail history for releases. Location The code that will manage the release automation for EdgeX Foundry will live in a repository called cd-management . This repository will have a branch named release that will track the releases of artifacts off the master branch of the EdgeX Foundry repositories. Multiple Release Streams EdgeX Foundry has this idea of multple release streams that basically coincides with different named branches in GitHub. For the majority of the main releases we will be targeting those off the master branch. In our cd-management repository we will have a release branch that will track the master branches EdgeX repositories. In the future we will mark a specific release for long term support (LTS). When this happens we will have to branch off master in the EdgeX repositories and create a separate release stream for the LTS. The suggestion at that point will be to branch off the release branch in cd-management as well and use this new release branch to track the LTS branches in the EdgeX repositories. Release Flow Go Modules, Device and Application SDKs During Development Go modules, Application and Device SDKs only release a GitHub tag as their release. Go modules are set up to automatically release a new patch version on every merge into the master branch of the repository. (IE: 1.0.0 -> 1.0.1) For Application and Device SDKs we increment a developmental version tag. (IE: 1.0.0-dev.1 -> 1.0.0-dev.2) Release The release automation for go modules is used to set a new version that increases the major or minor version for the module. (IE: 1.0.0 -> 1.1.0) For the Application and Device SDKs it is used to set the final release version. (IE: 1.0.0-dev.X -> 1.0.0) Application, Device Services and Supporting Docker Images During Development For the Device, Application services and supporting docker images we release Github tags, docker images and snaps. On every merge to the master branch we will do the following; increment a developmental version tag on GitHub, (IE: 1.0.0-dev.1 -> 1.0.0-dev.2), stage docker images in our Nexus repository (docker.staging) and snaps will be pushed to the edge (experimental) and beta (QA) channels in the Snapcraft store. Release The release automation for the Application and Device services is the same as the services found in the edgex-go repository. Core Services During Development For the Core services we release Github tags, docker images and snaps. On every merge to the master branch we will do the following; increment a developmental version tag on GitHub, (IE: 1.0.0-dev.1 -> 1.0.0-dev.2), stage docker images in our Nexus repository (docker.staging). Snaps will be pushed daily to the edge (experimental) and beta (QA) channels in the Snapcraft store because the core services snap can take up to a hour and half to build. Release The release automation will need to do the following: Set version tag on GitHub. (IE: 1.0.0-dev.X -> 1.0.0) Promote docker images in our Nexus repository from docker.staging to docker.release and public Docker hub. Promote snaps from the beta (QA) channel to the release channel in the Snapcraft store.","title":"Release Automation"},{"location":"design/adr/devops/0007-Release-Automation/#release-automation","text":"","title":"Release Automation"},{"location":"design/adr/devops/0007-Release-Automation/#status","text":"Approved by TSC 04/08/2020","title":"Status"},{"location":"design/adr/devops/0007-Release-Automation/#context","text":"EdgeX Foundry is a framework composed of microservices to ease development of IoT/Edge solutions. With the framework getting richer, project growth, the number of artifacts to be released has increased. This proposal outlines a method for automating the release process for the base artifacts.","title":"Context"},{"location":"design/adr/devops/0007-Release-Automation/#requirements","text":"","title":"Requirements"},{"location":"design/adr/devops/0007-Release-Automation/#release-artifact-definition","text":"For the scope of Geneva release artifact types are defined as: GitHub tags in the repositories. Docker images in our Nexus repository and Docker hub. Snaps in the Snapcraft store. This list is likely to expand in future releases.","title":"Release Artifact Definition"},{"location":"design/adr/devops/0007-Release-Automation/#general-requirements","text":"As the EdgeX Release Czar I gathered the following requirements for automating this part of the release. The release automation needs a manual trigger to be triggered by the EdgeX Release Czar or the Linux Foundation Release Engineers. The goal of this automation is to have a \"push button\" release mechanism to reduce human error in our release process. Release artifacts can come from one or more GitHub repositories at a time. GitHub repositories can have one or more release artifact types to release. GitHub repositories can have one or more artifacts of a specific type to release. (For example: The mono repository, edgex-go, has more than 20 docker images to release.) GitHub repositories may be released at different times. (For example: Application and Device service repositories can be released on a different day than the Core services in the mono repository.) Ability to track multiple release streams for the project. An audit trail history for releases.","title":"General Requirements"},{"location":"design/adr/devops/0007-Release-Automation/#location","text":"The code that will manage the release automation for EdgeX Foundry will live in a repository called cd-management . This repository will have a branch named release that will track the releases of artifacts off the master branch of the EdgeX Foundry repositories.","title":"Location"},{"location":"design/adr/devops/0007-Release-Automation/#multiple-release-streams","text":"EdgeX Foundry has this idea of multple release streams that basically coincides with different named branches in GitHub. For the majority of the main releases we will be targeting those off the master branch. In our cd-management repository we will have a release branch that will track the master branches EdgeX repositories. In the future we will mark a specific release for long term support (LTS). When this happens we will have to branch off master in the EdgeX repositories and create a separate release stream for the LTS. The suggestion at that point will be to branch off the release branch in cd-management as well and use this new release branch to track the LTS branches in the EdgeX repositories.","title":"Multiple Release Streams"},{"location":"design/adr/devops/0007-Release-Automation/#release-flow","text":"","title":"Release Flow"},{"location":"design/adr/devops/0007-Release-Automation/#go-modules-device-and-application-sdks","text":"","title":"Go Modules, Device and Application SDKs"},{"location":"design/adr/devops/0007-Release-Automation/#during-development","text":"Go modules, Application and Device SDKs only release a GitHub tag as their release. Go modules are set up to automatically release a new patch version on every merge into the master branch of the repository. (IE: 1.0.0 -> 1.0.1) For Application and Device SDKs we increment a developmental version tag. (IE: 1.0.0-dev.1 -> 1.0.0-dev.2)","title":"During Development"},{"location":"design/adr/devops/0007-Release-Automation/#release","text":"The release automation for go modules is used to set a new version that increases the major or minor version for the module. (IE: 1.0.0 -> 1.1.0) For the Application and Device SDKs it is used to set the final release version. (IE: 1.0.0-dev.X -> 1.0.0)","title":"Release"},{"location":"design/adr/devops/0007-Release-Automation/#application-device-services-and-supporting-docker-images","text":"","title":"Application, Device Services and Supporting Docker Images"},{"location":"design/adr/devops/0007-Release-Automation/#during-development_1","text":"For the Device, Application services and supporting docker images we release Github tags, docker images and snaps. On every merge to the master branch we will do the following; increment a developmental version tag on GitHub, (IE: 1.0.0-dev.1 -> 1.0.0-dev.2), stage docker images in our Nexus repository (docker.staging) and snaps will be pushed to the edge (experimental) and beta (QA) channels in the Snapcraft store.","title":"During Development"},{"location":"design/adr/devops/0007-Release-Automation/#release_1","text":"The release automation for the Application and Device services is the same as the services found in the edgex-go repository.","title":"Release"},{"location":"design/adr/devops/0007-Release-Automation/#core-services","text":"","title":"Core Services"},{"location":"design/adr/devops/0007-Release-Automation/#during-development_2","text":"For the Core services we release Github tags, docker images and snaps. On every merge to the master branch we will do the following; increment a developmental version tag on GitHub, (IE: 1.0.0-dev.1 -> 1.0.0-dev.2), stage docker images in our Nexus repository (docker.staging). Snaps will be pushed daily to the edge (experimental) and beta (QA) channels in the Snapcraft store because the core services snap can take up to a hour and half to build.","title":"During Development"},{"location":"design/adr/devops/0007-Release-Automation/#release_2","text":"The release automation will need to do the following: Set version tag on GitHub. (IE: 1.0.0-dev.X -> 1.0.0) Promote docker images in our Nexus repository from docker.staging to docker.release and public Docker hub. Promote snaps from the beta (QA) channel to the release channel in the Snapcraft store.","title":"Release"},{"location":"design/adr/devops/0010-Release-Artifacts/","text":"Release Artifacts Status In Review 06/10/2020 Context During the Geneva release of EdgeX Foundry the DevOps WG transformed the CI/CD process with new Jenkins pipeline functionality. After this new functionality was added we also started adding release automation. This new automation is outlined in ADR 0007 Release Automation. However, in ADR 0007 Release Automation only three release artifact types are outlined. This document is meant to be a living document to try to outlines all currently supported artifacts associated with an EdgeX Foundry release, and should be updated if/when this list changes. Release Artifact Types Docker Images Tied to Code Release? Yes Docker images are released for every named release of EdgeX Foundry. During development the community releases images to the docker.staging repository in Nexus . At the time of release we promote the last tested image from docker.staging to docker.release . In addition to that we will publish the docker image on DockerHub . Nexus Retention Policy docker.snapshots Retention Policy: 90 days since last download Contains: Docker images that are not expected to be released. This contains images to optimize the builds in the CI infrastructure. The definitions of these docker images can be found in the edgexfoundry/ci-build-images Github repository. Docker Tags Used: Version, Latest docker.staging Retention Policy: 180 days since last download Contains: Docker images built for potential release and testing purposes during development. Docker Tags Used: Version (ie: v1.x), Release Branch (master, fuji, etc), Latest docker.release Retention Policy: No automatic removal. Requires TSC approval to remove images from this repository. Contains: Officially released docker images for EdgeX. Docker Tags Used:\u2022Version (ie: v1.x), Latest Nexus Cleanup Policies Reference Docker Compose Files Tied to Code Release? Yes Docker compose files are released alongside the docker images for every release of EdgeX Foundry. During development the community maintains compose files a folder named nightly-build . These compose files are meant to be used by our testing frameworks. At the time of release the community makes compose files for that release in a folder matching it's name. (ie: geneva ) Github Page: EdgeX Docs Tied to Code Release? No EdgeX Foundry releases a set of documentation for our project at http://docs.edgexfoundry.org . This page is a Github page that is managed by the edgex/foundry/edgex-docs Github repository. As a community we make our best effort to keep these docs up to date. On this page we are also versioning the docs with the semantic versions of the named releases. As a community we try to version our documentation site shortly after the official release date but documentation changes are addressed as we find them throughout the release cycle. GitHub Tags Tied to Code Release? Yes, for the final semantic version Github tags are used to track the releases of EdgeX Foundry. During development the tags are incremented automatically for each commit using a development suffix (ie: v1.1.1-dev.1 -> v1.1.1-dev.2 ). At the time of release we release a tag with the final semantic version (ie: v1.1.1 ). Snaps Tied to Code Release? Yes Snaps are released for every named release of EdgeX Foundry. During development the community stages snaps to the latest/edge channel of the Snapcraft Store . At the time of code freeze we will promote the snaps from latest/edge to the latest/beta amd latest/candidate channels as beta release. This beta release is to trigger validation of the release candidate. At the time of release we will promote the snaps from latest/beta to latest/stable . In addition we also create a named release track for the release. (ie: geneva/stable ). SwaggerHub API Docs Tied to Code Release? No In addition to our documentation site EdgeX foundry also releases our API specifications on Swaggerhub. Testing Framework Tied to Code Release? Yes The EdgeX Foundry community has a set of tests we maintain to do regression testing during development this framework is tracking the master branch of the components of EdgeX. At the time of release we will update the testing frameworks to point at the released Github tags and add a version tag to the testing frameworks themselves. This creates a snapshot of testing framework at the time of release for validation of the official release. Known Build Dependencies for EdgeX Foundry There are some internal build dependencies within the EdgeX Foundry organization. When building artifacts for validation or a release you will need to take into the account the build dependencies to make sure you build them in the correct order. Edgex-go Snap: Has dependencies on app-service-configurable, device-virtual-go and support-rulesengine because they are included in the snap. Application services have a dependency on the Application services SDK. Go Device services have a dependency on the Go Device SDK. C Device services have a dependency on the C Device SDK. Decision Consequences This document is meant to be a living document of all the release artifacts of EdgeX Foundry. With this ADR we would have a good understanding on what needs to be released and when they are released. Without this document this information will remain tribal knowledge within the community.","title":"Release Artifacts"},{"location":"design/adr/devops/0010-Release-Artifacts/#release-artifacts","text":"","title":"Release Artifacts"},{"location":"design/adr/devops/0010-Release-Artifacts/#status","text":"In Review 06/10/2020","title":"Status"},{"location":"design/adr/devops/0010-Release-Artifacts/#context","text":"During the Geneva release of EdgeX Foundry the DevOps WG transformed the CI/CD process with new Jenkins pipeline functionality. After this new functionality was added we also started adding release automation. This new automation is outlined in ADR 0007 Release Automation. However, in ADR 0007 Release Automation only three release artifact types are outlined. This document is meant to be a living document to try to outlines all currently supported artifacts associated with an EdgeX Foundry release, and should be updated if/when this list changes.","title":"Context"},{"location":"design/adr/devops/0010-Release-Artifacts/#release-artifact-types","text":"","title":"Release Artifact Types"},{"location":"design/adr/devops/0010-Release-Artifacts/#docker-images","text":"Tied to Code Release? Yes Docker images are released for every named release of EdgeX Foundry. During development the community releases images to the docker.staging repository in Nexus . At the time of release we promote the last tested image from docker.staging to docker.release . In addition to that we will publish the docker image on DockerHub .","title":"Docker Images"},{"location":"design/adr/devops/0010-Release-Artifacts/#nexus-retention-policy","text":"","title":"Nexus Retention Policy"},{"location":"design/adr/devops/0010-Release-Artifacts/#dockersnapshots","text":"Retention Policy: 90 days since last download Contains: Docker images that are not expected to be released. This contains images to optimize the builds in the CI infrastructure. The definitions of these docker images can be found in the edgexfoundry/ci-build-images Github repository. Docker Tags Used: Version, Latest","title":"docker.snapshots"},{"location":"design/adr/devops/0010-Release-Artifacts/#dockerstaging","text":"Retention Policy: 180 days since last download Contains: Docker images built for potential release and testing purposes during development. Docker Tags Used: Version (ie: v1.x), Release Branch (master, fuji, etc), Latest","title":"docker.staging"},{"location":"design/adr/devops/0010-Release-Artifacts/#dockerrelease","text":"Retention Policy: No automatic removal. Requires TSC approval to remove images from this repository. Contains: Officially released docker images for EdgeX. Docker Tags Used:\u2022Version (ie: v1.x), Latest Nexus Cleanup Policies Reference","title":"docker.release"},{"location":"design/adr/devops/0010-Release-Artifacts/#docker-compose-files","text":"Tied to Code Release? Yes Docker compose files are released alongside the docker images for every release of EdgeX Foundry. During development the community maintains compose files a folder named nightly-build . These compose files are meant to be used by our testing frameworks. At the time of release the community makes compose files for that release in a folder matching it's name. (ie: geneva )","title":"Docker Compose Files"},{"location":"design/adr/devops/0010-Release-Artifacts/#github-page-edgex-docs","text":"Tied to Code Release? No EdgeX Foundry releases a set of documentation for our project at http://docs.edgexfoundry.org . This page is a Github page that is managed by the edgex/foundry/edgex-docs Github repository. As a community we make our best effort to keep these docs up to date. On this page we are also versioning the docs with the semantic versions of the named releases. As a community we try to version our documentation site shortly after the official release date but documentation changes are addressed as we find them throughout the release cycle.","title":"Github Page: EdgeX Docs"},{"location":"design/adr/devops/0010-Release-Artifacts/#github-tags","text":"Tied to Code Release? Yes, for the final semantic version Github tags are used to track the releases of EdgeX Foundry. During development the tags are incremented automatically for each commit using a development suffix (ie: v1.1.1-dev.1 -> v1.1.1-dev.2 ). At the time of release we release a tag with the final semantic version (ie: v1.1.1 ).","title":"GitHub Tags"},{"location":"design/adr/devops/0010-Release-Artifacts/#snaps","text":"Tied to Code Release? Yes Snaps are released for every named release of EdgeX Foundry. During development the community stages snaps to the latest/edge channel of the Snapcraft Store . At the time of code freeze we will promote the snaps from latest/edge to the latest/beta amd latest/candidate channels as beta release. This beta release is to trigger validation of the release candidate. At the time of release we will promote the snaps from latest/beta to latest/stable . In addition we also create a named release track for the release. (ie: geneva/stable ).","title":"Snaps"},{"location":"design/adr/devops/0010-Release-Artifacts/#swaggerhub-api-docs","text":"Tied to Code Release? No In addition to our documentation site EdgeX foundry also releases our API specifications on Swaggerhub.","title":"SwaggerHub API Docs"},{"location":"design/adr/devops/0010-Release-Artifacts/#testing-framework","text":"Tied to Code Release? Yes The EdgeX Foundry community has a set of tests we maintain to do regression testing during development this framework is tracking the master branch of the components of EdgeX. At the time of release we will update the testing frameworks to point at the released Github tags and add a version tag to the testing frameworks themselves. This creates a snapshot of testing framework at the time of release for validation of the official release.","title":"Testing Framework"},{"location":"design/adr/devops/0010-Release-Artifacts/#known-build-dependencies-for-edgex-foundry","text":"There are some internal build dependencies within the EdgeX Foundry organization. When building artifacts for validation or a release you will need to take into the account the build dependencies to make sure you build them in the correct order. Edgex-go Snap: Has dependencies on app-service-configurable, device-virtual-go and support-rulesengine because they are included in the snap. Application services have a dependency on the Application services SDK. Go Device services have a dependency on the Go Device SDK. C Device services have a dependency on the C Device SDK.","title":"Known Build Dependencies for EdgeX Foundry"},{"location":"design/adr/devops/0010-Release-Artifacts/#decision","text":"","title":"Decision"},{"location":"design/adr/devops/0010-Release-Artifacts/#consequences","text":"This document is meant to be a living document of all the release artifacts of EdgeX Foundry. With this ADR we would have a good understanding on what needs to be released and when they are released. Without this document this information will remain tribal knowledge within the community.","title":"Consequences"},{"location":"design/legacy-design/","text":"Legacy Design Documents Name/Link Short Description Registry Abstraction Decouple EdgeX services from Consul device-service/Discovery Dynamically discover new devices","title":"Legacy Design Documents"},{"location":"design/legacy-design/#legacy-design-documents","text":"Name/Link Short Description Registry Abstraction Decouple EdgeX services from Consul device-service/Discovery Dynamically discover new devices","title":"Legacy Design Documents"},{"location":"design/legacy-design/device-service/discovery/","text":"Dynamic Device Discovery Overview Some device protocols allow for devices to be discovered automatically. A Device Service may include a capability for discovering devices and creating the corresponding Device objects within EdgeX. A framework for doing so will be implemented in the Device Service SDKs. The discovery process will operate as follows: Discovery is triggered either on an internal timer or by a call to a REST endpoint The SDK will call a function provided by the DS implementation to request a device scan The implementation calls back to the SDK with details of devices which it has found The SDK filters these devices against a set of acceptance criteria The SDK adds accepted devices in core-metadata. These are now available in the EdgeX system Triggering Discovery A boolean configuration value Device/Discovery/Enabled defaults to false. If this value is set true, and the DS implementation supports discovery, discovery is enabled. The SDK will respond to POST requests on the the /discovery endpoint. No content is required in the request. This call will return one of the following codes: 202: discovery has been triggered or is already running. The response should indicate which, and contain the correlation id that will be used by any resulting requests for device addition 423: the service is locked (admin state) or disabled (operating state) 500: unknown or unanticipated issues exist 501: discovery is not supported by this protocol implementation 503: discovery is disabled by configuration In each of the failure cases a meaningful error message should be returned. In the case where discovery is triggered, the discovery process will run in a new thread or goroutine, so that the REST call may return immediately. An integer configuration value Device/Discovery/Interval defaults to zero. If this value is set to a positive value, and discovery is enabled, the discovery process will be triggered at the specified interval (in seconds). Finding Devices When discovery is triggered, the SDK calls the implementation function provided by the Device Service. This should perform whatever protocol-specific procedure is necessary to find devices, and pass these devices into the SDK by calling the SDK's filtered device addition function. Note: The implementation should call back for every device found. The SDK is to take responsibility for filtering out devices which have already been added. The information required for a found device is as follows: An autogenerated device name The Protocol Properties of the device Optionally, a description string Optionally, a list of label strings The filtered device addition function will take as an argument a collection of structs containing the above data. An implementation may choose to make one call per discovered device, but implementors are encouraged to batch the devices if practical, as in future EdgeX versions it will be possible for the SDK to create all required new devices in a single call to core-metadata. Rationale: An alternative design would have the implementation function return the collection of discovered devices to the SDK. Using a callback mechanism instead has the following advantages: Allows for asynchronous operation. In this mode the DS implementation will intiate discovery and return immediately. For example discovery may be initiated by sending a broadcast packet. Devices will then send return packets indicating their existence. The thread handling inbound network traffic can on receipt of such packets call the filtered device addition function directly. Allows DS implementations where devices self-announce to call the filtered device addition function independent of the discovery process Filtered Device Addition The filter criteria for discovered devices are represented by Provision Watchers. A Provision Watcher contains the following fields: Identifiers : A set of name-value pairs against which a new device's ProtocolProperties are matched BlockingIdentifiers : A further set of name-value pairs which are also matched against a new device's ProtocolProperties Profile : The name of a DeviceProfile which should be assigned to new devices which pass this ProvisionWatcher AdminState : The initial Administrative State for new devices which pass this ProvisionWatcher A candidate new device passes a ProvisionWatcher if all of the Identifiers match, and none of the BlockingIdentifiers . The values specified in Identifiers are regular expressions. Note: the above is a whitelist+blacklist scheme. If a discovered Device is manually removed from EdgeX, it will be necessary to adjust the ProvisionWatcher via which it was added, either by making the Identifiers more specific or by adding BlockingIdentifiers , otherwise the Device will be re-added the next time Discovery is initiated. Note: ProvisionWatchers are stored in core-metadata. A facility for managing ProvisionWatchers is needed, eg edgex-cli could be extended","title":"Discovery"},{"location":"design/legacy-design/device-service/discovery/#dynamic-device-discovery","text":"","title":"Dynamic Device Discovery"},{"location":"design/legacy-design/device-service/discovery/#overview","text":"Some device protocols allow for devices to be discovered automatically. A Device Service may include a capability for discovering devices and creating the corresponding Device objects within EdgeX. A framework for doing so will be implemented in the Device Service SDKs. The discovery process will operate as follows: Discovery is triggered either on an internal timer or by a call to a REST endpoint The SDK will call a function provided by the DS implementation to request a device scan The implementation calls back to the SDK with details of devices which it has found The SDK filters these devices against a set of acceptance criteria The SDK adds accepted devices in core-metadata. These are now available in the EdgeX system","title":"Overview"},{"location":"design/legacy-design/device-service/discovery/#triggering-discovery","text":"A boolean configuration value Device/Discovery/Enabled defaults to false. If this value is set true, and the DS implementation supports discovery, discovery is enabled. The SDK will respond to POST requests on the the /discovery endpoint. No content is required in the request. This call will return one of the following codes: 202: discovery has been triggered or is already running. The response should indicate which, and contain the correlation id that will be used by any resulting requests for device addition 423: the service is locked (admin state) or disabled (operating state) 500: unknown or unanticipated issues exist 501: discovery is not supported by this protocol implementation 503: discovery is disabled by configuration In each of the failure cases a meaningful error message should be returned. In the case where discovery is triggered, the discovery process will run in a new thread or goroutine, so that the REST call may return immediately. An integer configuration value Device/Discovery/Interval defaults to zero. If this value is set to a positive value, and discovery is enabled, the discovery process will be triggered at the specified interval (in seconds).","title":"Triggering Discovery"},{"location":"design/legacy-design/device-service/discovery/#finding-devices","text":"When discovery is triggered, the SDK calls the implementation function provided by the Device Service. This should perform whatever protocol-specific procedure is necessary to find devices, and pass these devices into the SDK by calling the SDK's filtered device addition function. Note: The implementation should call back for every device found. The SDK is to take responsibility for filtering out devices which have already been added. The information required for a found device is as follows: An autogenerated device name The Protocol Properties of the device Optionally, a description string Optionally, a list of label strings The filtered device addition function will take as an argument a collection of structs containing the above data. An implementation may choose to make one call per discovered device, but implementors are encouraged to batch the devices if practical, as in future EdgeX versions it will be possible for the SDK to create all required new devices in a single call to core-metadata. Rationale: An alternative design would have the implementation function return the collection of discovered devices to the SDK. Using a callback mechanism instead has the following advantages: Allows for asynchronous operation. In this mode the DS implementation will intiate discovery and return immediately. For example discovery may be initiated by sending a broadcast packet. Devices will then send return packets indicating their existence. The thread handling inbound network traffic can on receipt of such packets call the filtered device addition function directly. Allows DS implementations where devices self-announce to call the filtered device addition function independent of the discovery process","title":"Finding Devices"},{"location":"design/legacy-design/device-service/discovery/#filtered-device-addition","text":"The filter criteria for discovered devices are represented by Provision Watchers. A Provision Watcher contains the following fields: Identifiers : A set of name-value pairs against which a new device's ProtocolProperties are matched BlockingIdentifiers : A further set of name-value pairs which are also matched against a new device's ProtocolProperties Profile : The name of a DeviceProfile which should be assigned to new devices which pass this ProvisionWatcher AdminState : The initial Administrative State for new devices which pass this ProvisionWatcher A candidate new device passes a ProvisionWatcher if all of the Identifiers match, and none of the BlockingIdentifiers . The values specified in Identifiers are regular expressions. Note: the above is a whitelist+blacklist scheme. If a discovered Device is manually removed from EdgeX, it will be necessary to adjust the ProvisionWatcher via which it was added, either by making the Identifiers more specific or by adding BlockingIdentifiers , otherwise the Device will be re-added the next time Discovery is initiated. Note: ProvisionWatchers are stored in core-metadata. A facility for managing ProvisionWatchers is needed, eg edgex-cli could be extended","title":"Filtered Device Addition"},{"location":"design/legacy-requirements/","text":"Legacy Requirements","title":"Legacy Requirements"},{"location":"design/legacy-requirements/#legacy-requirements","text":"","title":"Legacy Requirements"},{"location":"examples/AppServiceExamples/","text":"App Service Examples The following is a list of examples we currently have available that demonstrate various ways that the Application Functions SDK can be used. All of the examples can be found in https://github.com/edgexfoundry-holding/app-service-examples . They focus on how to leverage various built in provided functions as mentioned above as well as how to write your own in the case that the SDK does not provide what is needed. Example Name Description Simple Filter XML Demonstrates Filter of data by device ID and transforming data to XML Simple Filter XML Post Same example as #1, but result published to HTTP Endpoint Simple Filter XML MQTT Same example as #1, but result published to MQTT Broker Advanced Filter Convert Publish Advanced Target Type App Functions AWS App Functions Azure Cloud Event Transforms Http Command Service Secrets Simple CBOR Filter","title":"App Service Examples"},{"location":"examples/AppServiceExamples/#app-service-examples","text":"The following is a list of examples we currently have available that demonstrate various ways that the Application Functions SDK can be used. All of the examples can be found in https://github.com/edgexfoundry-holding/app-service-examples . They focus on how to leverage various built in provided functions as mentioned above as well as how to write your own in the case that the SDK does not provide what is needed. Example Name Description Simple Filter XML Demonstrates Filter of data by device ID and transforming data to XML Simple Filter XML Post Same example as #1, but result published to HTTP Endpoint Simple Filter XML MQTT Same example as #1, but result published to MQTT Broker Advanced Filter Convert Publish Advanced Target Type App Functions AWS App Functions Azure Cloud Event Transforms Http Command Service Secrets Simple CBOR Filter","title":"App Service Examples"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/","text":"MQTT EdgeX - Edinburgh Release Overview In this example, we use the simulator instead of real device. This provides a straight-forward way to test the device-mqtt features. Run an MQTT Broker Eclipse Mosquitto is an open source (EPL/EDL licensed) message broker that implements the MQTT protocol versions 5.0, 3.1.1 and 3.1. Run Mosquitto using the following docker command: docker run -d --rm --name broker -p 1883:1883 eclipse-mosquitto Run an MQTT Device Simulator This simulator has three behaviors: Publish random number data every 15 seconds Receive the reading request, then return the response Receive the put request, then change the device value We created the following script to simulate the MQTT device: // mock - device . js function getRandomFloat ( min , max ) { return Math . random () * ( max - min ) + min ; } const deviceName = \"MQ_DEVICE\" ; let message = \"test-message\" ; // 1 . Publish random number every 15 seconds schedule ( '*/15 * * * * *' , () => { let body = { \"name\" : deviceName , \"cmd\" : \"randnum\" , \"randnum\" : getRandomFloat ( 25 , 29 ). toFixed ( 1 ) } ; publish ( 'DataTopic' , JSON . stringify ( body )); } ); // 2 . Receive the reading request , then return the response // 3 . Receive the put request , then change the device value subscribe ( \"CommandTopic\" , ( topic , val ) => { var data = val ; if ( data . method == \"set\" ) { message = data [ data . cmd ] } else { switch ( data . cmd ) { case \"ping\" : data . ping = \"pong\" ; break ; case \"message\" : data . message = message ; break ; case \"randnum\" : data . randnum = 12 . 123 ; break ; } } publish ( \"ResponseTopic\" , JSON . stringify ( data )); } ); To run the device simulator, enter the commands shown below with the following changes: Replace the /path/to/mqtt-scripts in the example mv command with the correct path Replace the mqtt-broker-ip in the example docker run command with the correct broker IP: mv mock-device.js /path/to/mqtt-scripts docker run -d --restart=always --name=mqtt-scripts \\ -v /path/to/mqtt-scripts:/scripts \\ dersimn/mqtt-scripts --url mqtt://mqtt-broker-ip --dir /scripts Setup In this section, we create a folder that contains files required for deployment: - device-service-demo |- docker-compose.yml |- mqtt |- configuration.toml |- mqtt.test.device.profile.yml Device Profile (mqtt.test.device.profile.yml) The DeviceProfile defines the device's values and operation method, which can be Read or Write. Create a device profile, named mqtt.test.device.profile.yml, with the following content: # mqtt . test . device . profile . yml name : \"Test.Device.MQTT.Profile\" manufacturer : \"iot\" model : \"MQTT-DEVICE\" description : \"Test device profile\" labels : - \"mqtt\" - \"test\" deviceResources : - name : randnum description : \"device random number\" properties : value : { type : \"Float64\" , size : \"4\" , readWrite : \"R\" , floatEncoding : \"eNotation\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"\" } - name : ping description : \"device awake\" properties : value : { type : \"String\" , size : \"0\" , readWrite : \"R\" , defaultValue : \"pong\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"\" } - name : message description : \"device message\" properties : value : { type : \"String\" , size : \"0\" , readWrite : \"W\" , scale : \"\" , offset : \"\" , base : \"\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"\" } deviceCommands : - name : testrandnum get : - { index : \"1\" , operation : \"get\" , object : \"randnum\" , parameter : \"randnum\" } - name : testping get : - { index : \"1\" , operation : \"get\" , object : \"ping\" , parameter : \"ping\" } - name : testmessage get : - { index : \"1\" , operation : \"get\" , object : \"message\" , parameter : \"message\" } set : - { index : \"1\" , operation : \"set\" , object : \"message\" , parameter : \"message\" } coreCommands : - name : testrandnum get : path : \"/api/v1/device/{deviceId}/testrandnum\" responses : - code : \"200\" description : \"get the random value\" expectedValues : [ \"randnum\" ] - code : \"503\" description : \"service unavailable\" expectedValues : [] - name : testping get : path : \"/api/v1/device/{deviceId}/testping\" responses : - code : \"200\" description : \"ping the device\" expectedValues : [ \"ping\" ] - code : \"503\" description : \"service unavailable\" expectedValues : [] - name : testmessage get : path : \"/api/v1/device/{deviceId}/testmessage\" responses : - code : \"200\" description : \"get the message\" expectedValues : [ \"message\" ] - code : \"503\" description : \"service unavailable\" expectedValues : [] put : path : \"/api/v1/device/{deviceId}/testmessage\" parameterNames : [ \"message\" ] responses : - code : \"204\" description : \"set the message.\" expectedValues : [] - code : \"503\" description : \"service unavailable\" expectedValues : [] Device Service Configuration (configuration.toml) Use this configuration file to define devices and schedule jobs. device-mqtt generates a relative instance on start-up. MQTT is subscribe/publish pattern, so we must define the MQTT connection information in the [DeviceList.Protocols] section of the configuration file. Create the configuration file, named configuration.toml, as shown below replacing the host IP with your host address: # configuration . toml [ Writable ] LogLevel = 'DEBUG' [ Service ] Host = \"edgex-device-mqtt\" Port = 49982 ConnectRetries = 3 Labels = [] OpenMsg = \"device mqtt started\" Timeout = 5000 EnableAsyncReadings = true AsyncBufferSize = 16 [ Registry ] Host = \"edgex-core-consul\" Port = 8500 CheckInterval = \"10s\" FailLimit = 3 FailWaitTime = 10 Type = \"consul\" [ Logging ] EnableRemote = false File = \"./device-mqtt.log\" [ Clients ] [ Clients.Data ] Name = \"edgex-core-data\" Protocol = \"http\" Host = \"edgex-core-data\" Port = 48080 Timeout = 50000 [ Clients.Metadata ] Name = \"edgex-core-metadata\" Protocol = \"http\" Host = \"edgex-core-metadata\" Port = 48081 Timeout = 50000 [ Clients.Logging ] Name = \"edgex-support-logging\" Protocol = \"http\" Host = \"edgex-support-logging\" Port = 48061 [ Device ] DataTransform = true InitCmd = \"\" InitCmdArgs = \"\" MaxCmdOps = 128 MaxCmdValueLen = 256 RemoveCmd = \"\" RemoveCmdArgs = \"\" ProfilesDir = \"/custom-config\" # Pre - define Devices [ [DeviceList ] ] Name = \"MQ_DEVICE\" Profile = \"Test.Device.MQTT.Profile\" Description = \"General MQTT device\" Labels = [ \"MQTT\" ] [ DeviceList.Protocols ] [ DeviceList.Protocols.mqtt ] Schema = \"tcp\" Host = \"192.168.16.68\" Port = \"1883\" ClientId = \"CommandPublisher\" User = \"\" Password = \"\" Topic = \"CommandTopic\" [ [DeviceList.AutoEvents ] ] Frequency = \"30s\" OnChange = false Resource = \"testrandnum\" # Driver configs [ Driver ] IncomingSchema = \"tcp\" IncomingHost = \"192.168.16.68\" IncomingPort = \"1883\" IncomingUser = \"\" IncomingPassword = \"\" IncomingQos = \"0\" IncomingKeepAlive = \"3600\" IncomingClientId = \"IncomingDataSubscriber\" IncomingTopic = \"DataTopic\" ResponseSchema = \"tcp\" ResponseHost = \"192.168.16.68\" ResponsePort = \"1883\" ResponseUser = \"\" ResponsePassword = \"\" ResponseQos = \"0\" ResponseKeepAlive = \"3600\" ResponseClientId = \"CommandResponseSubscriber\" ResponseTopic = \"ResponseTopic\" In the Driver configs section : IncomingXxx defines the DataTopic for receiving an async value from the device ResponseXxx defines the ResponseTopic for receiving a command response from the device Add Device Service to docker-compose File (docker-compose.yml) Download the Geneva release docker-compose file from https://github.com/edgexfoundry/developer-scripts/blob/master/releases/geneva/compose-files/docker-compose-geneva-redis.yml . Because we deploy EdgeX using docker-compose, we must add device-mqtt to the docker-compose file. If you have prepared configuration files, you can mount them using volumes and change the entrypoint for device-mqtt internal use. This is illustrated in the following docker-compose file snippet: device-mqtt: image: edgexfoundry/docker-device-mqtt-go:1.0.0 ports: - \"49982:49982\" container_name: edgex-device-mqtt hostname: edgex-device-mqtt networks: - edgex-network volumes: - db-data:/data/db - log-data:/edgex/logs - consul-config:/consul/config - consul-data:/consul/data - ./mqtt:/custom-config depends_on: - data - command entrypoint: - /device-mqtt - --registry=consul://edgex-core-consul:8500 - --confdir=/custom-config When using Device Services, the user has to provide the registry URL in --registry argument. Start EdgeX Foundry on Docker Once the following folder has been populated, we can deploy EdgeX: - device-service-demo |- docker-compose.yml |- mqtt |- configuration.toml |- mqtt.test.device.profile.yml Deploy EdgeX using the following commands: cd path/to/device-service-demo docker-compose pull docker-compose up -d After the services start, check the consul dashboard as follows: Execute Commands Now we're ready to run some commands. Find Executable Commands Use the following query to find executable commands: $ curl http : // your - edgex - server - ip : 48082 / api / v1 / device | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1972 100 1972 0 0 64349 0 --:--:-- --:--:-- --:--:-- 65733 [ { \"location\" : null , \"adminState\" : \"UNLOCKED\" , \"commands\" : [ { ... }, { ... }, { \"get\" : { \"responses\" : [ { \"code\" : \"503\" , \"description\" : \"service unavailable\" } ], \"path\" : \"/api/v1/device/{deviceId}/testmessage\" , \"url\" : \"http://edgex-core-command:48082/api/v1/device/ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff/command/0c257a37-2f72-4d23-b2b1-2c08e895060a\" }, \"modified\" : 1559195042046 , \"name\" : \"testmessage\" , \"put\" : { \"parameterNames\" : [ \"message\" ], \"path\" : \"/api/v1/device/{deviceId}/testmessage\" , \"url\" : \"http://edgex-core-command:48082/api/v1/device/ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff/command/0c257a37-2f72-4d23-b2b1-2c08e895060a\" }, \"created\" : 1559195042046 , \"id\" : \"0c257a37-2f72-4d23-b2b1-2c08e895060a\" } ], \"lastReported\" : 0 , \"operatingState\" : \"ENABLED\" , \"name\" : \"MQ_DEVICE\" , \"lastConnected\" : 0 , \"id\" : \"ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff\" , \"labels\" : [ \"MQTT\" ] } ] Execute put Command Execute a put command according to the url and parameterNames, replacing [host] with the server IP when running the edgex-core-command. This can be done in either of the following ways: $ curl http://your-edgex-server-ip:48082/api/v1/device/ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff/command/0c257a37-2f72-4d23-b2b1-2c08e895060a \\ -H \"Content-Type:application/json\" -X PUT \\ -d '{\"message\":\"Hello!\"}' or \\$ curl \" http://your-edgex-server-ip:48082/api/v1/device/name/MQ_DEVICE/command/testmessage \" -H \"Content-Type:application/json\" -X PUT -d '{\"message\":\"Hello!\"}' Execute get Command Execute a get command as follows: $ curl \"http://your-edgex-server-ip:48082/api/v1/device/name/MQ_DEVICE/command/testmessage\" | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 139 100 139 0 0 132 0 0 : 00 : 01 0 : 00 : 01 --:--:-- 132 { \"readings\" : [ { \"name\" : \"message\" , \"device\" : \"MQ_DEVICE\" , \"value\" : \"Hello!\" , \"origin\" : 1559196276732 } ], \"device\" : \"MQ_DEVICE\" , \"origin\" : 1559196276738 } Schedule Job The schedule job is defined in the [[DeviceList.AutoEvents]] section of the TOML configuration file: # Pre - define Devices [ [DeviceList ] ] Name = \"MQ_DEVICE\" Profile = \"Test.Device.MQTT.Profile\" Description = \"General MQTT device\" Labels = [ \"MQTT\" ] [ DeviceList.Protocols ] [ DeviceList.Protocols.mqtt ] Schema = \"tcp\" Host = \"192.168.16.68\" Port = \"1883\" ClientId = \"CommandPublisher\" User = \"\" Password = \"\" Topic = \"CommandTopic\" [ [DeviceList.AutoEvents ] ] Frequency = \"30s\" OnChange = false Resource = \"testrandnum\" After the service starts, query core-data's reading API. The results show that the service auto-executes the command every 30 secs, as shown below: $ curl http : // your - edgex - server - ip : 48080 / api / v1 / reading | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1613 100 1613 0 0 372 k 0 --:--:-- --:--:-- --:--:-- 393 k [ { \"value\" : \"1.212300e+01\" , \"origin\" : 1559197206092 , \"modified\" : 1559197206104 , \"id\" : \"59f2a768-ad72-49a1-9df9-700d8599a890\" , \"created\" : 1559197206104 , \"device\" : \"MQ_DEVICE\" , \"name\" : \"randnum\" }, { ... }, { \"name\" : \"randnum\" , \"device\" : \"MQ_DEVICE\" , \"modified\" : 1559197175109 , \"created\" : 1559197175109 , \"id\" : \"f9dc39e0-5326-45d0-831d-fd0cd106fe2f\" , \"origin\" : 1559197175098 , \"value\" : \"1.212300e+01\" }, ] Async Device Reading device-mqtt subscribes to a DataTopic , which is wait for real device* to send value to broker , then device-mqtt parses the value and sends it back to core-data . The data format contains the following values: name = device name cmd = deviceResource name method = get or put cmd = device reading You must define this connection information in the driver configuration file, as follows: [Driver] IncomingSchema = \"tcp\" IncomingHost = \"192.168.16.68\" IncomingPort = \"1883\" IncomingUser = \"\" IncomingPassword = \"\" IncomingQos = \"0\" IncomingKeepAlive = \"3600\" IncomingClientId = \"IncomingDataSubscriber\" IncomingTopic = \"DataTopic\" The following results show that the mock device sent the reading every 15 secs: $ curl http : // your - edgex - server - ip : 48080 / api / v1 / reading | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 539 100 539 0 0 169 k 0 --:--:-- --:--:-- --:--:-- 175 k [ { ... }, { \"name\" : \"randnum\" , \"created\" : 1559197140013 , \"origin\" : 1559197140006 , \"modified\" : 1559197140013 , \"id\" : \"286cc305-42f6-4bca-ad41-3af52301c9f7\" , \"value\" : \"2.830000e+01\" , \"device\" : \"MQ_DEVICE\" }, { \"modified\" : 1559197125011 , \"name\" : \"randnum\" , \"created\" : 1559197125011 , \"origin\" : 1559197125004 , \"device\" : \"MQ_DEVICE\" , \"value\" : \"2.690000e+01\" , \"id\" : \"c243e8c6-a904-4102-baff-8a5e4829c4f6\" } ]","title":"MQTT"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#mqtt","text":"EdgeX - Edinburgh Release","title":"MQTT"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#overview","text":"In this example, we use the simulator instead of real device. This provides a straight-forward way to test the device-mqtt features.","title":"Overview"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#run-an-mqtt-broker","text":"Eclipse Mosquitto is an open source (EPL/EDL licensed) message broker that implements the MQTT protocol versions 5.0, 3.1.1 and 3.1. Run Mosquitto using the following docker command: docker run -d --rm --name broker -p 1883:1883 eclipse-mosquitto","title":"Run an MQTT Broker"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#run-an-mqtt-device-simulator","text":"This simulator has three behaviors: Publish random number data every 15 seconds Receive the reading request, then return the response Receive the put request, then change the device value We created the following script to simulate the MQTT device: // mock - device . js function getRandomFloat ( min , max ) { return Math . random () * ( max - min ) + min ; } const deviceName = \"MQ_DEVICE\" ; let message = \"test-message\" ; // 1 . Publish random number every 15 seconds schedule ( '*/15 * * * * *' , () => { let body = { \"name\" : deviceName , \"cmd\" : \"randnum\" , \"randnum\" : getRandomFloat ( 25 , 29 ). toFixed ( 1 ) } ; publish ( 'DataTopic' , JSON . stringify ( body )); } ); // 2 . Receive the reading request , then return the response // 3 . Receive the put request , then change the device value subscribe ( \"CommandTopic\" , ( topic , val ) => { var data = val ; if ( data . method == \"set\" ) { message = data [ data . cmd ] } else { switch ( data . cmd ) { case \"ping\" : data . ping = \"pong\" ; break ; case \"message\" : data . message = message ; break ; case \"randnum\" : data . randnum = 12 . 123 ; break ; } } publish ( \"ResponseTopic\" , JSON . stringify ( data )); } ); To run the device simulator, enter the commands shown below with the following changes: Replace the /path/to/mqtt-scripts in the example mv command with the correct path Replace the mqtt-broker-ip in the example docker run command with the correct broker IP: mv mock-device.js /path/to/mqtt-scripts docker run -d --restart=always --name=mqtt-scripts \\ -v /path/to/mqtt-scripts:/scripts \\ dersimn/mqtt-scripts --url mqtt://mqtt-broker-ip --dir /scripts","title":"Run an MQTT Device Simulator"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#setup","text":"In this section, we create a folder that contains files required for deployment: - device-service-demo |- docker-compose.yml |- mqtt |- configuration.toml |- mqtt.test.device.profile.yml","title":"Setup"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#device-profile-mqtttestdeviceprofileyml","text":"The DeviceProfile defines the device's values and operation method, which can be Read or Write. Create a device profile, named mqtt.test.device.profile.yml, with the following content: # mqtt . test . device . profile . yml name : \"Test.Device.MQTT.Profile\" manufacturer : \"iot\" model : \"MQTT-DEVICE\" description : \"Test device profile\" labels : - \"mqtt\" - \"test\" deviceResources : - name : randnum description : \"device random number\" properties : value : { type : \"Float64\" , size : \"4\" , readWrite : \"R\" , floatEncoding : \"eNotation\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"\" } - name : ping description : \"device awake\" properties : value : { type : \"String\" , size : \"0\" , readWrite : \"R\" , defaultValue : \"pong\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"\" } - name : message description : \"device message\" properties : value : { type : \"String\" , size : \"0\" , readWrite : \"W\" , scale : \"\" , offset : \"\" , base : \"\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"\" } deviceCommands : - name : testrandnum get : - { index : \"1\" , operation : \"get\" , object : \"randnum\" , parameter : \"randnum\" } - name : testping get : - { index : \"1\" , operation : \"get\" , object : \"ping\" , parameter : \"ping\" } - name : testmessage get : - { index : \"1\" , operation : \"get\" , object : \"message\" , parameter : \"message\" } set : - { index : \"1\" , operation : \"set\" , object : \"message\" , parameter : \"message\" } coreCommands : - name : testrandnum get : path : \"/api/v1/device/{deviceId}/testrandnum\" responses : - code : \"200\" description : \"get the random value\" expectedValues : [ \"randnum\" ] - code : \"503\" description : \"service unavailable\" expectedValues : [] - name : testping get : path : \"/api/v1/device/{deviceId}/testping\" responses : - code : \"200\" description : \"ping the device\" expectedValues : [ \"ping\" ] - code : \"503\" description : \"service unavailable\" expectedValues : [] - name : testmessage get : path : \"/api/v1/device/{deviceId}/testmessage\" responses : - code : \"200\" description : \"get the message\" expectedValues : [ \"message\" ] - code : \"503\" description : \"service unavailable\" expectedValues : [] put : path : \"/api/v1/device/{deviceId}/testmessage\" parameterNames : [ \"message\" ] responses : - code : \"204\" description : \"set the message.\" expectedValues : [] - code : \"503\" description : \"service unavailable\" expectedValues : []","title":"Device Profile (mqtt.test.device.profile.yml)"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#device-service-configuration-configurationtoml","text":"Use this configuration file to define devices and schedule jobs. device-mqtt generates a relative instance on start-up. MQTT is subscribe/publish pattern, so we must define the MQTT connection information in the [DeviceList.Protocols] section of the configuration file. Create the configuration file, named configuration.toml, as shown below replacing the host IP with your host address: # configuration . toml [ Writable ] LogLevel = 'DEBUG' [ Service ] Host = \"edgex-device-mqtt\" Port = 49982 ConnectRetries = 3 Labels = [] OpenMsg = \"device mqtt started\" Timeout = 5000 EnableAsyncReadings = true AsyncBufferSize = 16 [ Registry ] Host = \"edgex-core-consul\" Port = 8500 CheckInterval = \"10s\" FailLimit = 3 FailWaitTime = 10 Type = \"consul\" [ Logging ] EnableRemote = false File = \"./device-mqtt.log\" [ Clients ] [ Clients.Data ] Name = \"edgex-core-data\" Protocol = \"http\" Host = \"edgex-core-data\" Port = 48080 Timeout = 50000 [ Clients.Metadata ] Name = \"edgex-core-metadata\" Protocol = \"http\" Host = \"edgex-core-metadata\" Port = 48081 Timeout = 50000 [ Clients.Logging ] Name = \"edgex-support-logging\" Protocol = \"http\" Host = \"edgex-support-logging\" Port = 48061 [ Device ] DataTransform = true InitCmd = \"\" InitCmdArgs = \"\" MaxCmdOps = 128 MaxCmdValueLen = 256 RemoveCmd = \"\" RemoveCmdArgs = \"\" ProfilesDir = \"/custom-config\" # Pre - define Devices [ [DeviceList ] ] Name = \"MQ_DEVICE\" Profile = \"Test.Device.MQTT.Profile\" Description = \"General MQTT device\" Labels = [ \"MQTT\" ] [ DeviceList.Protocols ] [ DeviceList.Protocols.mqtt ] Schema = \"tcp\" Host = \"192.168.16.68\" Port = \"1883\" ClientId = \"CommandPublisher\" User = \"\" Password = \"\" Topic = \"CommandTopic\" [ [DeviceList.AutoEvents ] ] Frequency = \"30s\" OnChange = false Resource = \"testrandnum\" # Driver configs [ Driver ] IncomingSchema = \"tcp\" IncomingHost = \"192.168.16.68\" IncomingPort = \"1883\" IncomingUser = \"\" IncomingPassword = \"\" IncomingQos = \"0\" IncomingKeepAlive = \"3600\" IncomingClientId = \"IncomingDataSubscriber\" IncomingTopic = \"DataTopic\" ResponseSchema = \"tcp\" ResponseHost = \"192.168.16.68\" ResponsePort = \"1883\" ResponseUser = \"\" ResponsePassword = \"\" ResponseQos = \"0\" ResponseKeepAlive = \"3600\" ResponseClientId = \"CommandResponseSubscriber\" ResponseTopic = \"ResponseTopic\" In the Driver configs section : IncomingXxx defines the DataTopic for receiving an async value from the device ResponseXxx defines the ResponseTopic for receiving a command response from the device","title":"Device Service Configuration (configuration.toml)"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#add-device-service-to-docker-compose-file-docker-composeyml","text":"Download the Geneva release docker-compose file from https://github.com/edgexfoundry/developer-scripts/blob/master/releases/geneva/compose-files/docker-compose-geneva-redis.yml . Because we deploy EdgeX using docker-compose, we must add device-mqtt to the docker-compose file. If you have prepared configuration files, you can mount them using volumes and change the entrypoint for device-mqtt internal use. This is illustrated in the following docker-compose file snippet: device-mqtt: image: edgexfoundry/docker-device-mqtt-go:1.0.0 ports: - \"49982:49982\" container_name: edgex-device-mqtt hostname: edgex-device-mqtt networks: - edgex-network volumes: - db-data:/data/db - log-data:/edgex/logs - consul-config:/consul/config - consul-data:/consul/data - ./mqtt:/custom-config depends_on: - data - command entrypoint: - /device-mqtt - --registry=consul://edgex-core-consul:8500 - --confdir=/custom-config When using Device Services, the user has to provide the registry URL in --registry argument.","title":"Add Device Service to docker-compose File (docker-compose.yml)"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#start-edgex-foundry-on-docker","text":"Once the following folder has been populated, we can deploy EdgeX: - device-service-demo |- docker-compose.yml |- mqtt |- configuration.toml |- mqtt.test.device.profile.yml Deploy EdgeX using the following commands: cd path/to/device-service-demo docker-compose pull docker-compose up -d After the services start, check the consul dashboard as follows:","title":"Start EdgeX Foundry on Docker"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#execute-commands","text":"Now we're ready to run some commands.","title":"Execute Commands"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#find-executable-commands","text":"Use the following query to find executable commands: $ curl http : // your - edgex - server - ip : 48082 / api / v1 / device | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1972 100 1972 0 0 64349 0 --:--:-- --:--:-- --:--:-- 65733 [ { \"location\" : null , \"adminState\" : \"UNLOCKED\" , \"commands\" : [ { ... }, { ... }, { \"get\" : { \"responses\" : [ { \"code\" : \"503\" , \"description\" : \"service unavailable\" } ], \"path\" : \"/api/v1/device/{deviceId}/testmessage\" , \"url\" : \"http://edgex-core-command:48082/api/v1/device/ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff/command/0c257a37-2f72-4d23-b2b1-2c08e895060a\" }, \"modified\" : 1559195042046 , \"name\" : \"testmessage\" , \"put\" : { \"parameterNames\" : [ \"message\" ], \"path\" : \"/api/v1/device/{deviceId}/testmessage\" , \"url\" : \"http://edgex-core-command:48082/api/v1/device/ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff/command/0c257a37-2f72-4d23-b2b1-2c08e895060a\" }, \"created\" : 1559195042046 , \"id\" : \"0c257a37-2f72-4d23-b2b1-2c08e895060a\" } ], \"lastReported\" : 0 , \"operatingState\" : \"ENABLED\" , \"name\" : \"MQ_DEVICE\" , \"lastConnected\" : 0 , \"id\" : \"ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff\" , \"labels\" : [ \"MQTT\" ] } ]","title":"Find Executable Commands"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#execute-put-command","text":"Execute a put command according to the url and parameterNames, replacing [host] with the server IP when running the edgex-core-command. This can be done in either of the following ways: $ curl http://your-edgex-server-ip:48082/api/v1/device/ddb2f5cf-eec2-4345-86ee-f0d87e6f77ff/command/0c257a37-2f72-4d23-b2b1-2c08e895060a \\ -H \"Content-Type:application/json\" -X PUT \\ -d '{\"message\":\"Hello!\"}' or \\$ curl \" http://your-edgex-server-ip:48082/api/v1/device/name/MQ_DEVICE/command/testmessage \" -H \"Content-Type:application/json\" -X PUT -d '{\"message\":\"Hello!\"}'","title":"Execute put Command"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#execute-get-command","text":"Execute a get command as follows: $ curl \"http://your-edgex-server-ip:48082/api/v1/device/name/MQ_DEVICE/command/testmessage\" | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 139 100 139 0 0 132 0 0 : 00 : 01 0 : 00 : 01 --:--:-- 132 { \"readings\" : [ { \"name\" : \"message\" , \"device\" : \"MQ_DEVICE\" , \"value\" : \"Hello!\" , \"origin\" : 1559196276732 } ], \"device\" : \"MQ_DEVICE\" , \"origin\" : 1559196276738 }","title":"Execute get Command"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#schedule-job","text":"The schedule job is defined in the [[DeviceList.AutoEvents]] section of the TOML configuration file: # Pre - define Devices [ [DeviceList ] ] Name = \"MQ_DEVICE\" Profile = \"Test.Device.MQTT.Profile\" Description = \"General MQTT device\" Labels = [ \"MQTT\" ] [ DeviceList.Protocols ] [ DeviceList.Protocols.mqtt ] Schema = \"tcp\" Host = \"192.168.16.68\" Port = \"1883\" ClientId = \"CommandPublisher\" User = \"\" Password = \"\" Topic = \"CommandTopic\" [ [DeviceList.AutoEvents ] ] Frequency = \"30s\" OnChange = false Resource = \"testrandnum\" After the service starts, query core-data's reading API. The results show that the service auto-executes the command every 30 secs, as shown below: $ curl http : // your - edgex - server - ip : 48080 / api / v1 / reading | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1613 100 1613 0 0 372 k 0 --:--:-- --:--:-- --:--:-- 393 k [ { \"value\" : \"1.212300e+01\" , \"origin\" : 1559197206092 , \"modified\" : 1559197206104 , \"id\" : \"59f2a768-ad72-49a1-9df9-700d8599a890\" , \"created\" : 1559197206104 , \"device\" : \"MQ_DEVICE\" , \"name\" : \"randnum\" }, { ... }, { \"name\" : \"randnum\" , \"device\" : \"MQ_DEVICE\" , \"modified\" : 1559197175109 , \"created\" : 1559197175109 , \"id\" : \"f9dc39e0-5326-45d0-831d-fd0cd106fe2f\" , \"origin\" : 1559197175098 , \"value\" : \"1.212300e+01\" }, ]","title":"Schedule Job"},{"location":"examples/Ch-ExamplesAddingMQTTDevice/#async-device-reading","text":"device-mqtt subscribes to a DataTopic , which is wait for real device* to send value to broker , then device-mqtt parses the value and sends it back to core-data . The data format contains the following values: name = device name cmd = deviceResource name method = get or put cmd = device reading You must define this connection information in the driver configuration file, as follows: [Driver] IncomingSchema = \"tcp\" IncomingHost = \"192.168.16.68\" IncomingPort = \"1883\" IncomingUser = \"\" IncomingPassword = \"\" IncomingQos = \"0\" IncomingKeepAlive = \"3600\" IncomingClientId = \"IncomingDataSubscriber\" IncomingTopic = \"DataTopic\" The following results show that the mock device sent the reading every 15 secs: $ curl http : // your - edgex - server - ip : 48080 / api / v1 / reading | json_pp % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 539 100 539 0 0 169 k 0 --:--:-- --:--:-- --:--:-- 175 k [ { ... }, { \"name\" : \"randnum\" , \"created\" : 1559197140013 , \"origin\" : 1559197140006 , \"modified\" : 1559197140013 , \"id\" : \"286cc305-42f6-4bca-ad41-3af52301c9f7\" , \"value\" : \"2.830000e+01\" , \"device\" : \"MQ_DEVICE\" }, { \"modified\" : 1559197125011 , \"name\" : \"randnum\" , \"created\" : 1559197125011 , \"origin\" : 1559197125004 , \"device\" : \"MQ_DEVICE\" , \"value\" : \"2.690000e+01\" , \"id\" : \"c243e8c6-a904-4102-baff-8a5e4829c4f6\" } ]","title":"Async Device Reading"},{"location":"examples/Ch-ExamplesAddingModbusDevice/","text":"Modbus EdgeX - Delhi Release PowerScout 3037 Power Submeter https://shop.dentinstruments.com/products/powerscout-3037-ps3037 https://www.dentinstruments.com/hs-fs/hub/472997/file-2378482732-pdf/Pdf_Files/PS3037_Manual.pdf In this example, we simulate the PowerScout meter instead of using a real device. This provides a straight-forward way to test the device-modbus features. Environment You can use any operating system that can install docker and docker-compose. In this example, we use Photon OS to delpoy EdgeX using docker. The system requirements can be found at https://docs.edgexfoundry.org/Ch-GettingStartedUsers.html#what-you-need . Modbus Device (Simulator) http://modbuspal.sourceforge.net/ To simulate sensors, such as temperature and humidity, do the following: Add two mock devices: Add registers according to the device manual: Add the ModbusPal support value auto-generator, which can bind to registers: Set Up Before Starting Services The following sections describe how to complete the set up before starting the services. If you prefer to start the services and then add the device, see Set Up After Starting Services Set Up Device Profile The DeviceProfile defines the device's values and operation method, which can be Read or Write. You can download and use the provided modbus.test.device.profile.yml <modbus.test.device.profile.yml> {.interpreted-text role=\"download\"}. In the Modbus protocol, we must define attributes: primaryTable : HOLDING_REGISTERS, INPUT_REGISTERS, COILS, DISCRETES_INPUT startingAddress specifies the address in Modbus device The Property value type decides how many registers will be read. Like Holding registers, a register has 16 bits. If the device manual specifies that a value has two registers, define it as FLOAT32 or INT32 or UINT32 in the deviceProfile. Once we execute a command, device-modbus knows its value type and register type, startingAddress, and register length. So it can read or write value using the modbus protocol. | | Set Up Device Service Configuration Use this configuration file to define devices and schedule jobs. The device-modbus generates a relative instance on startup. device-modbus offers two types of protocol, Modbus TCP and Modbus RTU. An addressable can be defined as shown below: protocol Name Protocol Address Port Path Modbus TCP Gateway address TCP 10.211.55.6 502 1 1 Modbus RTU Gateway address RTU /tmp/slave,19200,8,1,0 502 2 2 Path defines the Modbus device's unit ID (or slave ID). In the RTU protocol, address is defined in five comma-separated parts: serial port baud rate data bits stop bits parity (N - None is 0, O - Odd is 1, E - Even is 2, default is E). [Logging] EnableRemote = false File = \"./device-Modbus.log\" Level = \"DEBUG\" [Device] DataTransform = true InitCmd = \"\" InitCmdArgs = \"\" MaxCmdOps = 128 MaxCmdValueLen = 256 RemoveCmd = \"\" RemoveCmdArgs = \"\" ProfilesDir = \"/custom-config\" # Pre-define Devices [[DeviceList]] Name = \"Modbus TCP test device\" Profile = \"Network Power Meter\" Description = \"This device is a product for monitoring and controlling digital inputs and outputs over a LAN.\" labels = [ \"Air conditioner\",\"modbus TCP\" ] [DeviceList.Addressable] name = \"Gateway address 1\" Protocol = \"TCP\" Address = \"10.211.55.6\" Port = 502 Path = \"1\" [[DeviceList]] Name = \"Modbus TCP test device 2\" Profile = \"Network Power Meter\" Description = \"This device is a product for monitoring and controlling digital inputs and outputs over a LAN.\" labels = [ \"Air conditioner\",\"modbus TCP\" ] [DeviceList.Addressable] name = \"Gateway address 1\" Protocol = \"TCP\" Address = \"10.211.55.6\" Port = 502 Path = \"2\" # Pre-define Schedule Configuration : [[Schedules]] Name = \"20sec-schedule\" Frequency = \"PT20S\" \\ [ \\ [ ScheduleEvents \\ ] \\ ] Name = \"Read Switch status\" Schedule = \"20sec-schedule\" \\ [ ScheduleEvents . Addressable \\ ] HTTPMethod = \"GET\" Path = \"/api/v1/device/name/Modbus TCP test device 1/Configuration\" \\ [ \\ [ ScheduleEvents \\ ] \\ ] Name = \"Put Configuration\" Parameters = \"\\[{\" DemandWindowSize \": \" 110 \"},{\" LineFrequency \": \" 50 \"}\\]\" Schedule = \"20sec-schedule\" \\ [ ScheduleEvents . Addressable \\ ] HTTPMethod = \"Put\" Path = \"/api/v1/device/name/Modbus TCP test device 1/Configuration\" You can download and use the provided EdgeX_ExampleModbus_configuration.toml <EdgeX_ExampleModbus_configuration.toml> {.interpreted-text role=\"download\"}. Add Device Service to docker-compose File Because we deploy EdgeX using docker-compose, we must add the device-modbus to the docker-compose file ( https://github.com/edgexfoundry/developer-scripts/blob/master/releases/geneva/compose-files/docker-compose-geneva-redis.yml ). If you have prepared configuration files, you can mount them using volumes and change the entrypoint for device-modbus internal use. Start EdgeX Foundry on Docker Finally, we can deploy EdgeX in the Photon OS. Prepare configuration files by moving the files to the Photon OS Deploy EdgeX using the following commands: docker-compose pull docker-compose up -d Check the consul dashboard Set Up After Starting Services If the services are already running and you want to add a device, you can use the Core Metadata API as outlined in this section. If you set up the device profile and Service as described in Set Up Before Starting Services , you can skip this section. To add a device after starting the services, complete the following steps: Upload the device profile above to metadata with a POST to http://localhost:48081/api/v1/deviceprofile/uploadfile and add the file as key \"file\" to the body in form-data format, and the created ID will be returned. The following figure is an example if you use Postman to send the request Add the addressable containing reachability information for the device with a POST to http://localhost:48081/api/v1/addressable : a. If IP connected, the body will look something like: { \"name\": \"Motor\", \"method\": \"GET\", \"protocol\": \"HTTP\", \"address\": \"10.0.1.29\", \"port\": 502 } b. If serially connected, the body will look something like: { \"name\": \"Motor\", \"method\": \"GET\", \"protocol\": \"OTHER\", \"address\": \"/dev/ttyS5,9600,8,1,1\", \"port\": 0 } (address field contains port, baud rate, number of data bits, stop bits, and parity bits in CSV form) Ensure the Modbus device service is running, adjust the service name below to match if necessary or if using other device services. Add the device with a POST to http://localhost:48081/api/v1/device , the body will look something like: { \"description\" : \"MicroMax Variable Speed Motor\" , \"name\" : \"Variable Speed motor\" , \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"addressable\" : { \"name\" : \"Motor\" } , \"labels\" : [ ], \"location\" : null , \"service\" : { \"name\" : \"edgex-device-modbus\" } , \"profile\" : { \"name\" : \"GS1-VariableSpeedMotor\" } } The addressable name must match/refer to the addressable added in Step 2, the service name must match/refer to the target device service, and the profile name must match the device profile name from Step 1. Execute Commands Now we're ready to run some commands. Find Executable Commands Use the following query to find executable commands: photon-ip:48082/api/v1/device | Execute GET command Replace \\<host> with the server IP when running the edgex-core-command. Execute PUT command Execute PUT command according to url and parameterNames . Schedule Job After service startup, query core-data's reading API. The results show that the service auto-executes the command every 20 seconds.","title":"Modbus"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#modbus","text":"EdgeX - Delhi Release PowerScout 3037 Power Submeter https://shop.dentinstruments.com/products/powerscout-3037-ps3037 https://www.dentinstruments.com/hs-fs/hub/472997/file-2378482732-pdf/Pdf_Files/PS3037_Manual.pdf In this example, we simulate the PowerScout meter instead of using a real device. This provides a straight-forward way to test the device-modbus features.","title":"Modbus"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#environment","text":"You can use any operating system that can install docker and docker-compose. In this example, we use Photon OS to delpoy EdgeX using docker. The system requirements can be found at https://docs.edgexfoundry.org/Ch-GettingStartedUsers.html#what-you-need .","title":"Environment"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#modbus-device-simulator","text":"http://modbuspal.sourceforge.net/ To simulate sensors, such as temperature and humidity, do the following: Add two mock devices: Add registers according to the device manual: Add the ModbusPal support value auto-generator, which can bind to registers:","title":"Modbus Device (Simulator)"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#set-up-before-starting-services","text":"The following sections describe how to complete the set up before starting the services. If you prefer to start the services and then add the device, see Set Up After Starting Services","title":"Set Up Before Starting Services"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#set-up-device-profile","text":"The DeviceProfile defines the device's values and operation method, which can be Read or Write. You can download and use the provided modbus.test.device.profile.yml <modbus.test.device.profile.yml> {.interpreted-text role=\"download\"}. In the Modbus protocol, we must define attributes: primaryTable : HOLDING_REGISTERS, INPUT_REGISTERS, COILS, DISCRETES_INPUT startingAddress specifies the address in Modbus device The Property value type decides how many registers will be read. Like Holding registers, a register has 16 bits. If the device manual specifies that a value has two registers, define it as FLOAT32 or INT32 or UINT32 in the deviceProfile. Once we execute a command, device-modbus knows its value type and register type, startingAddress, and register length. So it can read or write value using the modbus protocol.","title":"Set Up Device Profile"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#_1","text":"","title":"|"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#_2","text":"","title":"|"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#set-up-device-service-configuration","text":"Use this configuration file to define devices and schedule jobs. The device-modbus generates a relative instance on startup. device-modbus offers two types of protocol, Modbus TCP and Modbus RTU. An addressable can be defined as shown below: protocol Name Protocol Address Port Path Modbus TCP Gateway address TCP 10.211.55.6 502 1 1 Modbus RTU Gateway address RTU /tmp/slave,19200,8,1,0 502 2 2 Path defines the Modbus device's unit ID (or slave ID). In the RTU protocol, address is defined in five comma-separated parts: serial port baud rate data bits stop bits parity (N - None is 0, O - Odd is 1, E - Even is 2, default is E). [Logging] EnableRemote = false File = \"./device-Modbus.log\" Level = \"DEBUG\" [Device] DataTransform = true InitCmd = \"\" InitCmdArgs = \"\" MaxCmdOps = 128 MaxCmdValueLen = 256 RemoveCmd = \"\" RemoveCmdArgs = \"\" ProfilesDir = \"/custom-config\" # Pre-define Devices [[DeviceList]] Name = \"Modbus TCP test device\" Profile = \"Network Power Meter\" Description = \"This device is a product for monitoring and controlling digital inputs and outputs over a LAN.\" labels = [ \"Air conditioner\",\"modbus TCP\" ] [DeviceList.Addressable] name = \"Gateway address 1\" Protocol = \"TCP\" Address = \"10.211.55.6\" Port = 502 Path = \"1\" [[DeviceList]] Name = \"Modbus TCP test device 2\" Profile = \"Network Power Meter\" Description = \"This device is a product for monitoring and controlling digital inputs and outputs over a LAN.\" labels = [ \"Air conditioner\",\"modbus TCP\" ] [DeviceList.Addressable] name = \"Gateway address 1\" Protocol = \"TCP\" Address = \"10.211.55.6\" Port = 502 Path = \"2\" # Pre-define Schedule Configuration : [[Schedules]] Name = \"20sec-schedule\" Frequency = \"PT20S\" \\ [ \\ [ ScheduleEvents \\ ] \\ ] Name = \"Read Switch status\" Schedule = \"20sec-schedule\" \\ [ ScheduleEvents . Addressable \\ ] HTTPMethod = \"GET\" Path = \"/api/v1/device/name/Modbus TCP test device 1/Configuration\" \\ [ \\ [ ScheduleEvents \\ ] \\ ] Name = \"Put Configuration\" Parameters = \"\\[{\" DemandWindowSize \": \" 110 \"},{\" LineFrequency \": \" 50 \"}\\]\" Schedule = \"20sec-schedule\" \\ [ ScheduleEvents . Addressable \\ ] HTTPMethod = \"Put\" Path = \"/api/v1/device/name/Modbus TCP test device 1/Configuration\" You can download and use the provided EdgeX_ExampleModbus_configuration.toml <EdgeX_ExampleModbus_configuration.toml> {.interpreted-text role=\"download\"}.","title":"Set Up Device Service Configuration"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#add-device-service-to-docker-compose-file","text":"Because we deploy EdgeX using docker-compose, we must add the device-modbus to the docker-compose file ( https://github.com/edgexfoundry/developer-scripts/blob/master/releases/geneva/compose-files/docker-compose-geneva-redis.yml ). If you have prepared configuration files, you can mount them using volumes and change the entrypoint for device-modbus internal use.","title":"Add Device Service to docker-compose File"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#start-edgex-foundry-on-docker","text":"Finally, we can deploy EdgeX in the Photon OS. Prepare configuration files by moving the files to the Photon OS Deploy EdgeX using the following commands: docker-compose pull docker-compose up -d Check the consul dashboard","title":"Start EdgeX Foundry on Docker"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#set-up-after-starting-services","text":"If the services are already running and you want to add a device, you can use the Core Metadata API as outlined in this section. If you set up the device profile and Service as described in Set Up Before Starting Services , you can skip this section. To add a device after starting the services, complete the following steps: Upload the device profile above to metadata with a POST to http://localhost:48081/api/v1/deviceprofile/uploadfile and add the file as key \"file\" to the body in form-data format, and the created ID will be returned. The following figure is an example if you use Postman to send the request Add the addressable containing reachability information for the device with a POST to http://localhost:48081/api/v1/addressable : a. If IP connected, the body will look something like: { \"name\": \"Motor\", \"method\": \"GET\", \"protocol\": \"HTTP\", \"address\": \"10.0.1.29\", \"port\": 502 } b. If serially connected, the body will look something like: { \"name\": \"Motor\", \"method\": \"GET\", \"protocol\": \"OTHER\", \"address\": \"/dev/ttyS5,9600,8,1,1\", \"port\": 0 } (address field contains port, baud rate, number of data bits, stop bits, and parity bits in CSV form) Ensure the Modbus device service is running, adjust the service name below to match if necessary or if using other device services. Add the device with a POST to http://localhost:48081/api/v1/device , the body will look something like: { \"description\" : \"MicroMax Variable Speed Motor\" , \"name\" : \"Variable Speed motor\" , \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"addressable\" : { \"name\" : \"Motor\" } , \"labels\" : [ ], \"location\" : null , \"service\" : { \"name\" : \"edgex-device-modbus\" } , \"profile\" : { \"name\" : \"GS1-VariableSpeedMotor\" } } The addressable name must match/refer to the addressable added in Step 2, the service name must match/refer to the target device service, and the profile name must match the device profile name from Step 1.","title":"Set Up After Starting Services"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#execute-commands","text":"Now we're ready to run some commands.","title":"Execute Commands"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#find-executable-commands","text":"Use the following query to find executable commands: photon-ip:48082/api/v1/device |","title":"Find Executable Commands"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#execute-get-command","text":"Replace \\<host> with the server IP when running the edgex-core-command.","title":"Execute GET command"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#execute-put-command","text":"Execute PUT command according to url and parameterNames .","title":"Execute PUT command"},{"location":"examples/Ch-ExamplesAddingModbusDevice/#schedule-job","text":"After service startup, query core-data's reading API. The results show that the service auto-executes the command every 20 seconds.","title":"Schedule Job"},{"location":"examples/Ch-ExamplesAddingSNMPDevice/","text":"SNMP EdgeX - Barcelona Release Ubuntu Desktop 16.04 with Docker/Docker-Compose Adding a new SNMP Device Moxa ioLogik E2210 Smart Ethernet Remote I/O with 12 DIs, 8 Dos Project Components Needed Hardware needed X86 computer with native RS485 communication device or RS485 adapter Moxa E2210 Ethernet IO -- https://www.moxa.com/product/ioLogik-E2210.htm Software needed Ubuntu Desktop 16.04 - new installation The following software was installed via the \"apt-get install\" command (ubuntu default) git curl vim (or your favorite editor) java (I used openjdk-8-jdk - 1.8.0_131) maven docker docker-compose The following software was installed from 3rd parties Postman (Linux 64bit) -- https://www.getpostman.com/ EdgeX - barcelona-docker-compose.yaml -- https://github.com/edgexfoundry/developer-scripts/blob/master/releases/barcelona/compose-files/docker-compose-barcelona-0.2.1.yml SNMP - Device documentation Device: Moxa E2210 (Smart Ethernet Remote I/O with 12 DIs, 8 DOs) https://www.moxa.com/product/ioLogik-E2210.htm Ensuring success Verify the following, prior to following the instruction on the following pages Do you know the IP address of the E2210? Do you know what port number of the E2210 is using? Does the E2210 power on? With a separate utility, can you read(from)/write(to) the E2210? Creating the Modbus yaml file An example SNMP device yaml file can be found here: SNMP device yaml . The SNMP device yaml file used in this example can be found here: this example SNMP device yaml . When you are creating your yaml file you will need to know what command options are available to use, they can be found here: https://github.com/edgexfoundry/core-domain/blob/master/src/main/java/org/edgexfoundry/domain/meta/PropertyValue.java With your favorite file editor, open the file Modify the following fields name \\<-- A/a \\~Z/z and 0 \\~ 9 && this will be needed in the future manufacturer \\<-- A/a \\~Z/z and 0 \\~ 9 model \\<-- A/a \\~Z/z and 0 \\~ 9 description \\<-- A/a \\~Z/z and 0 \\~ 9 labels \\<-- A/a \\~Z/z and 0 \\~ 9 deviceResources name: \\<-- A/a \\~Z/z and 0 \\~ 9 description: \\<-- A/a \\~Z/z and 0 \\~ 9 attributes: only edit the text inside the parenthesis value: only edit the text inside the parenthesis units: only edit the text inside the parenthesis resources name: \\<-- A/a \\~Z/z and 0 \\~ 9 get : only edit the text inside the parenthesis set: only edit the text inside the parenthesis commands name: \\<-- A/a \\~Z/z and 0 \\~ 9 path: \"/api/v1/device/{deviceId}/OnlyEditThisWord\" \\<-- A/a \\~Z/z and 0 \\~ 9 Code \"200\" expectedvalues: [make same as OnlyEditThisWord] Code \"500\" Do not edit this section Bringing up EdgeX via Docker Starting with following system configuration: A fresh installation of Ubuntu Desktop 16.04 with all the available system updates. A working directory > /home/tester/Development/edgex Verify your Docker installation Verify that Docker is installed and working as expected. >\\$ sudo docker run hello-world Verify that the image is on the system >\\$ sudo docker ps -a Download docker-compose file Download the barcelona-docker-compose.yaml file from the EdgeX Wiki Go to \" https://wiki.edgexfoundry.org/display/FA/Barcelona \" Scroll to the bottom a look for the \"barcelona-docker-compose.yml\" file. Once downloaded, rename the file to \"docker-compose.yml\" Once the file is download, move the file into your desired working directory. Create a copy of the file and rename the copy \"docker-compose.yml\" Verify the version of dockerized EdgeX that you will be running With your favorite file editor, open the docker-compose.yml file Within the first couple of lines you will see the word \"Version\", next to that you will see a number - it should be \"2\". Version 2 refers to the Barcelona release Enable SNMP in the Docker Compose file With your favorite file editor, open the docker-compose file Find the section \"device-snmp\" section, which will be commented out with \"#\" symbols. Uncomment the entire section Save your changes and exit out of the editor Starting EdgeX Docker components Start EdgeX by using the following commands Docker Command Description Suggested Wait Time After Completing docker-compose pull Pull down, but don\u2019t start, all the EdgeX Foundry microservices Docker Compose will indicate when all the containers have been pulled successfully docker-compose up -d volume Start the EdgeX Foundry file volume\u2013must be done before the other services are started A couple of seconds docker-compose up -d config-seed Start and populate the configuration/registry microservice which all services must register with and get their configuration from 60 seconds docker-compose up -d mongo Start the NoSQL MongoDB container 10 seconds docker-compose up -d logging Start the logging microservice - used by all micro services that make log entries 1 minute docker-compose up -d notifications Start the notifications and alerts microservice\u2013used by many of the microservices 30 seconds docker-compose up -d metadata Start the Core Metadata microservice 1 minute docker-compose up -d data Start the Core Data microservice 1 minute docker-compose up -d command Start the Core Command microservice 1 minute docker-compose up -d scheduler Start the scheduling microservice -used by many of the microservices 1 minute docker-compose up -d export -client Start the Export Client registration microservice 1 minute docker-compose up -d export -distro Start the Export Distribution microservice 1 minute docker-compose up -d rulesengine Start the Rules Engine microservice 1 minute docker-compose up -d device -virtual Start the virtual device service 1 minute docker-compose up -d device -snmp Start the SNMP device service 1 minute Check the containers status Run a \"docker ps -a\" command to confirm that all the containers have been downloaded and started Show containers To get a list of all the EdgeX containers, you can use \"docker-compose config --services\" Stop Containers To stop (but not remove) all containers, issue \"docker-compose stop\". To stop an individual container, you can use \"docker-compose stop [compose-container-name]\". Start Containers To start all the containers (after a stop) issue \"docker-compose start\" to re-start To start an individual container, you can use \"docker-compose start [compose-container-name]\" (after that container has been stopped). Delete Containers * DANGER * To stop all the containers running and DELETE them, you can use \"docker-compose down\" EdgeX Foundry Container Logs To view the log of any container, use the command: \"docker-compose logs -f compose-container-name\" (ex. docker-compose logs -f edgex-device-snmp) At this point the Dockerized version of EdgeX is running. Adding the Device to EdgeX Importing APIs In this section you will be using the program Postman to interact with EdgeX. You will also need to have the file \"core-metadata.raml\" available to load into the Postman application. The file \"core-metadata.raml\" can be found here: \"edgex/core-metadata..../src/test/resources/raml/core-metadata.raml\" Viewing available APIs Open Postman Click on the Import button Add the file to the import dialog box - the application will take a about 30 seconds to digest the file you added. If a list of API commands do not show up on the left hand side of the application then click on the \"Collections\" tab to the right of the \"History\" tab. Create an addressable In the \"Collections\" tab, select the option \"POST /addressable action Open the body tab Modify its contents name: moxa-e2210-address protocol: HTTP (needs to be in ALL CAPS) address: 192.168.1.103 (IPV4 address) port: 161 (port # of snmp) path: empty (remove all text between parentheses) publisher, user, password, topic - do not need to be modified Press the \"Send\" button when you are finished Note the addressable id Upload the profile In the \"Collections\" tab select the option \"POST /deviceprofile/uploadfile Open the body tab Under \"Key\", look for the drop down menu for \"text\". Be sure to write \"file\" in the open box. Under \"Value\" click \"Choose Files\", locate your profile file. Press Upload Press the \"Send\" button when you are finished Note the profile id Post the device In the \"Collections\" tab select the option \"POST /device Click on the \"Body\" tab Modify its contents There are three components that are required to be modified. They are: \"Service\" \"Profile\" \"Addressable\" The others can be modified, however they are not required for operation name: moxa-e2210-device description: snmp smart ethernet io addressable: name: moxa-e2210-address (same as used in addressable) labels: labels: \"snmp\", \"rtu\",\"io\" (same as used in snmp device profile) service: name: edgex-device-snmp profile: name: name: moxa-iologik-e2210 (same as used in snmp device profile) Press the \"Send\" button when you are finished Note the addressable id What if a Mistake is Made Get device id Delete device id Get device profile id Delete device profile id Get addressable id Delete addressable id Verify Device Added Check the edgex-device-snmp logs to see if the device was added without issue \"sudo docker logs -f --tail 100 edgex-device-snmp\" Verify device is sending data In the \"Collections\" tab select the option \"GET /device Change the port number form \"48081\" http://localhost:48081/api/v1/device to port number \"48082\" http://localhost:48082/api/v1/device Press Send You should see something similar to { \"id\" : \"5a1d6f8ae4b0c3936013120f\" , \"name\" : \"diStatus0\" , \"get\" : { \"url\" : \"http://localhost:48082/api/v1/device/5a1d7134e4b0c39360131212/command/5a1d6f8ae4b0c3936013120f\" , < -- This \"responses\" : [ { \"code\" : \"200\" , \"description\" : \"Get di 0 Status.\" , \"expectedValues\" : [ \"diStatus0\" ] } , { \"code\" : \"503\" , \"description\" : \"service unavailable\" , \"expectedValues\" : [] } ] } , \"put\" : null } , Double click on the \"url\" and a new tab within Postman should open, Press Send If all went well you should see something similar to the following: {\"diStatus0\":\"0\"} If all did not go well the you will see an error or may \"{ }\" then you will need check the information you entered. If the data/result displayed was as expected, go ahead and proceed to creating a scheduled event Creating a Scheduled Event This is used to regularly get & push data to another service or for regularly viewing data. Gathering information for the addressable Got to http://localhost:48082/api/v1/device Look for the id or the device that you want to schedule an event for [ { \"name\" : \"moxa-e2210-device\" , \"id\" : \"5a280a0be4b0c39393ec7780\" , < --- This \"description\" : \"snmp smart ethernet io\" , \"labels\" : [ \"snmp\" , \"rtu\" , \"io\" ], \"adminState\" : \"unlocked\" , In this case the id is \"5a280a0be4b0c39393ec7780\" Next you want to get the \"name\" of the command you want to schedule an event for \"commands\" : [ { \"id\" : \"5a2808e6e4b0c39393ec777c\" , \"name\" : \"serverModel\" , \"get\" : { \"url\" : \"http://localhost:48082/api/v1/device/5a280a0be4b0c39393ec7780/command/5a2808e6e4b0c39393ec777c\" , \"responses\" : [ { \"code\" : \"200\" , \"description\" : \"Get server model number.\" , \"expectedValues\" : [ \"serverModel\" ] } , { \"code\" : \"503\" , \"description\" : \"service unavailable\" , \"expectedValues\" : [] } ] } , \"put\" : null } , { \"id\" : \"5a2808e6e4b0c39393ec777d\" , \"name\" : \"diStatus0\" , < --- This \"get\" : { \"url\" : \"http://localhost:48082/api/v1/device/5a280a0be4b0c39393ec7780/command/5a2808e6e4b0c39393ec777d\" , \"responses\" : [ { \"code\" : \"200\" , \"description\" : \"Get di 0 Status.\" , \"expectedValues\" : [ \"diStatus0\" ] In this example the name is \"diStatus0\". Create addressable In this section you will need to supply a path the the item you want to schedule. The path outline is: /api/v1/device/{device id}/{command name} In this case, the address would be / api / v1 / device / XXXX / diStatus0 / POST addressable \u201c name \u201d : \u201d schedule - moxa - di \u201d \u201c protocol \u201d : \u201c HTTP \u201d \u201c address \u201d : \u201c edgex - device - snmp \u201d \u201c port \u201d : \u201c xxxxx \u201d \u201c path \u201d : \u201d / api / v1 / device / { device id } / { command name }\u201d \u201c method \u201d : \u201c GET \u201d *** This will need to be added *** Create a schedule / POST schedule \u201c name \u201d : \u201d interval - moxa - di0 \u201d \u201c start \u201d : null ( remove parenthesis and replace ) \u201c end \u201d : null ( remove parenthesis and replace ) \u201c frequency \u201d : \u201c PT5S \u201d Create an event that will use the schedule / POST scheduleevent \u201c name \u201d : \u201c device - moxa - di0 \u201d \u201c addressable \u201d : {\u201c name \u201d : \u201d schedule - moxa - di \u201d} \u201c schedule \u201d : \u201d interval - moxa - di0 \u201d \u201c service \u201d : \u201c edgex - device - snmp \u201d *** This will need to be added ***","title":"SNMP"},{"location":"examples/Ch-ExamplesAddingSNMPDevice/#snmp","text":"EdgeX - Barcelona Release Ubuntu Desktop 16.04 with Docker/Docker-Compose Adding a new SNMP Device Moxa ioLogik E2210 Smart Ethernet Remote I/O with 12 DIs, 8 Dos","title":"SNMP"},{"location":"examples/Ch-ExamplesAddingSNMPDevice/#project-components-needed","text":"Hardware needed X86 computer with native RS485 communication device or RS485 adapter Moxa E2210 Ethernet IO -- https://www.moxa.com/product/ioLogik-E2210.htm Software needed Ubuntu Desktop 16.04 - new installation The following software was installed via the \"apt-get install\" command (ubuntu default) git curl vim (or your favorite editor) java (I used openjdk-8-jdk - 1.8.0_131) maven docker docker-compose The following software was installed from 3rd parties Postman (Linux 64bit) -- https://www.getpostman.com/ EdgeX - barcelona-docker-compose.yaml -- https://github.com/edgexfoundry/developer-scripts/blob/master/releases/barcelona/compose-files/docker-compose-barcelona-0.2.1.yml SNMP - Device documentation Device: Moxa E2210 (Smart Ethernet Remote I/O with 12 DIs, 8 DOs) https://www.moxa.com/product/ioLogik-E2210.htm Ensuring success Verify the following, prior to following the instruction on the following pages Do you know the IP address of the E2210? Do you know what port number of the E2210 is using? Does the E2210 power on? With a separate utility, can you read(from)/write(to) the E2210? Creating the Modbus yaml file An example SNMP device yaml file can be found here: SNMP device yaml . The SNMP device yaml file used in this example can be found here: this example SNMP device yaml . When you are creating your yaml file you will need to know what command options are available to use, they can be found here: https://github.com/edgexfoundry/core-domain/blob/master/src/main/java/org/edgexfoundry/domain/meta/PropertyValue.java With your favorite file editor, open the file Modify the following fields name \\<-- A/a \\~Z/z and 0 \\~ 9 && this will be needed in the future manufacturer \\<-- A/a \\~Z/z and 0 \\~ 9 model \\<-- A/a \\~Z/z and 0 \\~ 9 description \\<-- A/a \\~Z/z and 0 \\~ 9 labels \\<-- A/a \\~Z/z and 0 \\~ 9 deviceResources name: \\<-- A/a \\~Z/z and 0 \\~ 9 description: \\<-- A/a \\~Z/z and 0 \\~ 9 attributes: only edit the text inside the parenthesis value: only edit the text inside the parenthesis units: only edit the text inside the parenthesis resources name: \\<-- A/a \\~Z/z and 0 \\~ 9 get : only edit the text inside the parenthesis set: only edit the text inside the parenthesis commands name: \\<-- A/a \\~Z/z and 0 \\~ 9 path: \"/api/v1/device/{deviceId}/OnlyEditThisWord\" \\<-- A/a \\~Z/z and 0 \\~ 9 Code \"200\" expectedvalues: [make same as OnlyEditThisWord] Code \"500\" Do not edit this section Bringing up EdgeX via Docker Starting with following system configuration: A fresh installation of Ubuntu Desktop 16.04 with all the available system updates. A working directory > /home/tester/Development/edgex Verify your Docker installation Verify that Docker is installed and working as expected. >\\$ sudo docker run hello-world Verify that the image is on the system >\\$ sudo docker ps -a Download docker-compose file Download the barcelona-docker-compose.yaml file from the EdgeX Wiki Go to \" https://wiki.edgexfoundry.org/display/FA/Barcelona \" Scroll to the bottom a look for the \"barcelona-docker-compose.yml\" file. Once downloaded, rename the file to \"docker-compose.yml\" Once the file is download, move the file into your desired working directory. Create a copy of the file and rename the copy \"docker-compose.yml\" Verify the version of dockerized EdgeX that you will be running With your favorite file editor, open the docker-compose.yml file Within the first couple of lines you will see the word \"Version\", next to that you will see a number - it should be \"2\". Version 2 refers to the Barcelona release Enable SNMP in the Docker Compose file With your favorite file editor, open the docker-compose file Find the section \"device-snmp\" section, which will be commented out with \"#\" symbols. Uncomment the entire section Save your changes and exit out of the editor Starting EdgeX Docker components Start EdgeX by using the following commands Docker Command Description Suggested Wait Time After Completing docker-compose pull Pull down, but don\u2019t start, all the EdgeX Foundry microservices Docker Compose will indicate when all the containers have been pulled successfully docker-compose up -d volume Start the EdgeX Foundry file volume\u2013must be done before the other services are started A couple of seconds docker-compose up -d config-seed Start and populate the configuration/registry microservice which all services must register with and get their configuration from 60 seconds docker-compose up -d mongo Start the NoSQL MongoDB container 10 seconds docker-compose up -d logging Start the logging microservice - used by all micro services that make log entries 1 minute docker-compose up -d notifications Start the notifications and alerts microservice\u2013used by many of the microservices 30 seconds docker-compose up -d metadata Start the Core Metadata microservice 1 minute docker-compose up -d data Start the Core Data microservice 1 minute docker-compose up -d command Start the Core Command microservice 1 minute docker-compose up -d scheduler Start the scheduling microservice -used by many of the microservices 1 minute docker-compose up -d export -client Start the Export Client registration microservice 1 minute docker-compose up -d export -distro Start the Export Distribution microservice 1 minute docker-compose up -d rulesengine Start the Rules Engine microservice 1 minute docker-compose up -d device -virtual Start the virtual device service 1 minute docker-compose up -d device -snmp Start the SNMP device service 1 minute Check the containers status Run a \"docker ps -a\" command to confirm that all the containers have been downloaded and started Show containers To get a list of all the EdgeX containers, you can use \"docker-compose config --services\" Stop Containers To stop (but not remove) all containers, issue \"docker-compose stop\". To stop an individual container, you can use \"docker-compose stop [compose-container-name]\". Start Containers To start all the containers (after a stop) issue \"docker-compose start\" to re-start To start an individual container, you can use \"docker-compose start [compose-container-name]\" (after that container has been stopped). Delete Containers * DANGER * To stop all the containers running and DELETE them, you can use \"docker-compose down\" EdgeX Foundry Container Logs To view the log of any container, use the command: \"docker-compose logs -f compose-container-name\" (ex. docker-compose logs -f edgex-device-snmp) At this point the Dockerized version of EdgeX is running. Adding the Device to EdgeX Importing APIs In this section you will be using the program Postman to interact with EdgeX. You will also need to have the file \"core-metadata.raml\" available to load into the Postman application. The file \"core-metadata.raml\" can be found here: \"edgex/core-metadata..../src/test/resources/raml/core-metadata.raml\" Viewing available APIs Open Postman Click on the Import button Add the file to the import dialog box - the application will take a about 30 seconds to digest the file you added. If a list of API commands do not show up on the left hand side of the application then click on the \"Collections\" tab to the right of the \"History\" tab. Create an addressable In the \"Collections\" tab, select the option \"POST /addressable action Open the body tab Modify its contents name: moxa-e2210-address protocol: HTTP (needs to be in ALL CAPS) address: 192.168.1.103 (IPV4 address) port: 161 (port # of snmp) path: empty (remove all text between parentheses) publisher, user, password, topic - do not need to be modified Press the \"Send\" button when you are finished Note the addressable id Upload the profile In the \"Collections\" tab select the option \"POST /deviceprofile/uploadfile Open the body tab Under \"Key\", look for the drop down menu for \"text\". Be sure to write \"file\" in the open box. Under \"Value\" click \"Choose Files\", locate your profile file. Press Upload Press the \"Send\" button when you are finished Note the profile id Post the device In the \"Collections\" tab select the option \"POST /device Click on the \"Body\" tab Modify its contents There are three components that are required to be modified. They are: \"Service\" \"Profile\" \"Addressable\" The others can be modified, however they are not required for operation name: moxa-e2210-device description: snmp smart ethernet io addressable: name: moxa-e2210-address (same as used in addressable) labels: labels: \"snmp\", \"rtu\",\"io\" (same as used in snmp device profile) service: name: edgex-device-snmp profile: name: name: moxa-iologik-e2210 (same as used in snmp device profile) Press the \"Send\" button when you are finished Note the addressable id What if a Mistake is Made Get device id Delete device id Get device profile id Delete device profile id Get addressable id Delete addressable id Verify Device Added Check the edgex-device-snmp logs to see if the device was added without issue \"sudo docker logs -f --tail 100 edgex-device-snmp\" Verify device is sending data In the \"Collections\" tab select the option \"GET /device Change the port number form \"48081\" http://localhost:48081/api/v1/device to port number \"48082\" http://localhost:48082/api/v1/device Press Send You should see something similar to { \"id\" : \"5a1d6f8ae4b0c3936013120f\" , \"name\" : \"diStatus0\" , \"get\" : { \"url\" : \"http://localhost:48082/api/v1/device/5a1d7134e4b0c39360131212/command/5a1d6f8ae4b0c3936013120f\" , < -- This \"responses\" : [ { \"code\" : \"200\" , \"description\" : \"Get di 0 Status.\" , \"expectedValues\" : [ \"diStatus0\" ] } , { \"code\" : \"503\" , \"description\" : \"service unavailable\" , \"expectedValues\" : [] } ] } , \"put\" : null } , Double click on the \"url\" and a new tab within Postman should open, Press Send If all went well you should see something similar to the following: {\"diStatus0\":\"0\"} If all did not go well the you will see an error or may \"{ }\" then you will need check the information you entered. If the data/result displayed was as expected, go ahead and proceed to creating a scheduled event Creating a Scheduled Event This is used to regularly get & push data to another service or for regularly viewing data. Gathering information for the addressable Got to http://localhost:48082/api/v1/device Look for the id or the device that you want to schedule an event for [ { \"name\" : \"moxa-e2210-device\" , \"id\" : \"5a280a0be4b0c39393ec7780\" , < --- This \"description\" : \"snmp smart ethernet io\" , \"labels\" : [ \"snmp\" , \"rtu\" , \"io\" ], \"adminState\" : \"unlocked\" , In this case the id is \"5a280a0be4b0c39393ec7780\" Next you want to get the \"name\" of the command you want to schedule an event for \"commands\" : [ { \"id\" : \"5a2808e6e4b0c39393ec777c\" , \"name\" : \"serverModel\" , \"get\" : { \"url\" : \"http://localhost:48082/api/v1/device/5a280a0be4b0c39393ec7780/command/5a2808e6e4b0c39393ec777c\" , \"responses\" : [ { \"code\" : \"200\" , \"description\" : \"Get server model number.\" , \"expectedValues\" : [ \"serverModel\" ] } , { \"code\" : \"503\" , \"description\" : \"service unavailable\" , \"expectedValues\" : [] } ] } , \"put\" : null } , { \"id\" : \"5a2808e6e4b0c39393ec777d\" , \"name\" : \"diStatus0\" , < --- This \"get\" : { \"url\" : \"http://localhost:48082/api/v1/device/5a280a0be4b0c39393ec7780/command/5a2808e6e4b0c39393ec777d\" , \"responses\" : [ { \"code\" : \"200\" , \"description\" : \"Get di 0 Status.\" , \"expectedValues\" : [ \"diStatus0\" ] In this example the name is \"diStatus0\". Create addressable In this section you will need to supply a path the the item you want to schedule. The path outline is: /api/v1/device/{device id}/{command name} In this case, the address would be / api / v1 / device / XXXX / diStatus0 / POST addressable \u201c name \u201d : \u201d schedule - moxa - di \u201d \u201c protocol \u201d : \u201c HTTP \u201d \u201c address \u201d : \u201c edgex - device - snmp \u201d \u201c port \u201d : \u201c xxxxx \u201d \u201c path \u201d : \u201d / api / v1 / device / { device id } / { command name }\u201d \u201c method \u201d : \u201c GET \u201d *** This will need to be added *** Create a schedule / POST schedule \u201c name \u201d : \u201d interval - moxa - di0 \u201d \u201c start \u201d : null ( remove parenthesis and replace ) \u201c end \u201d : null ( remove parenthesis and replace ) \u201c frequency \u201d : \u201c PT5S \u201d Create an event that will use the schedule / POST scheduleevent \u201c name \u201d : \u201c device - moxa - di0 \u201d \u201c addressable \u201d : {\u201c name \u201d : \u201d schedule - moxa - di \u201d} \u201c schedule \u201d : \u201d interval - moxa - di0 \u201d \u201c service \u201d : \u201c edgex - device - snmp \u201d *** This will need to be added ***","title":"Project Components Needed"},{"location":"examples/Ch-ExamplesModbusdatatypeconversion/","text":"Modbus - Data Type Conversion In use cases where the Device Resource uses an integer data type with a float scale, precision can be lost following transformation. For example, a Modbus device stores the temperature and humidity in an INT16 data type with a float scale of 0.01. If the temperature is 26.53, the read value is 2653. However, following transformation, the value is 26. To avoid this scenario, the device resource data type must differ from the value descriptor data type. This is achieved using the optional rawType attribute in the device profile to define the binary data read from the Modbus device, and a value type to indicate what data type the user wants to receive. If the rawType attribute exists, the Device Service parses the binary data according to the defined rawType , then casts the value according to the value type defined in the properties of the Device Resources . The following extract from a device profile defines the rawType as INT16 and the value type as FLOAT32: deviceResources : - name : \"humidity\" description : \"The response value is the result of the original value multiplied by 100.\" attributes : { primaryTable : \"HOLDING_REGISTERS\" , startingAddress : \"1\" , rawType : \"INT16\" } properties : value : { type : \"FLOAT32\" , readWrite : \"RW\" , scale : \"0.01\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"%RH\" } - name : \"temperature\" description : \"The response value is the result of the original value multiplied by 100.\" attributes : { primaryTable : \"HOLDING_REGISTERS\" , startingAddress : \"2\" , rawType : \"INT16\" } properties : value : { type : \"FLOAT32\" , readWrite : \"RW\" , scale : \"0.01\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"degrees Celsius\" } Read Command A Read command is executed as follows: The Device Service executes the Read command to read binary data The binary reading data is parsed as an INT16 data type The integer value is cast to a FLOAT32 value Write Command A Write command is executed as follows: The Device Service cast the requested FLOAT32 value to an integer value The integer value is converted to binary data The Device Service executes the Write command When to Transform Data You generally need to transform data when scaling readings between a 16-bit integer and a float value. The following limitations apply: rawType supports only INT16 and UINT16 data types The corresponding value type must be FLOAT32 or FLOAT64 If an unsupported data type is defined for the rawType attribute, the Device Service throws an exception similar to the following: Handler - execReadCmd: error for Device: Modbus-TCP-Device cmd: readAll, the raw type INT32 is not supported /api/v1/device/91f6430d-9268-43e3-88a6-19dbe7f98dad/readAll Supported Transformations The supported transformations are as follows: From rawType To value type INT16 FLOAT32 INT16 FLOAT64 UINT16 FLOAT32 UINT16 FLOAT64","title":"Modbus - Data Type Conversion"},{"location":"examples/Ch-ExamplesModbusdatatypeconversion/#modbus-data-type-conversion","text":"In use cases where the Device Resource uses an integer data type with a float scale, precision can be lost following transformation. For example, a Modbus device stores the temperature and humidity in an INT16 data type with a float scale of 0.01. If the temperature is 26.53, the read value is 2653. However, following transformation, the value is 26. To avoid this scenario, the device resource data type must differ from the value descriptor data type. This is achieved using the optional rawType attribute in the device profile to define the binary data read from the Modbus device, and a value type to indicate what data type the user wants to receive. If the rawType attribute exists, the Device Service parses the binary data according to the defined rawType , then casts the value according to the value type defined in the properties of the Device Resources . The following extract from a device profile defines the rawType as INT16 and the value type as FLOAT32: deviceResources : - name : \"humidity\" description : \"The response value is the result of the original value multiplied by 100.\" attributes : { primaryTable : \"HOLDING_REGISTERS\" , startingAddress : \"1\" , rawType : \"INT16\" } properties : value : { type : \"FLOAT32\" , readWrite : \"RW\" , scale : \"0.01\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"%RH\" } - name : \"temperature\" description : \"The response value is the result of the original value multiplied by 100.\" attributes : { primaryTable : \"HOLDING_REGISTERS\" , startingAddress : \"2\" , rawType : \"INT16\" } properties : value : { type : \"FLOAT32\" , readWrite : \"RW\" , scale : \"0.01\" } units : { type : \"String\" , readWrite : \"R\" , defaultValue : \"degrees Celsius\" }","title":"Modbus - Data Type Conversion"},{"location":"examples/Ch-ExamplesModbusdatatypeconversion/#read-command","text":"A Read command is executed as follows: The Device Service executes the Read command to read binary data The binary reading data is parsed as an INT16 data type The integer value is cast to a FLOAT32 value","title":"Read Command"},{"location":"examples/Ch-ExamplesModbusdatatypeconversion/#write-command","text":"A Write command is executed as follows: The Device Service cast the requested FLOAT32 value to an integer value The integer value is converted to binary data The Device Service executes the Write command","title":"Write Command"},{"location":"examples/Ch-ExamplesModbusdatatypeconversion/#when-to-transform-data","text":"You generally need to transform data when scaling readings between a 16-bit integer and a float value. The following limitations apply: rawType supports only INT16 and UINT16 data types The corresponding value type must be FLOAT32 or FLOAT64 If an unsupported data type is defined for the rawType attribute, the Device Service throws an exception similar to the following: Handler - execReadCmd: error for Device: Modbus-TCP-Device cmd: readAll, the raw type INT32 is not supported /api/v1/device/91f6430d-9268-43e3-88a6-19dbe7f98dad/readAll","title":"When to Transform Data"},{"location":"examples/Ch-ExamplesModbusdatatypeconversion/#supported-transformations","text":"The supported transformations are as follows: From rawType To value type INT16 FLOAT32 INT16 FLOAT64 UINT16 FLOAT32 UINT16 FLOAT64","title":"Supported Transformations"},{"location":"examples/Ch-ExamplesProvisionDevice/","text":"Provison a Device - Modbus Example For this example we will use the GS1-10P5 Modbus motor profile we have available as reference GS1 Profile , device reference: Marathon Electric MicroMax motors via PLC ( http://www.marathon-motors.com/Inverter-Vector-Duty-C-Face-Footed-TEFC-Micromax-Motor_c333.htm )). I would recommend using a tool like Postman for simplifying interactions with the REST APIs (refer to the \"Device and Device Service Setup (aka Device Service Creation and Device Provisioning)\" section for further details at API Demo Walkthrough , all REST content is JSON). Also note that Postman is capable of importing RAML documents for API framing (RAML docs for the EdgeX services may be found in src/test/resources/raml or on the wiki). Note that this specific example can be tweaked for use with the other Device Services. Upload the device profile above to metadata with a POST to http://localhost:48081/api/v1/deviceprofile/uploadfile and add the file as key \"file\" to the body Add the addressable containing reachability information for the device with a POST to http://localhost:48081/api/v1/addressable : If IP connected, the body will look something like: { \"name\":\"Motor\", \"method\": \"GET\", \"protocol\": \"HTTP\",\"address\": \"10.0.1.29\", \"port\": 502 } If serially connected, the body will look something like: {\"name\": \"Motor\", \"method\": \"GET\", \"protocol\": \"OTHER\", \"address\": \"/dev/ttyS5,9600,8,1,1\", \"port\": 0 } (address field contains port, baud rate, number of data bits, stop bits, and parity bits in CSV form) Ensure the Modbus device service is running, adjust the service name below to match if necessary or if using other device services Add the device with a POST to http://localhost:48081/api/v1/device , the body will look something like: { \"description\" : \"MicroMax Variable Speed Motor\" , \"name\" : \"Variable Speed motor\" , \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"addressable\" : { \"name\" : \"Motor\" }, \"labels\" : [], \"location\" : null , \"service\" : { \"name\" : \"edgex-device-modbus\" }, \"profile\" : { \"name\" : \"GS1-VariableSpeedMotor\" } } The addressable name must match/refer to the addressable added in Step 2, the service name must match/refer to the target device service, and the profile name must match the device profile name from Step 1. Further deep dives on the different microservices and layers can be found in our EdgeX Tech Talks series ( EdgeX Tech Talks .) where Jim and I cover some of the intricacies of various services. Of particular relevance here is the Metadata Part 2 discussion covering Device Profiles and Device Provisioning.","title":"Provison a Device - Modbus Example"},{"location":"examples/Ch-ExamplesProvisionDevice/#provison-a-device-modbus-example","text":"For this example we will use the GS1-10P5 Modbus motor profile we have available as reference GS1 Profile , device reference: Marathon Electric MicroMax motors via PLC ( http://www.marathon-motors.com/Inverter-Vector-Duty-C-Face-Footed-TEFC-Micromax-Motor_c333.htm )). I would recommend using a tool like Postman for simplifying interactions with the REST APIs (refer to the \"Device and Device Service Setup (aka Device Service Creation and Device Provisioning)\" section for further details at API Demo Walkthrough , all REST content is JSON). Also note that Postman is capable of importing RAML documents for API framing (RAML docs for the EdgeX services may be found in src/test/resources/raml or on the wiki). Note that this specific example can be tweaked for use with the other Device Services. Upload the device profile above to metadata with a POST to http://localhost:48081/api/v1/deviceprofile/uploadfile and add the file as key \"file\" to the body Add the addressable containing reachability information for the device with a POST to http://localhost:48081/api/v1/addressable : If IP connected, the body will look something like: { \"name\":\"Motor\", \"method\": \"GET\", \"protocol\": \"HTTP\",\"address\": \"10.0.1.29\", \"port\": 502 } If serially connected, the body will look something like: {\"name\": \"Motor\", \"method\": \"GET\", \"protocol\": \"OTHER\", \"address\": \"/dev/ttyS5,9600,8,1,1\", \"port\": 0 } (address field contains port, baud rate, number of data bits, stop bits, and parity bits in CSV form) Ensure the Modbus device service is running, adjust the service name below to match if necessary or if using other device services Add the device with a POST to http://localhost:48081/api/v1/device , the body will look something like: { \"description\" : \"MicroMax Variable Speed Motor\" , \"name\" : \"Variable Speed motor\" , \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"addressable\" : { \"name\" : \"Motor\" }, \"labels\" : [], \"location\" : null , \"service\" : { \"name\" : \"edgex-device-modbus\" }, \"profile\" : { \"name\" : \"GS1-VariableSpeedMotor\" } } The addressable name must match/refer to the addressable added in Step 2, the service name must match/refer to the target device service, and the profile name must match the device profile name from Step 1. Further deep dives on the different microservices and layers can be found in our EdgeX Tech Talks series ( EdgeX Tech Talks .) where Jim and I cover some of the intricacies of various services. Of particular relevance here is the Metadata Part 2 discussion covering Device Profiles and Device Provisioning.","title":"Provison a Device - Modbus Example"},{"location":"examples/Ch-ExamplesRandomDeviceService/","text":"Random Integer Device Service Example The Random Integer Device Service is a sample Device Service that can run directly with other EdgeX services. It has a default, pre-defined device profile (see the device.random.yaml file), device and schedule events (see the configuration.toml file). After the EdgeX Core Service and Random Integer Device Service start, the following Core Service APIs can be viewed in the browser: Core Service API URL Description Core Metadata http://[host]:48081/api/v1/deviceservice/device-random Device Service created Core Metadata http://[host]:48081/api/v1/deviceprofile/Random-Integer-Generator Device profile created Core Metadata http://[host]:48081/api/v1/device/Random-Integer-Generator01 Device created Core Metadata http://[host]:48081/api/v1/scheduleevent/readValue_int16 Schedule created Core Metadata http://[host]:48081/api/v1/scheduleevent/readValue_int32 Schedule created Core Data http://[host]:48080/api/v1/event GenerateRandomValue_Int16 and GenerateRandomValue_Int32 called every 5 seconds to produce events and readings according to the readValue_int16 and readValue_int32 Core Command http://[host]:48082/api/v1/device The following commands are available for GET and PUT methods: - GenerateRandomValue_Int8 - GenerateRandomValue_Int16 - GenerateRandomValue_Int32 Running Commands The command execution URLs can be acquired using a Core Command API inquiry. The command URL is http://[host]:48082/api/v1/device/[device id]/command/[command id] . For example: GET Command If you replace the host and run the GET command for GenerateRandomValue_Int8, you receive an event with a random reading value between -128 and 127, as illustrated below: PUT Command PUT commands can adjust the minimum and maximum values for future random reading values, but they must be valid values for the data type. For example, the minimum value for GenerateRandomValue_Int16 cannot be less than -32768. In the following example, the PUT command limits the future reading value of GenerateRandomValue_Int8 to a range of -2 to 2: Note The parameter of the PUT command body is defined in the parameterNames field of the Command model. To validate the result, send the following GET command:","title":"Random Integer Device Service Example"},{"location":"examples/Ch-ExamplesRandomDeviceService/#random-integer-device-service-example","text":"The Random Integer Device Service is a sample Device Service that can run directly with other EdgeX services. It has a default, pre-defined device profile (see the device.random.yaml file), device and schedule events (see the configuration.toml file). After the EdgeX Core Service and Random Integer Device Service start, the following Core Service APIs can be viewed in the browser: Core Service API URL Description Core Metadata http://[host]:48081/api/v1/deviceservice/device-random Device Service created Core Metadata http://[host]:48081/api/v1/deviceprofile/Random-Integer-Generator Device profile created Core Metadata http://[host]:48081/api/v1/device/Random-Integer-Generator01 Device created Core Metadata http://[host]:48081/api/v1/scheduleevent/readValue_int16 Schedule created Core Metadata http://[host]:48081/api/v1/scheduleevent/readValue_int32 Schedule created Core Data http://[host]:48080/api/v1/event GenerateRandomValue_Int16 and GenerateRandomValue_Int32 called every 5 seconds to produce events and readings according to the readValue_int16 and readValue_int32 Core Command http://[host]:48082/api/v1/device The following commands are available for GET and PUT methods: - GenerateRandomValue_Int8 - GenerateRandomValue_Int16 - GenerateRandomValue_Int32","title":"Random Integer Device Service Example"},{"location":"examples/Ch-ExamplesRandomDeviceService/#running-commands","text":"The command execution URLs can be acquired using a Core Command API inquiry. The command URL is http://[host]:48082/api/v1/device/[device id]/command/[command id] . For example:","title":"Running Commands"},{"location":"examples/Ch-ExamplesRandomDeviceService/#get-command","text":"If you replace the host and run the GET command for GenerateRandomValue_Int8, you receive an event with a random reading value between -128 and 127, as illustrated below:","title":"GET Command"},{"location":"examples/Ch-ExamplesRandomDeviceService/#put-command","text":"PUT commands can adjust the minimum and maximum values for future random reading values, but they must be valid values for the data type. For example, the minimum value for GenerateRandomValue_Int16 cannot be less than -32768. In the following example, the PUT command limits the future reading value of GenerateRandomValue_Int8 to a range of -2 to 2: Note The parameter of the PUT command body is defined in the parameterNames field of the Command model. To validate the result, send the following GET command:","title":"PUT Command"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/","text":"Sending and Consuming Binary Data From EdgeX Device Services EdgeX - Fuji Release Overview In this example, we will demonstrate how to send EdgeX Events and Readings that contain arbitrary binary data. DeviceService Implementation Device Profile To indicate that a deviceResource represents a Binary type, the following format is used: - name : \"camera_snapshot\" description : \"snapshot from camera\" properties : value : { type : \"Binary\" , readWrite : \"R\" } units : { type : \"Binary\" , readWrite : \"R\" , defaultValue : \"CameraSnapshot\" } Device Service Here is a snippet from a hypothetical Device Service's HandleReadCommands() method that produces an event that represents a JPEG image captured from a camera: if req.DeviceResourceName == \"camera_snapshot\" { data, err := cameraClient.GetSnapshot() // returns ([]byte, error) check(err) cv, err := sdkModel.NewBinaryValue(reqs[i].DeviceResourceName, 0, data) check(err) responses[i] = cv } Calling Device Service Command Querying core-metadata for the Device's Commands and DeviceID provides the following as the URL to request a reading from the snapshot command: http://localhost:49990/api/v1/device/3469a658-c3b8-46f1-9098-7d19973af402/OnvifSnapshot Unlike with non-binary Events, making a request to this URL will return an event in CBOR representation. CBOR is a representation of binary data loosely based off of the JSON data model. This Event will not be human-readable. Parsing CBOR Encoded Events To access the data enclosed in these Events and Readings, they will first need to be decoded from CBOR. The following is a simple Go program that reads in the CBOR response from a file containing the response from the previous HTTP request. The Go library recommended for parsing these events can be found at https://github.com/ugorji/go package main import ( \u201cio/ioutil\u201d contracts \u201cgithub.com/edgexfoundry/go-mod-core-contracts/models\u201d \u201cgithub.com/ugorji/go/codec\u201d ) func check(e error) { if e != nil { panic(e) } } func main() { // Read in our cbor data fileBytes, err := ioutil.ReadFile(\u201c/Users/johndoe/Desktop/image.cbor\u201d) check(err) // Create a cbor decoder from the cbor bytes and a cbor code handle var h codec.Handle = new(codec.CborHandle) var dec *codec.Decoder = codec.NewDecoderBytes(fileBytes, h) // Decode into an EdgeX Event var event contracts.Event err = dec.Decode(&event) check(err) // Grab binary data and write to a file imgBytes := event.Readings[0].BinaryValue ioutil.WriteFile(\u201c/Users/johndoe/Desktop/image.jpeg\u201d, imgBytes, 0644) } In the code above, the CBOR data is read into a buffer, a code.Decoder is created to decode the CBOR data, an EdgeX Event struct is created, and a pointer is passed into the decoder's Decode() method to be filled in. Finally, the binary payload is written to a file from the BinaryValue field of the Reading. This method would work as well for decoding Events off the EdgeX message bus. Encoding Arbitrary Structures in Events The Device SDK's NewBinaryValue() function above only accepts a byte slice as binary data. Any arbitrary Go structure can be encoded in a binary reading by first encoding the structure into a byte slice using CBOR. The following illustrates this method: // DeviceService HandleReadCommands() code: foo := struct { X int Y int Z int Bar string } { X: 7, Y: 3, Z: 100, Bar: \"Hello world!\", } buffer := new(bytes.Buffer) ch := new(codec.CborHandle) encoder := codec.NewEncoder(buffer, ch) err = encoder.Encode(&foo) check(err) cv, err := sdkModel.NewBinaryValue(reqs[i].DeviceResourceName, 0, buffer.Bytes()) responses[i] = cv This code takes the anonymous struct with fields X, Y, Z, and Bar (of different types) and serializes it into a byte slice using the same codec library, and passing the output to NewBinaryValue() . When consuming these events, another level of decoding will need to take place to get the structure out of the binary payload. func main() { // Read in our cbor data fileBytes, err := ioutil.ReadFile(\u201c/Users/johndoe/Desktop/image.cbor\u201d) check(err) // Create a cbor decoder from the cbor bytes and a cbor code handle var h codec.Handle = new(codec.CborHandle) var dec *codec.Decoder = codec.NewDecoderBytes(fileBytes, h) // Decode into an EdgeX Event var event contracts.Event err = dec.Decode(&event) check(err) // Decode into arbitrary type foo := struct { X int Y int Z int Bar string }{} dec = codec.NewDecoderBytes(event.Readings[0].BinaryValue, h) err = dec.Decode(&foo) check(err) fmt.Println(foo) } This code takes a command response in the same format as the previous example, but uses the codec library to decode the CBOR data inside the EdgeX Reading's BinaryValue field. Using this approach, an Event can be sent containing data containing an arbitrary, flexible structure. Use cases could be a Reading containing multiple images, a variable length list of integer read-outs, etc. Creating a CBOR Payload for use with PUT Commands To create a CBOR payload that, for example, can be used with PUT commands, we first need to set up some content which will be used to create the CBOR data. Then we encode that content and finally write the CBOR-encoded data to a file, followed by using that file with an example PUT command. The relevant data structures are as follows, containing details of the key and the corresponding value, where you should note in particular the Put field in the Command struct, and below the Command struct is the Put struct itself. More details available at https://github.com/edgexfoundry/go-mod-core-contracts/blob/master/models/put.go: type Command struct { Timestamps `yaml:\",inline\"` Id string `json:\"id\" yaml:\"id,omitempty\"` // Id is a unique identifier, such as a UUID Name string `json:\"name\" yaml:\"name,omitempty\"` // Command name (unique on the profile) Get Get `json:\"get\" yaml:\"get,omitempty\"` // Get Command Put Put `json:\"put\" yaml:\"put,omitempty\"` // Put Command isValidated bool // internal member used for validation check } type Put struct { Action `yaml:\",inline\"` ParameterNames []string `json:\"parameterNames,omitempty\" yaml:\"parameterNames,omitempty\"` } What follows below is the accompanying golang code that accomplishes the steps above: package main import ( \"io/ioutil\" \"github.com/ugorji/go/codec\" ) const ( fileLocation = \"/Users/johnpoe/Desktop/CBOR_Binary\" ) const ( enableRandomizationBinary = \"EnableRandomization_Binary\" path = \"Path\" url = \"Url\" ) func main() { // Set up some records which will be used to create the CBOR data cborContents := make(map[string]string) // The user should put values in the cborContents variable above, which will // be converted to CBOR. Please refer to the earlier note containing details // of the key and the corresponding value each keys represent. What follows // below is an example of populating the \"ParameterNames\" (aka \"Put\") cborContents[enableRandomizationBinary] = \"true\" cborContents[path] = \"/api/v1/device/9f872d68/Binary\" cborContents[url] = \"http://localhost:48082/api/v1/device/9f872d68/command/7ff8d51ea50d\" // Encode the contents that were set up above. input := make([]byte, 0) check(codec.NewEncoderBytes(&input, &codec.CborHandle{}).Encode(cborContents)) // Write the CBOR-encoded data to a file. ioutil.WriteFile(fileLocation, input, 0644) } func check(e error) { if e != nil { panic(e) } } In the code above, as a final step, the CBOR payload has been written to the filesystem, into a file that we are calling CBOR_Binary . Here is how to use the PUT command: Via the --data-binary flag in cURL , supply as follows the CBOR-encoded file created above. You will want to replace the fileLocation (i.e. '@/Users/johnpoe/Desktop/CBOR_Binary' by a suitably-located local file on your filesystem: curl --location --request PUT 'http://localhost:48082/api/v1/device/9f872d68-2281-4af4-959d-29e4d51c2192/command/b349df4a-6c3d-4218-b8bc-7ff8d51ea50d' \\ --header 'Content-Type: application/cbor' \\ --data-binary '@/Users/johnpoe/Desktop/CBOR_Binary'","title":"Sending and Consuming Binary Data From EdgeX Device Services"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#sending-and-consuming-binary-data-from-edgex-device-services","text":"EdgeX - Fuji Release","title":"Sending and Consuming Binary Data From EdgeX Device Services"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#overview","text":"In this example, we will demonstrate how to send EdgeX Events and Readings that contain arbitrary binary data.","title":"Overview"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#deviceservice-implementation","text":"","title":"DeviceService Implementation"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#device-profile","text":"To indicate that a deviceResource represents a Binary type, the following format is used: - name : \"camera_snapshot\" description : \"snapshot from camera\" properties : value : { type : \"Binary\" , readWrite : \"R\" } units : { type : \"Binary\" , readWrite : \"R\" , defaultValue : \"CameraSnapshot\" }","title":"Device Profile"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#device-service","text":"Here is a snippet from a hypothetical Device Service's HandleReadCommands() method that produces an event that represents a JPEG image captured from a camera: if req.DeviceResourceName == \"camera_snapshot\" { data, err := cameraClient.GetSnapshot() // returns ([]byte, error) check(err) cv, err := sdkModel.NewBinaryValue(reqs[i].DeviceResourceName, 0, data) check(err) responses[i] = cv }","title":"Device Service"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#calling-device-service-command","text":"Querying core-metadata for the Device's Commands and DeviceID provides the following as the URL to request a reading from the snapshot command: http://localhost:49990/api/v1/device/3469a658-c3b8-46f1-9098-7d19973af402/OnvifSnapshot Unlike with non-binary Events, making a request to this URL will return an event in CBOR representation. CBOR is a representation of binary data loosely based off of the JSON data model. This Event will not be human-readable.","title":"Calling Device Service Command"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#parsing-cbor-encoded-events","text":"To access the data enclosed in these Events and Readings, they will first need to be decoded from CBOR. The following is a simple Go program that reads in the CBOR response from a file containing the response from the previous HTTP request. The Go library recommended for parsing these events can be found at https://github.com/ugorji/go package main import ( \u201cio/ioutil\u201d contracts \u201cgithub.com/edgexfoundry/go-mod-core-contracts/models\u201d \u201cgithub.com/ugorji/go/codec\u201d ) func check(e error) { if e != nil { panic(e) } } func main() { // Read in our cbor data fileBytes, err := ioutil.ReadFile(\u201c/Users/johndoe/Desktop/image.cbor\u201d) check(err) // Create a cbor decoder from the cbor bytes and a cbor code handle var h codec.Handle = new(codec.CborHandle) var dec *codec.Decoder = codec.NewDecoderBytes(fileBytes, h) // Decode into an EdgeX Event var event contracts.Event err = dec.Decode(&event) check(err) // Grab binary data and write to a file imgBytes := event.Readings[0].BinaryValue ioutil.WriteFile(\u201c/Users/johndoe/Desktop/image.jpeg\u201d, imgBytes, 0644) } In the code above, the CBOR data is read into a buffer, a code.Decoder is created to decode the CBOR data, an EdgeX Event struct is created, and a pointer is passed into the decoder's Decode() method to be filled in. Finally, the binary payload is written to a file from the BinaryValue field of the Reading. This method would work as well for decoding Events off the EdgeX message bus.","title":"Parsing CBOR Encoded Events"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#encoding-arbitrary-structures-in-events","text":"The Device SDK's NewBinaryValue() function above only accepts a byte slice as binary data. Any arbitrary Go structure can be encoded in a binary reading by first encoding the structure into a byte slice using CBOR. The following illustrates this method: // DeviceService HandleReadCommands() code: foo := struct { X int Y int Z int Bar string } { X: 7, Y: 3, Z: 100, Bar: \"Hello world!\", } buffer := new(bytes.Buffer) ch := new(codec.CborHandle) encoder := codec.NewEncoder(buffer, ch) err = encoder.Encode(&foo) check(err) cv, err := sdkModel.NewBinaryValue(reqs[i].DeviceResourceName, 0, buffer.Bytes()) responses[i] = cv This code takes the anonymous struct with fields X, Y, Z, and Bar (of different types) and serializes it into a byte slice using the same codec library, and passing the output to NewBinaryValue() . When consuming these events, another level of decoding will need to take place to get the structure out of the binary payload. func main() { // Read in our cbor data fileBytes, err := ioutil.ReadFile(\u201c/Users/johndoe/Desktop/image.cbor\u201d) check(err) // Create a cbor decoder from the cbor bytes and a cbor code handle var h codec.Handle = new(codec.CborHandle) var dec *codec.Decoder = codec.NewDecoderBytes(fileBytes, h) // Decode into an EdgeX Event var event contracts.Event err = dec.Decode(&event) check(err) // Decode into arbitrary type foo := struct { X int Y int Z int Bar string }{} dec = codec.NewDecoderBytes(event.Readings[0].BinaryValue, h) err = dec.Decode(&foo) check(err) fmt.Println(foo) } This code takes a command response in the same format as the previous example, but uses the codec library to decode the CBOR data inside the EdgeX Reading's BinaryValue field. Using this approach, an Event can be sent containing data containing an arbitrary, flexible structure. Use cases could be a Reading containing multiple images, a variable length list of integer read-outs, etc.","title":"Encoding Arbitrary Structures in Events"},{"location":"examples/Ch-ExamplesSendingAndConsumingBinary/#creating-a-cbor-payload-for-use-with-put-commands","text":"To create a CBOR payload that, for example, can be used with PUT commands, we first need to set up some content which will be used to create the CBOR data. Then we encode that content and finally write the CBOR-encoded data to a file, followed by using that file with an example PUT command. The relevant data structures are as follows, containing details of the key and the corresponding value, where you should note in particular the Put field in the Command struct, and below the Command struct is the Put struct itself. More details available at https://github.com/edgexfoundry/go-mod-core-contracts/blob/master/models/put.go: type Command struct { Timestamps `yaml:\",inline\"` Id string `json:\"id\" yaml:\"id,omitempty\"` // Id is a unique identifier, such as a UUID Name string `json:\"name\" yaml:\"name,omitempty\"` // Command name (unique on the profile) Get Get `json:\"get\" yaml:\"get,omitempty\"` // Get Command Put Put `json:\"put\" yaml:\"put,omitempty\"` // Put Command isValidated bool // internal member used for validation check } type Put struct { Action `yaml:\",inline\"` ParameterNames []string `json:\"parameterNames,omitempty\" yaml:\"parameterNames,omitempty\"` } What follows below is the accompanying golang code that accomplishes the steps above: package main import ( \"io/ioutil\" \"github.com/ugorji/go/codec\" ) const ( fileLocation = \"/Users/johnpoe/Desktop/CBOR_Binary\" ) const ( enableRandomizationBinary = \"EnableRandomization_Binary\" path = \"Path\" url = \"Url\" ) func main() { // Set up some records which will be used to create the CBOR data cborContents := make(map[string]string) // The user should put values in the cborContents variable above, which will // be converted to CBOR. Please refer to the earlier note containing details // of the key and the corresponding value each keys represent. What follows // below is an example of populating the \"ParameterNames\" (aka \"Put\") cborContents[enableRandomizationBinary] = \"true\" cborContents[path] = \"/api/v1/device/9f872d68/Binary\" cborContents[url] = \"http://localhost:48082/api/v1/device/9f872d68/command/7ff8d51ea50d\" // Encode the contents that were set up above. input := make([]byte, 0) check(codec.NewEncoderBytes(&input, &codec.CborHandle{}).Encode(cborContents)) // Write the CBOR-encoded data to a file. ioutil.WriteFile(fileLocation, input, 0644) } func check(e error) { if e != nil { panic(e) } } In the code above, as a final step, the CBOR payload has been written to the filesystem, into a file that we are calling CBOR_Binary . Here is how to use the PUT command: Via the --data-binary flag in cURL , supply as follows the CBOR-encoded file created above. You will want to replace the fileLocation (i.e. '@/Users/johnpoe/Desktop/CBOR_Binary' by a suitably-located local file on your filesystem: curl --location --request PUT 'http://localhost:48082/api/v1/device/9f872d68-2281-4af4-959d-29e4d51c2192/command/b349df4a-6c3d-4218-b8bc-7ff8d51ea50d' \\ --header 'Content-Type: application/cbor' \\ --data-binary '@/Users/johnpoe/Desktop/CBOR_Binary'","title":"Creating a CBOR Payload for use with PUT Commands"},{"location":"examples/Ch-ExamplesVirtualDeviceService/","text":"Using the Virtual Device Service Overview The Virtual Device Service GO can simulate different kinds of devices to generate Events and Readings to the Core Data Micro Service. Furthermore, users can send commands and get responses through the Command and Control Micro Service. The Virtual Device Service allows you to execute functional or performance tests without any real devices. This version of the Virtual Device Service is implemented based on Device SDK GO , and uses ql (an embedded SQL database engine) to simulate virtual resources. Sequence Diagram Virtual Resource Table Schema Column Type DEVICE_NAME STRING COMMAND_NAME STRING DEVICE_RESOURCE_NAME STRING ENABLE_RANDOMIZATION BOOL DATA_TYPE STRING VALUE STRING How to Use The Virtual Device Service depends on the EdgeX Core Services. If you're going to download the source code and run the Virtual Device Service in dev mode, make sure that the EdgeX Core Services are up before starting the Virtual Device Service. The Virtual Device Service currently contains four pre-defined devices (see the configuration.toml ) as random value generators: Device Name Device Profile Random-Boolean-Device device.virtual.bool.yaml Random-Float-Device device.virtual.float.yaml Random-Integer-Device device.virtual.int.yaml Random-UnsignedInteger-Device device.virtual.uint.yaml Restricted: To control the randomization of device resource values, it has to add additional device resources with the prefix \"EnableRandomization_\" for each device resource. (Need to do the same for device commands and core commands) Please find the above default device profiles for example. Acquire the executable commands information by inquiring the Core Command API: http://[host]:48082/api/v1/device/name/Random-Boolean-Device http://[host]:48082/api/v1/device/name/Random-Integer-Device http://[host]:48082/api/v1/device/name/Random-UnsignedInteger-Device http://[host]:48082/api/v1/device/name/Random-Float-Device GET command example curl -X GET localhost:48082/api/v1/device/1bd5d4c3-9d43-42f2-8c4a-f32f5999edf7/command/e5d7c2b8-eab7-4da4-9d41-388da05979a4 ` { \"device\" : \"Random-Integer-Device\" , \"origin\" : 1574325994604494491 , \"readings\" : [ { \"origin\" : 1574325994572380549 , \"device\" : \"Random-Integer-Device\" , \"name\" : \"Int8\" , \"value\" : \"42\" } ], \"EncodedEvent\" : null } PUT command example - Assign a value to a resource Note The value of the resource's EnableRandomization property is simultaneously updated to false when sending a put command to assign a specified value to the resource The minimum and maximum values of the resource can be defined in the property value field of the Device Resource model, for example: deviceResources: - name: \"Int8\" description: \"Generate random int8 value\" properties: value: { type: \"Int8\", readWrite: \"R\", minimum: \"-100\", maximum: \"100\", defaultValue: \"0\" } units: { type: \"String\", readWrite: \"R\", defaultValue: \"random int8 value\" } Manipulate Virtual Resources Using the command ql Tool Install command ql If the Virtual Device Service runs in a Docker container, it must mount the directory (/db) that contains the ql database in the container. For example: device-virtual : image : edgexfoundry/docker-device-virtual-go:1.1.0 ports : - \"49990:49990\" container_name : device-virtual hostname : device-virtual networks : - edgex-network volumes : - db-data:/data/db - log-data:/edgex/logs - consul-config:/consul/config - consul-data:/consul/data - /mnt/hgfs/EdgeX/DeviceVirtualDB:/db # Mount ql database directory depends_on : - data - command If the Virtual Device Service runs in dev mode, the ql database directory is under the driver directory Command examples: Query all data: ql -db /path-to-the-ql-db-folder/deviceVirtual.db -fld \"select * from VIRTUAL_RESOURCE\" Update Enable_Randomization: ql -db /path-to-the-ql-db-folder/deviceVirtual.db \"update VIRTUAL_RESOURCE set ENABLE_RANDOMIZATION=false where DEVICE_NAME=\" Random-Integer-Device \" and DEVICE_RESOURCE_NAME=\" Int8 \" \" Update Value: ql -db /path-to-the-ql-db-folder/deviceVirtual.db \"update VIRTUAL_RESOURCE set VALUE=\" 26 \" where DEVICE_NAME=\" Random-Integer-Device \" and DEVICE_RESOURCE_NAME=\" Int8 \" \"","title":"Using the Virtual Device Service"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#using-the-virtual-device-service","text":"","title":"Using the Virtual Device Service"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#overview","text":"The Virtual Device Service GO can simulate different kinds of devices to generate Events and Readings to the Core Data Micro Service. Furthermore, users can send commands and get responses through the Command and Control Micro Service. The Virtual Device Service allows you to execute functional or performance tests without any real devices. This version of the Virtual Device Service is implemented based on Device SDK GO , and uses ql (an embedded SQL database engine) to simulate virtual resources.","title":"Overview"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#sequence-diagram","text":"","title":"Sequence Diagram"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#virtual-resource-table-schema","text":"Column Type DEVICE_NAME STRING COMMAND_NAME STRING DEVICE_RESOURCE_NAME STRING ENABLE_RANDOMIZATION BOOL DATA_TYPE STRING VALUE STRING","title":"Virtual Resource Table Schema"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#how-to-use","text":"The Virtual Device Service depends on the EdgeX Core Services. If you're going to download the source code and run the Virtual Device Service in dev mode, make sure that the EdgeX Core Services are up before starting the Virtual Device Service. The Virtual Device Service currently contains four pre-defined devices (see the configuration.toml ) as random value generators: Device Name Device Profile Random-Boolean-Device device.virtual.bool.yaml Random-Float-Device device.virtual.float.yaml Random-Integer-Device device.virtual.int.yaml Random-UnsignedInteger-Device device.virtual.uint.yaml Restricted: To control the randomization of device resource values, it has to add additional device resources with the prefix \"EnableRandomization_\" for each device resource. (Need to do the same for device commands and core commands) Please find the above default device profiles for example. Acquire the executable commands information by inquiring the Core Command API: http://[host]:48082/api/v1/device/name/Random-Boolean-Device http://[host]:48082/api/v1/device/name/Random-Integer-Device http://[host]:48082/api/v1/device/name/Random-UnsignedInteger-Device http://[host]:48082/api/v1/device/name/Random-Float-Device","title":"How to Use"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#get-command-example","text":"curl -X GET localhost:48082/api/v1/device/1bd5d4c3-9d43-42f2-8c4a-f32f5999edf7/command/e5d7c2b8-eab7-4da4-9d41-388da05979a4 ` { \"device\" : \"Random-Integer-Device\" , \"origin\" : 1574325994604494491 , \"readings\" : [ { \"origin\" : 1574325994572380549 , \"device\" : \"Random-Integer-Device\" , \"name\" : \"Int8\" , \"value\" : \"42\" } ], \"EncodedEvent\" : null }","title":"GET command example"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#put-command-example-assign-a-value-to-a-resource","text":"Note The value of the resource's EnableRandomization property is simultaneously updated to false when sending a put command to assign a specified value to the resource The minimum and maximum values of the resource can be defined in the property value field of the Device Resource model, for example: deviceResources: - name: \"Int8\" description: \"Generate random int8 value\" properties: value: { type: \"Int8\", readWrite: \"R\", minimum: \"-100\", maximum: \"100\", defaultValue: \"0\" } units: { type: \"String\", readWrite: \"R\", defaultValue: \"random int8 value\" }","title":"PUT command example - Assign a value to a resource"},{"location":"examples/Ch-ExamplesVirtualDeviceService/#manipulate-virtual-resources-using-the-command-ql-tool","text":"Install command ql If the Virtual Device Service runs in a Docker container, it must mount the directory (/db) that contains the ql database in the container. For example: device-virtual : image : edgexfoundry/docker-device-virtual-go:1.1.0 ports : - \"49990:49990\" container_name : device-virtual hostname : device-virtual networks : - edgex-network volumes : - db-data:/data/db - log-data:/edgex/logs - consul-config:/consul/config - consul-data:/consul/data - /mnt/hgfs/EdgeX/DeviceVirtualDB:/db # Mount ql database directory depends_on : - data - command If the Virtual Device Service runs in dev mode, the ql database directory is under the driver directory Command examples: Query all data: ql -db /path-to-the-ql-db-folder/deviceVirtual.db -fld \"select * from VIRTUAL_RESOURCE\" Update Enable_Randomization: ql -db /path-to-the-ql-db-folder/deviceVirtual.db \"update VIRTUAL_RESOURCE set ENABLE_RANDOMIZATION=false where DEVICE_NAME=\" Random-Integer-Device \" and DEVICE_RESOURCE_NAME=\" Int8 \" \" Update Value: ql -db /path-to-the-ql-db-folder/deviceVirtual.db \"update VIRTUAL_RESOURCE set VALUE=\" 26 \" where DEVICE_NAME=\" Random-Integer-Device \" and DEVICE_RESOURCE_NAME=\" Int8 \" \"","title":"Manipulate Virtual Resources Using the command ql Tool"},{"location":"general/Definitions/","text":"Definitions The following glossary provides terms used in EdgeX Foundry. The definition are based on how EdgeX and its community use the term versus any strict technical or industry definition. Brownfield and Greenfield Brownfield refers to older legacy equipment (nodes, devices, sensors) in an edge/IoT deployment, which typically uses older protocols. Greenfield refers to, typically, new equipment with modern protocols. Containerized EdgeX micro services and infrastrucutre (i.e. databases, registry, etc.) are built as executable programs, put into Docker images, and made available via Docker Hub (and Nexus repository for nightly builds). A service (or infrastructure element) that is available in Docker Hub (or Nexus) is said to be containerized. Docker images can be quickly downloaded and new Docker containers created from the images. Contributor/Developer If you want to change, add to or at least build the existing EdgeX code base, then you are a \"Developer\". \"Contributors\" are developers that further wish to contribute their code back into the EdgeX open source effort. Device In EdgeX parlance, \"device\" is used to refer to a sensor, actuator, or IoT \"thing\". A sensor generally collects information from the physical world - like a temperature or vibration sensor. Actuators are machines that can be told to do something. Actuators move or otherwise control a mechanism or system - like a value on a pump. While there may be some technical differences, for the purposes of EdgeX documentation, device will refer to a sensor, actuator or \"thing\". Edge Analytics The terms edge or local analytics (the terms are used interchangeably and have the same meaning in this context) for the purposes of edge computing (and EdgeX), refers to an \u201canalytics\u201d service is that: - Receives and interprets the EdgeX sensor data to some degree; some analytics services are more sophisticated and able to provide more insights than others - Make determinations on what actions and actuations need to occur based on the insights it has achieved, thereby driving actuation requests to EdgeX associated devices or other services (like notifications) The analytics service could be some simple logic built into an app service, a rules engine package, or an agent of some artificial intelligence/machine learning system. From an EdgeX perspective, actionable intelligence generation is all the same. From an EdgeX perspective, edge analytics = seeing the edge data and be able to make requests to act on what is seen. While EdgeX provides a rules engine service as its reference implementation of local analytics, app services and its data preparation capability allow sensor data to be streamed to any analytics package. Because of EdgeX\u2019s micro service architecture and distributed nature, the analytics service would not necessarily have to run local to the devices / sensors. In other words, it would not have to run at the edge. App services could deliver the edge data to analytics living in the cloud. However, in these scenarios, the insight intelligence would not be considered local or edge in context. Because of latency concerns, data security and privacy needs, intermittent connectivity of edge systems, and other reasons, it is often vital for edge platforms to retain an analytic capability at the edge or local. Gateway An IoT gateway is a compute platform at the farthest ends of an edge or IoT network. It is the host or \u201cbox\u201d to which physical sensors and devices connect and that is, in turn, connected to the networks (wired or wirelessly) of the information technology realm. IoT or edge gateways are compute platforms that connect \u201cthings\u201d (sensors and devices) to IT networks and systems. Micro service In a micro service architecture, each component has its own process. This is in contrast to a monolithic architecture in which all components of the application run in the same process. Benefits of micro service architectures include: - Allow any one service to be replaced and upgraded more easily - Allow services to be programmed using different programming languages and underlying technical solutions (use the best technology for each specific service) - Ex: services written in C can communicate and work with services written in Go - This allows organizations building solutions to maximize available developer resources and some legacy code - Allow services to be distributed across host compute platforms - allowing better utilization of available compute resources - Allow for more scalable solutions by adding copies of services when needed Reference Implementation Default and example implementation(s) offered by the EdgeX community. Other implementations may be offered by 3rd parties or for specialization. South and North Side South Side: All IoT objects, within the physical realm, and the edge of the network that communicates directly with those devices, sensors, actuators, and other IoT objects, and collects the data from them, is known collectively as the \"south side.\" North Side: The cloud (or enterprise system) where data is collected, stored, aggregated, analyzed, and turned into information, and the part of the network that communicates with the cloud, is referred to as the \"north side\" of the network. EdgeX enables data to be sent \"north, \" \"south, \" or laterally as needed and as directed. User If you want to get the EdgeX platform and run it (but do not intend to change or add to the existing code base now) then you are considered a \"User\".","title":"Definitions"},{"location":"general/Definitions/#definitions","text":"The following glossary provides terms used in EdgeX Foundry. The definition are based on how EdgeX and its community use the term versus any strict technical or industry definition.","title":"Definitions"},{"location":"general/Definitions/#brownfield-and-greenfield","text":"Brownfield refers to older legacy equipment (nodes, devices, sensors) in an edge/IoT deployment, which typically uses older protocols. Greenfield refers to, typically, new equipment with modern protocols.","title":"Brownfield and Greenfield"},{"location":"general/Definitions/#containerized","text":"EdgeX micro services and infrastrucutre (i.e. databases, registry, etc.) are built as executable programs, put into Docker images, and made available via Docker Hub (and Nexus repository for nightly builds). A service (or infrastructure element) that is available in Docker Hub (or Nexus) is said to be containerized. Docker images can be quickly downloaded and new Docker containers created from the images.","title":"Containerized"},{"location":"general/Definitions/#contributordeveloper","text":"If you want to change, add to or at least build the existing EdgeX code base, then you are a \"Developer\". \"Contributors\" are developers that further wish to contribute their code back into the EdgeX open source effort.","title":"Contributor/Developer"},{"location":"general/Definitions/#device","text":"In EdgeX parlance, \"device\" is used to refer to a sensor, actuator, or IoT \"thing\". A sensor generally collects information from the physical world - like a temperature or vibration sensor. Actuators are machines that can be told to do something. Actuators move or otherwise control a mechanism or system - like a value on a pump. While there may be some technical differences, for the purposes of EdgeX documentation, device will refer to a sensor, actuator or \"thing\".","title":"Device"},{"location":"general/Definitions/#edge-analytics","text":"The terms edge or local analytics (the terms are used interchangeably and have the same meaning in this context) for the purposes of edge computing (and EdgeX), refers to an \u201canalytics\u201d service is that: - Receives and interprets the EdgeX sensor data to some degree; some analytics services are more sophisticated and able to provide more insights than others - Make determinations on what actions and actuations need to occur based on the insights it has achieved, thereby driving actuation requests to EdgeX associated devices or other services (like notifications) The analytics service could be some simple logic built into an app service, a rules engine package, or an agent of some artificial intelligence/machine learning system. From an EdgeX perspective, actionable intelligence generation is all the same. From an EdgeX perspective, edge analytics = seeing the edge data and be able to make requests to act on what is seen. While EdgeX provides a rules engine service as its reference implementation of local analytics, app services and its data preparation capability allow sensor data to be streamed to any analytics package. Because of EdgeX\u2019s micro service architecture and distributed nature, the analytics service would not necessarily have to run local to the devices / sensors. In other words, it would not have to run at the edge. App services could deliver the edge data to analytics living in the cloud. However, in these scenarios, the insight intelligence would not be considered local or edge in context. Because of latency concerns, data security and privacy needs, intermittent connectivity of edge systems, and other reasons, it is often vital for edge platforms to retain an analytic capability at the edge or local.","title":"Edge Analytics"},{"location":"general/Definitions/#gateway","text":"An IoT gateway is a compute platform at the farthest ends of an edge or IoT network. It is the host or \u201cbox\u201d to which physical sensors and devices connect and that is, in turn, connected to the networks (wired or wirelessly) of the information technology realm. IoT or edge gateways are compute platforms that connect \u201cthings\u201d (sensors and devices) to IT networks and systems.","title":"Gateway"},{"location":"general/Definitions/#micro-service","text":"In a micro service architecture, each component has its own process. This is in contrast to a monolithic architecture in which all components of the application run in the same process. Benefits of micro service architectures include: - Allow any one service to be replaced and upgraded more easily - Allow services to be programmed using different programming languages and underlying technical solutions (use the best technology for each specific service) - Ex: services written in C can communicate and work with services written in Go - This allows organizations building solutions to maximize available developer resources and some legacy code - Allow services to be distributed across host compute platforms - allowing better utilization of available compute resources - Allow for more scalable solutions by adding copies of services when needed","title":"Micro service"},{"location":"general/Definitions/#reference-implementation","text":"Default and example implementation(s) offered by the EdgeX community. Other implementations may be offered by 3rd parties or for specialization.","title":"Reference Implementation"},{"location":"general/Definitions/#south-and-north-side","text":"South Side: All IoT objects, within the physical realm, and the edge of the network that communicates directly with those devices, sensors, actuators, and other IoT objects, and collects the data from them, is known collectively as the \"south side.\" North Side: The cloud (or enterprise system) where data is collected, stored, aggregated, analyzed, and turned into information, and the part of the network that communicates with the cloud, is referred to as the \"north side\" of the network. EdgeX enables data to be sent \"north, \" \"south, \" or laterally as needed and as directed.","title":"South and North Side"},{"location":"general/Definitions/#user","text":"If you want to get the EdgeX platform and run it (but do not intend to change or add to the existing code base now) then you are considered a \"User\".","title":"User"},{"location":"getting-started/","text":"Getting Started To get started you need to get EdgeX Foundry either as a User or as a Developer/Contributor. User If you want to get the EdgeX platform and run it (but do not intend to change or add to the existing code base now) then you are considered a \"User\". You will want to follow the Getting Started Users guide. The Getting Started Users guide takes you through the process of getting the latest release EdgeX Docker Containers from Docker Hub. If you wish to get the latest EdgeX containers (those built from the current ongoing development efforts prior to release), then see Getting Started Users - Nexus . Warning Containers used from Nexus are considered \"work in progress\". There is no guarantee that these containers will function properly or function properly with other containers from the current release. Snap User As an alternative to Docker containers, users may wish to use Canonical's EdgeX Foundry 'snap'. Snap is a software deployment and package management system developed by Canonical for the Linux operating system. The packages, called snaps, and the tool for using them, snapd, work across a range of Linux distributions allowing distribution-agnostic upstream software packaging. The EdgeX snap is published by EdgeX Foundry and made available through the snap store . If you wish to get the latest EdgeX release snap, follow the Getting Started Snap Users guide. Developer and Contributor If you want to change, add to or at least build the existing EdgeX code base, then you are a \"Developer\". \"Contributors\" are developers that further wish to contribute their code back into the EdgeX open source effort. You will want to follow the Getting Started for Developers guide. Hybrid See Getting Started Hybrid if you are developing or working on a particular micro service, but want to run the other micro services via Docker Containers. When working on something like an analytics service (as a developer or contributor) you may not wish to download, build and run all the EdgeX code - you only want to work with the code of your service. Your new service may still need to communicate with other services while you test your new service. Unless you want to get and build all the services, developers will often get and run the containers for the other EdgeX micro services and run only their service natively in a development environment. The EdgeX community refers to this as \"Hybrid\" development. Device Service Developer As a developer, if you intend to connect IoT objects (device, sensor or other \"thing\") that are not currently connected to EdgeX Foundry, you may also want to obtain the Device Service Software Development Kit (DS SDK) and create new device services. The DS SDK creates all the scaffolding code for a new EdgeX Foundry device service; allowing you to focus on the details of interfacing with the device in its native protocol. See Getting Started with Device SDK for help on using the DS SDK to create a new device service. Learn more about Device Services and the Device Service SDK at Device Services . Application Service Developer As a developer, if you intend to get EdgeX sensor data to external systems (be that an enterprise application, on-prem server or Cloud platform like Azure IoT Hub, AWS IoT, Google Cloud IOT, etc.), you will likely want to obtain the Application Functions SDK (App Func SDK) and create new application services. The App Func SDK creates all the scaffolding code for a new EdgeX Foundry application service; allowing you to focus on the details of data transformation, filtering, and otherwise prepare the sensor data for the external endpoint. Learn more about Application Services and the Application Functions SDK at Application Services .","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"To get started you need to get EdgeX Foundry either as a User or as a Developer/Contributor.","title":"Getting Started"},{"location":"getting-started/#user","text":"If you want to get the EdgeX platform and run it (but do not intend to change or add to the existing code base now) then you are considered a \"User\". You will want to follow the Getting Started Users guide. The Getting Started Users guide takes you through the process of getting the latest release EdgeX Docker Containers from Docker Hub. If you wish to get the latest EdgeX containers (those built from the current ongoing development efforts prior to release), then see Getting Started Users - Nexus . Warning Containers used from Nexus are considered \"work in progress\". There is no guarantee that these containers will function properly or function properly with other containers from the current release.","title":"User"},{"location":"getting-started/#snap-user","text":"As an alternative to Docker containers, users may wish to use Canonical's EdgeX Foundry 'snap'. Snap is a software deployment and package management system developed by Canonical for the Linux operating system. The packages, called snaps, and the tool for using them, snapd, work across a range of Linux distributions allowing distribution-agnostic upstream software packaging. The EdgeX snap is published by EdgeX Foundry and made available through the snap store . If you wish to get the latest EdgeX release snap, follow the Getting Started Snap Users guide.","title":"Snap User"},{"location":"getting-started/#developer-and-contributor","text":"If you want to change, add to or at least build the existing EdgeX code base, then you are a \"Developer\". \"Contributors\" are developers that further wish to contribute their code back into the EdgeX open source effort. You will want to follow the Getting Started for Developers guide.","title":"Developer and Contributor"},{"location":"getting-started/#hybrid","text":"See Getting Started Hybrid if you are developing or working on a particular micro service, but want to run the other micro services via Docker Containers. When working on something like an analytics service (as a developer or contributor) you may not wish to download, build and run all the EdgeX code - you only want to work with the code of your service. Your new service may still need to communicate with other services while you test your new service. Unless you want to get and build all the services, developers will often get and run the containers for the other EdgeX micro services and run only their service natively in a development environment. The EdgeX community refers to this as \"Hybrid\" development.","title":"Hybrid"},{"location":"getting-started/#device-service-developer","text":"As a developer, if you intend to connect IoT objects (device, sensor or other \"thing\") that are not currently connected to EdgeX Foundry, you may also want to obtain the Device Service Software Development Kit (DS SDK) and create new device services. The DS SDK creates all the scaffolding code for a new EdgeX Foundry device service; allowing you to focus on the details of interfacing with the device in its native protocol. See Getting Started with Device SDK for help on using the DS SDK to create a new device service. Learn more about Device Services and the Device Service SDK at Device Services .","title":"Device Service Developer"},{"location":"getting-started/#application-service-developer","text":"As a developer, if you intend to get EdgeX sensor data to external systems (be that an enterprise application, on-prem server or Cloud platform like Azure IoT Hub, AWS IoT, Google Cloud IOT, etc.), you will likely want to obtain the Application Functions SDK (App Func SDK) and create new application services. The App Func SDK creates all the scaffolding code for a new EdgeX Foundry application service; allowing you to focus on the details of data transformation, filtering, and otherwise prepare the sensor data for the external endpoint. Learn more about Application Services and the Application Functions SDK at Application Services .","title":"Application Service Developer"},{"location":"getting-started/ApplicationFunctionsSDK/","text":"Getting Started The Application Functions SDK The SDK is built around the idea of a \"Functions Pipeline\". A functions pipeline is a collection of various functions that process the data in the order that you've specified. The functions pipeline is executed by the specified trigger in the configuration.toml . The first function in the pipeline is called with the event that triggered the pipeline (ex. events.Model ). Each successive call in the pipeline is called with the return result of the previous function. Let's take a look at a simple example that creates a pipeline to filter particular device ids and subsequently transform the data to XML: package main import ( \"fmt\" \"github.com/edgexfoundry/app-functions-sdk-go/appsdk\" \"github.com/edgexfoundry/app-functions-sdk-go/pkg/transforms\" \"os\" ) func main () { // 1) First thing to do is to create an instance of the EdgeX SDK, giving it a service key edgexSdk := & appsdk . AppFunctionsSDK { ServiceKey : \"SimpleFilterXMLApp\" , // Key used by Registry (Aka Consul) } // 2) Next, we need to initialize the SDK if err := edgexSdk . Initialize (); err != nil { message := fmt . Sprintf ( \"SDK initialization failed: %v\\n\" , err ) if edgexSdk . LoggingClient != nil { edgexSdk . LoggingClient . Error ( message ) } else { fmt . Println ( message ) } os . Exit ( - 1 ) } // 3) Shows how to access the application's specific configuration settings. deviceNames , err := edgexSdk . GetAppSettingStrings ( \"DeviceNames\" ) if err != nil { edgexSdk . LoggingClient . Error ( err . Error ()) os . Exit ( - 1 ) } // 4) This is our pipeline configuration, the collection of functions to // execute every time an event is triggered. if err = edgexSdk . SetFunctionsPipeline ( transforms . NewFilter ( deviceNames ). FilterByDeviceName , transforms . NewConversion (). TransformToXML , ); err != nil { edgexSdk . LoggingClient . Error ( fmt . Sprintf ( \"SDK SetPipeline failed: %v\\n\" , err )) os . Exit ( - 1 ) } // 5) Lastly, we'll go ahead and tell the SDK to \"start\" and begin listening for events to trigger the pipeline. err = edgexSdk . MakeItRun () if err != nil { edgexSdk . LoggingClient . Error ( \"MakeItRun returned error: \" , err . Error ()) os . Exit ( - 1 ) } // Do any required cleanup here os . Exit ( 0 ) } The above example is meant to merely demonstrate the structure of your application. Notice that the output of the last function is not available anywhere inside this application. You must provide a function in order to work with the data from the previous function. Let's go ahead and add the following function that prints the output to the console. func printXMLToConsole ( edgexcontext * appcontext . Context , params ... interface {}) ( bool , interface {}) { if len ( params ) < 1 { // We didn't receive a result return false , errors . New ( \"No Data Received\" ) } println ( params [ 0 ].( string )) return true , nil } After placing the above function in your code, the next step is to modify the pipeline to call this function: Note You can find this example called \"Simple Filter XML\" and more located in the examples section. Up until this point, the pipeline has been triggered by an event over HTTP and the data at the end of that pipeline lands in the last function specified. In the example, data ends up printed to the console. Perhaps we'd like to send the data back to where it came from. In the case of an HTTP trigger, this would be the HTTP response. In the case of a message bus, this could be a new topic to send the data back to for other applications that wish to receive it. To do this, simply call edgexcontext.Complete([]byte outputData) passing in the data you wish to \"respond\" with. In the above printXMLToConsole(...) function, replace println(params[0].(string)) with edgexcontext.Complete([]byte(params[0].(string))) . You should now see the response in your postman window when testing the pipeline.","title":"Application Functions SDK"},{"location":"getting-started/ApplicationFunctionsSDK/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/ApplicationFunctionsSDK/#the-application-functions-sdk","text":"The SDK is built around the idea of a \"Functions Pipeline\". A functions pipeline is a collection of various functions that process the data in the order that you've specified. The functions pipeline is executed by the specified trigger in the configuration.toml . The first function in the pipeline is called with the event that triggered the pipeline (ex. events.Model ). Each successive call in the pipeline is called with the return result of the previous function. Let's take a look at a simple example that creates a pipeline to filter particular device ids and subsequently transform the data to XML: package main import ( \"fmt\" \"github.com/edgexfoundry/app-functions-sdk-go/appsdk\" \"github.com/edgexfoundry/app-functions-sdk-go/pkg/transforms\" \"os\" ) func main () { // 1) First thing to do is to create an instance of the EdgeX SDK, giving it a service key edgexSdk := & appsdk . AppFunctionsSDK { ServiceKey : \"SimpleFilterXMLApp\" , // Key used by Registry (Aka Consul) } // 2) Next, we need to initialize the SDK if err := edgexSdk . Initialize (); err != nil { message := fmt . Sprintf ( \"SDK initialization failed: %v\\n\" , err ) if edgexSdk . LoggingClient != nil { edgexSdk . LoggingClient . Error ( message ) } else { fmt . Println ( message ) } os . Exit ( - 1 ) } // 3) Shows how to access the application's specific configuration settings. deviceNames , err := edgexSdk . GetAppSettingStrings ( \"DeviceNames\" ) if err != nil { edgexSdk . LoggingClient . Error ( err . Error ()) os . Exit ( - 1 ) } // 4) This is our pipeline configuration, the collection of functions to // execute every time an event is triggered. if err = edgexSdk . SetFunctionsPipeline ( transforms . NewFilter ( deviceNames ). FilterByDeviceName , transforms . NewConversion (). TransformToXML , ); err != nil { edgexSdk . LoggingClient . Error ( fmt . Sprintf ( \"SDK SetPipeline failed: %v\\n\" , err )) os . Exit ( - 1 ) } // 5) Lastly, we'll go ahead and tell the SDK to \"start\" and begin listening for events to trigger the pipeline. err = edgexSdk . MakeItRun () if err != nil { edgexSdk . LoggingClient . Error ( \"MakeItRun returned error: \" , err . Error ()) os . Exit ( - 1 ) } // Do any required cleanup here os . Exit ( 0 ) } The above example is meant to merely demonstrate the structure of your application. Notice that the output of the last function is not available anywhere inside this application. You must provide a function in order to work with the data from the previous function. Let's go ahead and add the following function that prints the output to the console. func printXMLToConsole ( edgexcontext * appcontext . Context , params ... interface {}) ( bool , interface {}) { if len ( params ) < 1 { // We didn't receive a result return false , errors . New ( \"No Data Received\" ) } println ( params [ 0 ].( string )) return true , nil } After placing the above function in your code, the next step is to modify the pipeline to call this function: Note You can find this example called \"Simple Filter XML\" and more located in the examples section. Up until this point, the pipeline has been triggered by an event over HTTP and the data at the end of that pipeline lands in the last function specified. In the example, data ends up printed to the console. Perhaps we'd like to send the data back to where it came from. In the case of an HTTP trigger, this would be the HTTP response. In the case of a message bus, this could be a new topic to send the data back to for other applications that wish to receive it. To do this, simply call edgexcontext.Complete([]byte outputData) passing in the data you wish to \"respond\" with. In the above printXMLToConsole(...) function, replace println(params[0].(string)) with edgexcontext.Complete([]byte(params[0].(string))) . You should now see the response in your postman window when testing the pipeline.","title":"The Application Functions SDK"},{"location":"getting-started/Ch-GettingStartedCDevelopers/","text":"Getting Started - C Developers Introduction These instructions are for C Developers and Contributors to get, run and otherwise work with C-based EdgeX Foundry micro services. Before reading this guide, review the general developer requirements . If you want to get the EdgeX platform and run it (but do not intend to change or add to the existing code base now) then you are considered a \"User\". Users should read: Getting Started Users ) What You Need For C Development Many of EdgeX device services are built in C. In the future, other services could be built in C. In additional to the hardware and software listed in the Developers guide , to build EdgeX C services, you will need the following: libmicrohttpd libcurl libyaml libcbor You can install these on Ubuntu by running: sudo apt install libcurl4-openssl-dev libmicrohttpd-dev libyaml-dev libcbor-dev Next Steps To explore how to create and build EdgeX device services in C, head to the Device Services, C SDK guide .","title":"Getting Started - C Developers"},{"location":"getting-started/Ch-GettingStartedCDevelopers/#getting-started-c-developers","text":"","title":"Getting Started - C Developers"},{"location":"getting-started/Ch-GettingStartedCDevelopers/#introduction","text":"These instructions are for C Developers and Contributors to get, run and otherwise work with C-based EdgeX Foundry micro services. Before reading this guide, review the general developer requirements . If you want to get the EdgeX platform and run it (but do not intend to change or add to the existing code base now) then you are considered a \"User\". Users should read: Getting Started Users )","title":"Introduction"},{"location":"getting-started/Ch-GettingStartedCDevelopers/#what-you-need-for-c-development","text":"Many of EdgeX device services are built in C. In the future, other services could be built in C. In additional to the hardware and software listed in the Developers guide , to build EdgeX C services, you will need the following: libmicrohttpd libcurl libyaml libcbor You can install these on Ubuntu by running: sudo apt install libcurl4-openssl-dev libmicrohttpd-dev libyaml-dev libcbor-dev","title":"What You Need For C Development"},{"location":"getting-started/Ch-GettingStartedCDevelopers/#next-steps","text":"To explore how to create and build EdgeX device services in C, head to the Device Services, C SDK guide .","title":"Next Steps"},{"location":"getting-started/Ch-GettingStartedDevelopers/","text":"Getting Started - Developers Introduction These instructions are for Developers and Contributors to get and run EdgeX Foundry. If you want to get the EdgeX platform and run it (but do not intend to change or add to the existing code base now) then you are considered a \"User\". Users should read: Getting Started Users ) EdgeX is a collection of more than a dozen micro services that are deployed to provide a minimal edge platform capability. EdgeX consists of a collection of reference implementation services and SDK tools. The micro services and SDKs are written in Go or C. These documentation pages provide a developer with the information and instructions to get and run EdgeX Foundry in development mode - that is running natively outside of containers and with the intent of adding to or changing the existing code base. What You Need Hardware EdgeX Foundry is an operating system (OS) and hardware (HW)-agnostic edge software platform. See the reference page for platform requirements . These provide guidance on a minimal platform to run the EdgeX platform. However, as a developer, you may find that additional memory, disk space, and improved CPU are essential to building and debugging. Software Developers need to install the following software to get, run and develop EdgeX Foundry micro services: Git Use this free and open source version control (SVC) system to download (and upload) the EdgeX Foundry source code from the project's GitHub repositories. See https://git-scm.com/downloads for download and install instructions. Alternative tools (Easy Git for example) could be used, but this document assumes use of git and leaves how to use alternative SVC tools to the reader. Redis By default, EdgeX Foundry uses Redis (version 5 starting with the Geneva release) as the persistence mechanism for sensor data as well as metadata about the devices/sensors that are connected. See https://redis.io/ for download and installation instructions. MongoDB As an alternative, EdgeX Foundry allows use of MongoDB (version 4.2 as of Geneva) as the alternative persistence mechanism in place of Redis for sensor data as well as metadata about the connected devices/sensors. See https://www.mongodb.com/download-center?jmp=nav#community for download and installation instructions. Warning Use of MongoDB is deprecated with the Geneva release. EdgeX will remove MongoDB support in a future release. Developers should start to migrate to Redis in all development efforts targeting future EdgeX releases. ZeroMQ Several EdgeX Foundry services depend on ZeroMQ for communications by default. See the installation for your OS. Linux/Unix The easiest way to get and install ZeroMQ on Linux is to use this setup script: https://gist.github.com/katopz/8b766a5cb0ca96c816658e9407e83d00 . Note The 0MQ install script above assumes bash is available on your system and the bash executable is in /usr/bin. Before running the script at the link, run which bash at your Linux terminal to insure that bash is in /usr/bin. If not, change the first line of the script so that it points to the correct location of bash. MacOS For MacOS, use brew to install ZeroMQ. brew install zeromq Windows For directions installing ZeroMQ on Windows, please see the Windows documentation: https://github.com/edgexfoundry/edgex-go/blob/master/ZMQWindows.md Docker (Optional) If you intend to create Docker images for your updated or newly created EdgeX services, you need to install Docker. See https://docs.docker.com/install/ to learn how to install Docker. If you are new to Docker, the same web site provides you educational information. Additional Programming Tools and Next Steps Depending on which part of EdgeX you work on, you need to install one or more programming languages (Go Lang, Gnu C, etc.) and associated tooling. These tools are covered under the documentation specific to each type of development. Go Lang C","title":"Getting Started - Developers"},{"location":"getting-started/Ch-GettingStartedDevelopers/#getting-started-developers","text":"","title":"Getting Started - Developers"},{"location":"getting-started/Ch-GettingStartedDevelopers/#introduction","text":"These instructions are for Developers and Contributors to get and run EdgeX Foundry. If you want to get the EdgeX platform and run it (but do not intend to change or add to the existing code base now) then you are considered a \"User\". Users should read: Getting Started Users ) EdgeX is a collection of more than a dozen micro services that are deployed to provide a minimal edge platform capability. EdgeX consists of a collection of reference implementation services and SDK tools. The micro services and SDKs are written in Go or C. These documentation pages provide a developer with the information and instructions to get and run EdgeX Foundry in development mode - that is running natively outside of containers and with the intent of adding to or changing the existing code base.","title":"Introduction"},{"location":"getting-started/Ch-GettingStartedDevelopers/#what-you-need","text":"","title":"What You Need"},{"location":"getting-started/Ch-GettingStartedDevelopers/#hardware","text":"EdgeX Foundry is an operating system (OS) and hardware (HW)-agnostic edge software platform. See the reference page for platform requirements . These provide guidance on a minimal platform to run the EdgeX platform. However, as a developer, you may find that additional memory, disk space, and improved CPU are essential to building and debugging.","title":"Hardware"},{"location":"getting-started/Ch-GettingStartedDevelopers/#software","text":"Developers need to install the following software to get, run and develop EdgeX Foundry micro services:","title":"Software"},{"location":"getting-started/Ch-GettingStartedDevelopers/#git","text":"Use this free and open source version control (SVC) system to download (and upload) the EdgeX Foundry source code from the project's GitHub repositories. See https://git-scm.com/downloads for download and install instructions. Alternative tools (Easy Git for example) could be used, but this document assumes use of git and leaves how to use alternative SVC tools to the reader.","title":"Git"},{"location":"getting-started/Ch-GettingStartedDevelopers/#redis","text":"By default, EdgeX Foundry uses Redis (version 5 starting with the Geneva release) as the persistence mechanism for sensor data as well as metadata about the devices/sensors that are connected. See https://redis.io/ for download and installation instructions.","title":"Redis"},{"location":"getting-started/Ch-GettingStartedDevelopers/#mongodb","text":"As an alternative, EdgeX Foundry allows use of MongoDB (version 4.2 as of Geneva) as the alternative persistence mechanism in place of Redis for sensor data as well as metadata about the connected devices/sensors. See https://www.mongodb.com/download-center?jmp=nav#community for download and installation instructions. Warning Use of MongoDB is deprecated with the Geneva release. EdgeX will remove MongoDB support in a future release. Developers should start to migrate to Redis in all development efforts targeting future EdgeX releases.","title":"MongoDB"},{"location":"getting-started/Ch-GettingStartedDevelopers/#zeromq","text":"Several EdgeX Foundry services depend on ZeroMQ for communications by default. See the installation for your OS. Linux/Unix The easiest way to get and install ZeroMQ on Linux is to use this setup script: https://gist.github.com/katopz/8b766a5cb0ca96c816658e9407e83d00 . Note The 0MQ install script above assumes bash is available on your system and the bash executable is in /usr/bin. Before running the script at the link, run which bash at your Linux terminal to insure that bash is in /usr/bin. If not, change the first line of the script so that it points to the correct location of bash. MacOS For MacOS, use brew to install ZeroMQ. brew install zeromq Windows For directions installing ZeroMQ on Windows, please see the Windows documentation: https://github.com/edgexfoundry/edgex-go/blob/master/ZMQWindows.md","title":"ZeroMQ"},{"location":"getting-started/Ch-GettingStartedDevelopers/#docker-optional","text":"If you intend to create Docker images for your updated or newly created EdgeX services, you need to install Docker. See https://docs.docker.com/install/ to learn how to install Docker. If you are new to Docker, the same web site provides you educational information.","title":"Docker (Optional)"},{"location":"getting-started/Ch-GettingStartedDevelopers/#additional-programming-tools-and-next-steps","text":"Depending on which part of EdgeX you work on, you need to install one or more programming languages (Go Lang, Gnu C, etc.) and associated tooling. These tools are covered under the documentation specific to each type of development. Go Lang C","title":"Additional Programming Tools and Next Steps"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/","text":"Getting Started - Go Developers Introduction These instructions are for Go Lang Developers and Contributors to get, run and otherwise work with Go-based EdgeX Foundry micro services. Before reading this guide, review the general developer requirements . If you want to get the EdgeX platform and run it (but do not intend to change or add to the existing code base now) then you are considered a \"User\". Users should read: Getting Started Users ) What You Need For Go Development In additional to the hardware and software listed in the Developers guide , you will need the following to work with the EdgeX Go-based micro services. Go The open sourced micro services of EdgeX Foundry are written in Go 1.13. See https://golang.org/dl/ for download and installation instructions. Newer versions of Go are available and may work, but the project has not built and tested to these newer versions of the language. Older versions of Go, especially 1.10 or older, are likely to cause issues (EdgeX now uses Go Modules which were introduced with Go Lang 1.11). Build Essentials In order to compile and build some elements of EdgeX, Gnu C compiler, utilities (like make), and associated librarires need to be installed. Some IDEs may already come with these tools. Some OS environments may already come with these tools. Others environments may require you install them. For Ubuntu environments, you can install a convenience package called Build Essentials . Note If you are installing Build Essentials, note that there is a build-essential pacakge for each Ubuntu release. Search for 'build-essential' associated to your Ubuntu version via Ubuntu Packages Search . IDE (Optional) There are many tool options for writing and editing Go Lang code. You could use a simple text editor. For more convenience, you may choose to use an integrated development environment (IDE). The list below highlights IDEs used by some of the EdgeX community (without any project endorsement). GoLand GoLand is a popular, although subscription-fee based, Go specific IDE. Learn how to purchase and download Go Land here: https://www.jetbrains.com/go/ . Visual Studio Code Visual Studio Code is a free, open source IDE developed by Microsoft. Find and download Visual Studio Code here: https://code.visualstudio.com/ . Atom Atom is also a free, open source IDE used with many languages. Find and download Atom here: https://ide.atom.io/ . Get the code This part of the documentation assumes you wish to get and work with the key EdgeX services. This includes but is not limited to Core, Supporting, some security, and system management services. To work with other Go-based security services, device services, application services, SDKs, user interface, or other service you may need to pull in other EdgeX repository code. See other getting started guides for working with other Go-based services. As you will see below, you do not need to explicitly pull in dependency modules (whether EdgeX or 3rd party provided). Dependencies will automatically be pulled through the building process. To work with the key services, you will need to download the source code from the EdgeX Go repository. The EdgeX Go-based micro services are all available in a single GitHub repository download. Once the code is pulled, the Go micro services are built and packaged as platform dependent executables. If Docker is installed, the executable can also be containerized for end user deployment/use. The EdgeX Foundry Go Lang micro service code is hosted at https://github.com/edgexfoundry/edgex-go . To download the EdgeX Go code, first change directories to the location where you want to download the code (to edgex in the image below). Then use your git tool and request to clone this repository with the following command: git clone <https://github.com/edgexfoundry/edgex-go.git> Note If you plan to contribute code back to the EdgeX project (as a Contributor), you are going to want to fork the repositories you plan to work with and then pull your fork versus the EdgeX repositories directly. This documentation does not address the process and procedures for working with an EdgeX fork, committing changes and submitting contribution pull requests (PRs). See some of the links below in the EdgeX Wiki for help on how to fork and contribute EdgeX code. https://wiki.edgexfoundry.org/display/FA/Contributor%27s+Guide https://wiki.edgexfoundry.org/display/FA/Contributor%27s+Guide+-+Go+Lang https://wiki.edgexfoundry.org/display/FA/Contributor+Process?searchId=AW768BAW7 Build EdgeX Foundry To build the Go Lang services found in edgex-go, first change directories to the root of the edgex-go code cd edgex-go Second, use the community provided Makefile to build all the services in a single call make build Info The first time EdgeX builds, it will take longer than other builds as it has to download all dependencies. Depending on the size of your host machine, an initial build can take several minutes. Make sure the build completes and has no errors. If it does build, you should find new service executables in each of the service folders under the service directories found in the /edgex-go/cmd folder. Run EdgeX Foundry Run the Database Several of the EdgeX Foundry micro services use a database. This includes core-data, core-metadata, support-scheduler, among others. Therefore, when working with EdgeX Foundry its a good idea to have the database up and running as a general rule. See the Redis Quick Start Guide for how to run Redis in a Linux environment (or find similar documentation for other environments). Note MongoDB can run in place of Redis with the Geneva release or earlier. MongoDB is deprecated and developers should transition to Redis. See the Run MongoDB documenation for how to run Mongo in a Linux environment (or find similar documentation for other environments). Running MongoDB in place of Redis will also require that you alter the configuration for all services that need the database to use MongoDB instead of Redis. As an example, the configuration of core-data is located in the file edgex-go/cmd/core-data/res/configuration.toml. In the configuration.toml file of the affected services, find the [Databases] section and change the \"Type\" to 'mongodb' along with any associated connection information similar to that shown below Host = 'localhost' Name = 'coredata' Password = 'password' Port = 27017 Username = 'core' Timeout = 5000 Type = 'mongodb' Run EdgeX Services With the services built, and the database up and running, you can now run all the services via second make command. Simply call make run This will start the EdgeX go services and leave them running until you terminate the process (with Ctrl-C). The log entries from each service will start to display in the terminal. Watch the log entries for any ERROR indicators. While the EdgeX services are running you can make EdgeX API calls to localhost . Note Use the ampersand ('&') sign at the end of make run if you wish to run the services in the background in detached mode. In so doing, Ctrl-C will not stop the services. You will have to kill the services by other means. Info No sensor data will flow yet as this just gets the key services up and running. To get sensor data flowing into EdgeX, you will need to get, build and run an EdgeX device service in a similar fashion. The community provides a virtual device service to test and experiment with ( https://github.com/edgexfoundry/device-virtual-go ). Verify EdgeX is Working Each EdgeX micro service has a built-in respond to a \"ping\" HTTP request. In networking environments, use a ping request to check the reach-ability of a network resource. EdgeX uses the same concept to check the availability or reach-ability of a micro service. After the EdgeX micro services are running, you can \"ping\" any one of the micro services to check that it is running. Open a browser or HTTP REST client tool and use the service's ping address (outlined below) to check that is available. http://localhost:[port]/api/v1/ping See EdgeX Default Service Ports for a list of the EdgeX default service ports. \"Pinging\" an EdgeX micro service allows you to check on its availability. If the service does not respond to ping, the service is down or having issues. Next Steps Application services and some device services are also built in Go. To explore how to create and build EdgeX application and devices services in Go, head to SDK documentation covering these EdgeX elements. Application Services and the Application Functions SDK Device Services in Go EdgeX Foundry in GoLand IDEs offer many code editing conveniences. Go Land was specifically built to edit and work with Go code. So if you are doing any significant code work with the EdgeX Go micro services, you will likely find it convenient to edit, build, run, test, etc. from GoLand or other IDE. Import EdgeX To bring in the EdgeX repository code into Go Land, use the File \u2192 Open... menu option in Go Land to open the Open File or Project Window. In the \"Open File or Project\" popup, select the location of the folder containing your cloned edgex-go repo. Open the Terminal From the View menu in Go Land, select the Terminal menu option. This will open a command terminal from which you can issue commands to install the dependencies, build the micro services, run the micro services, etc. Build the EdgeX Micro Services Run \"make build\" in the Terminal view (as shown below) to build the services. This can take a few minutes to build all the services. Warning In some cases, Go Land IDE may encounter an error (go: parsing \\$GOFLAGS: non-flag \"\"-X\") when building as shown below. If you encounter this issue, unset the GOFLAGS env var in GoLand. Make a call to unset GOFLAGS as shown below and then call make build again. Just as when running make build from the command line in a terminal, the micro service executables that get built in Go Land's terminal will be created in each of the service folders under the service directories found in the /edgex-go/cmd folder.. Run EdgeX With all the micro services built, you can now run EdgeX. You may first want to make sure the database is running. Then issue the command make run in the terminal. You can now call on the service APIs to make sure they are running correctly. Namely, call on localhost:[service port]/api/v1/ping to see each service respond to the simplest of requests.","title":"Getting Started - Go Developers"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#getting-started-go-developers","text":"","title":"Getting Started - Go Developers"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#introduction","text":"These instructions are for Go Lang Developers and Contributors to get, run and otherwise work with Go-based EdgeX Foundry micro services. Before reading this guide, review the general developer requirements . If you want to get the EdgeX platform and run it (but do not intend to change or add to the existing code base now) then you are considered a \"User\". Users should read: Getting Started Users )","title":"Introduction"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#what-you-need-for-go-development","text":"In additional to the hardware and software listed in the Developers guide , you will need the following to work with the EdgeX Go-based micro services.","title":"What You Need For Go Development"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#go","text":"The open sourced micro services of EdgeX Foundry are written in Go 1.13. See https://golang.org/dl/ for download and installation instructions. Newer versions of Go are available and may work, but the project has not built and tested to these newer versions of the language. Older versions of Go, especially 1.10 or older, are likely to cause issues (EdgeX now uses Go Modules which were introduced with Go Lang 1.11).","title":"Go"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#build-essentials","text":"In order to compile and build some elements of EdgeX, Gnu C compiler, utilities (like make), and associated librarires need to be installed. Some IDEs may already come with these tools. Some OS environments may already come with these tools. Others environments may require you install them. For Ubuntu environments, you can install a convenience package called Build Essentials . Note If you are installing Build Essentials, note that there is a build-essential pacakge for each Ubuntu release. Search for 'build-essential' associated to your Ubuntu version via Ubuntu Packages Search .","title":"Build Essentials"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#ide-optional","text":"There are many tool options for writing and editing Go Lang code. You could use a simple text editor. For more convenience, you may choose to use an integrated development environment (IDE). The list below highlights IDEs used by some of the EdgeX community (without any project endorsement).","title":"IDE (Optional)"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#goland","text":"GoLand is a popular, although subscription-fee based, Go specific IDE. Learn how to purchase and download Go Land here: https://www.jetbrains.com/go/ .","title":"GoLand"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#visual-studio-code","text":"Visual Studio Code is a free, open source IDE developed by Microsoft. Find and download Visual Studio Code here: https://code.visualstudio.com/ .","title":"Visual Studio Code"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#atom","text":"Atom is also a free, open source IDE used with many languages. Find and download Atom here: https://ide.atom.io/ .","title":"Atom"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#get-the-code","text":"This part of the documentation assumes you wish to get and work with the key EdgeX services. This includes but is not limited to Core, Supporting, some security, and system management services. To work with other Go-based security services, device services, application services, SDKs, user interface, or other service you may need to pull in other EdgeX repository code. See other getting started guides for working with other Go-based services. As you will see below, you do not need to explicitly pull in dependency modules (whether EdgeX or 3rd party provided). Dependencies will automatically be pulled through the building process. To work with the key services, you will need to download the source code from the EdgeX Go repository. The EdgeX Go-based micro services are all available in a single GitHub repository download. Once the code is pulled, the Go micro services are built and packaged as platform dependent executables. If Docker is installed, the executable can also be containerized for end user deployment/use. The EdgeX Foundry Go Lang micro service code is hosted at https://github.com/edgexfoundry/edgex-go . To download the EdgeX Go code, first change directories to the location where you want to download the code (to edgex in the image below). Then use your git tool and request to clone this repository with the following command: git clone <https://github.com/edgexfoundry/edgex-go.git> Note If you plan to contribute code back to the EdgeX project (as a Contributor), you are going to want to fork the repositories you plan to work with and then pull your fork versus the EdgeX repositories directly. This documentation does not address the process and procedures for working with an EdgeX fork, committing changes and submitting contribution pull requests (PRs). See some of the links below in the EdgeX Wiki for help on how to fork and contribute EdgeX code. https://wiki.edgexfoundry.org/display/FA/Contributor%27s+Guide https://wiki.edgexfoundry.org/display/FA/Contributor%27s+Guide+-+Go+Lang https://wiki.edgexfoundry.org/display/FA/Contributor+Process?searchId=AW768BAW7","title":"Get the code"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#build-edgex-foundry","text":"To build the Go Lang services found in edgex-go, first change directories to the root of the edgex-go code cd edgex-go Second, use the community provided Makefile to build all the services in a single call make build Info The first time EdgeX builds, it will take longer than other builds as it has to download all dependencies. Depending on the size of your host machine, an initial build can take several minutes. Make sure the build completes and has no errors. If it does build, you should find new service executables in each of the service folders under the service directories found in the /edgex-go/cmd folder.","title":"Build EdgeX Foundry"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#run-edgex-foundry","text":"","title":"Run EdgeX Foundry"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#run-the-database","text":"Several of the EdgeX Foundry micro services use a database. This includes core-data, core-metadata, support-scheduler, among others. Therefore, when working with EdgeX Foundry its a good idea to have the database up and running as a general rule. See the Redis Quick Start Guide for how to run Redis in a Linux environment (or find similar documentation for other environments). Note MongoDB can run in place of Redis with the Geneva release or earlier. MongoDB is deprecated and developers should transition to Redis. See the Run MongoDB documenation for how to run Mongo in a Linux environment (or find similar documentation for other environments). Running MongoDB in place of Redis will also require that you alter the configuration for all services that need the database to use MongoDB instead of Redis. As an example, the configuration of core-data is located in the file edgex-go/cmd/core-data/res/configuration.toml. In the configuration.toml file of the affected services, find the [Databases] section and change the \"Type\" to 'mongodb' along with any associated connection information similar to that shown below Host = 'localhost' Name = 'coredata' Password = 'password' Port = 27017 Username = 'core' Timeout = 5000 Type = 'mongodb'","title":"Run the Database"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#run-edgex-services","text":"With the services built, and the database up and running, you can now run all the services via second make command. Simply call make run This will start the EdgeX go services and leave them running until you terminate the process (with Ctrl-C). The log entries from each service will start to display in the terminal. Watch the log entries for any ERROR indicators. While the EdgeX services are running you can make EdgeX API calls to localhost . Note Use the ampersand ('&') sign at the end of make run if you wish to run the services in the background in detached mode. In so doing, Ctrl-C will not stop the services. You will have to kill the services by other means. Info No sensor data will flow yet as this just gets the key services up and running. To get sensor data flowing into EdgeX, you will need to get, build and run an EdgeX device service in a similar fashion. The community provides a virtual device service to test and experiment with ( https://github.com/edgexfoundry/device-virtual-go ).","title":"Run EdgeX Services"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#verify-edgex-is-working","text":"Each EdgeX micro service has a built-in respond to a \"ping\" HTTP request. In networking environments, use a ping request to check the reach-ability of a network resource. EdgeX uses the same concept to check the availability or reach-ability of a micro service. After the EdgeX micro services are running, you can \"ping\" any one of the micro services to check that it is running. Open a browser or HTTP REST client tool and use the service's ping address (outlined below) to check that is available. http://localhost:[port]/api/v1/ping See EdgeX Default Service Ports for a list of the EdgeX default service ports. \"Pinging\" an EdgeX micro service allows you to check on its availability. If the service does not respond to ping, the service is down or having issues.","title":"Verify EdgeX is Working"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#next-steps","text":"Application services and some device services are also built in Go. To explore how to create and build EdgeX application and devices services in Go, head to SDK documentation covering these EdgeX elements. Application Services and the Application Functions SDK Device Services in Go","title":"Next Steps"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#edgex-foundry-in-goland","text":"IDEs offer many code editing conveniences. Go Land was specifically built to edit and work with Go code. So if you are doing any significant code work with the EdgeX Go micro services, you will likely find it convenient to edit, build, run, test, etc. from GoLand or other IDE.","title":"EdgeX Foundry in GoLand"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#import-edgex","text":"To bring in the EdgeX repository code into Go Land, use the File \u2192 Open... menu option in Go Land to open the Open File or Project Window. In the \"Open File or Project\" popup, select the location of the folder containing your cloned edgex-go repo.","title":"Import EdgeX"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#open-the-terminal","text":"From the View menu in Go Land, select the Terminal menu option. This will open a command terminal from which you can issue commands to install the dependencies, build the micro services, run the micro services, etc.","title":"Open the Terminal"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#build-the-edgex-micro-services","text":"Run \"make build\" in the Terminal view (as shown below) to build the services. This can take a few minutes to build all the services. Warning In some cases, Go Land IDE may encounter an error (go: parsing \\$GOFLAGS: non-flag \"\"-X\") when building as shown below. If you encounter this issue, unset the GOFLAGS env var in GoLand. Make a call to unset GOFLAGS as shown below and then call make build again. Just as when running make build from the command line in a terminal, the micro service executables that get built in Go Land's terminal will be created in each of the service folders under the service directories found in the /edgex-go/cmd folder..","title":"Build the EdgeX Micro Services"},{"location":"getting-started/Ch-GettingStartedGoDevelopers/#run-edgex","text":"With all the micro services built, you can now run EdgeX. You may first want to make sure the database is running. Then issue the command make run in the terminal. You can now call on the service APIs to make sure they are running correctly. Namely, call on localhost:[service port]/api/v1/ping to see each service respond to the simplest of requests.","title":"Run EdgeX"},{"location":"getting-started/Ch-GettingStartedHybrid/","text":"Working in a Hybrid Environment In some cases, as a developer or contributor , you want to work on a particular micro service. Yet, you don't want to have to download all the source code, and then build and run for all the micro services. In this case, you can download and run the EdgeX Docker containers for all the micro services you need and run your single micro service (the one you are presumably working on) natively or from a developer tool of choice outside of a container. Within EdgeX, we call this a \"hybrid\" environment - where part of your EdgeX platform is running from a development environment, while other parts are running from the Dockerized containers. This page outlines how to do hybrid development. As an example of this process, let's say you want to do coding work with/on the Virtual Device service. You want the rest of the EdgeX environment up and running via Docker containers. How would you set up this hybrid environment? Let's take a look. Get and Run the EdgeX Docker Containers If you haven't already, follow the Getting Started with Docker Guide before continuing. Since you plan to work with the virtual device service, you probably don't need or want to run all the micro services. You just need the few that the Virtual Device will be communicating with or that will be required to run a minimal EdgeX environment. So you will need to run Consul, Redis, Core Data, Core Metadata, Support Notifications, and Core Command. Based on the instructions found in the Getting Started with Docker , locate and download the appropriate Docker Compose file for your development environment. Next, issue the following commands to start this set of EdgeX containers - providing a minimal functioning EdgeX environment. docker-compose up -d consul docker-compose up -d redis docker-compose up -d notifications docker-compose up -d metadata docker-compose up -d data docker-compose up -d command Note These notes assume you are working with the EdgeX Genva release. Some versions of EdgeX may require other or additional containers to run. Run the command below to confirm that all the containers have started. docker-compose ps Get, Build and Run the (non-Docker) Service With the EdgeX containers running, you can now download, build and run natively (outside of a container) the service you want to work on. In this example, the virtual device service is used to exemplify the steps necessary to get, build and run the native service with the EdgeX containerized services. However, the practice could be applied to any service. Get the service code Per Getting Started Go Developers , pull the micro service code you want to work on from GitHub. In this example, we assume you want to get the device-virtual-go. git clone https://github.com/edgexfoundry/device-virtual-go.git Build the service code At this time, you can add or modify the code to make the service changes you need. Once ready, you must compile and build the service into an executable. Change folders to the cloned micro service directory and build the service. cd device-virtual-go/ make build Change the configuration Depending on the service you are working on, you may need to change the configuration of the service to point to and use the other services that are containerized (running in Docker). In particular, if the service you are working on is not on the same host as the Docker Engine running the containerized services, you will likely need to change the configuration. Examine the configuration.toml file in the cmd/res folder of the device-virtual-go. Note that the Registry (located in the [Registry] section of the configuration) and all the \"clients\" (located in the [clients] section of the configuration file) suggest that the \"Host\" of these services is \"localhost\". These and other host configuration elements need to change when the services are not running on the same host. If you do have to change the configuration, save the configuration.toml file after making changes. Run the service code natively. The executable created by the make command is usually found in the cmd folder of the service. cd cmd ./device-virtual Check the results At this time, your virtual device micro service should be communicating with the other EdgeX micro services running in their Docker containers. Give the virtual device a few seconds or so to initialize itself and start sending data to Core Data. To check that it is working properly, open a browser and point your browser to Core Data to check that events are being deposited. You can do this by calling on the Core Data API that checks the count of events in Core Data http://[host].48080/api/v1/event/count. Note If you choose, you can also import the service into GoLand and then code and run the service from GoLand. Follow the instructions in the Getting Started - Go Developers to learn how to import, build and run a service in GoLand.","title":"Working in a Hybrid Environment"},{"location":"getting-started/Ch-GettingStartedHybrid/#working-in-a-hybrid-environment","text":"In some cases, as a developer or contributor , you want to work on a particular micro service. Yet, you don't want to have to download all the source code, and then build and run for all the micro services. In this case, you can download and run the EdgeX Docker containers for all the micro services you need and run your single micro service (the one you are presumably working on) natively or from a developer tool of choice outside of a container. Within EdgeX, we call this a \"hybrid\" environment - where part of your EdgeX platform is running from a development environment, while other parts are running from the Dockerized containers. This page outlines how to do hybrid development. As an example of this process, let's say you want to do coding work with/on the Virtual Device service. You want the rest of the EdgeX environment up and running via Docker containers. How would you set up this hybrid environment? Let's take a look.","title":"Working in a Hybrid Environment"},{"location":"getting-started/Ch-GettingStartedHybrid/#get-and-run-the-edgex-docker-containers","text":"If you haven't already, follow the Getting Started with Docker Guide before continuing. Since you plan to work with the virtual device service, you probably don't need or want to run all the micro services. You just need the few that the Virtual Device will be communicating with or that will be required to run a minimal EdgeX environment. So you will need to run Consul, Redis, Core Data, Core Metadata, Support Notifications, and Core Command. Based on the instructions found in the Getting Started with Docker , locate and download the appropriate Docker Compose file for your development environment. Next, issue the following commands to start this set of EdgeX containers - providing a minimal functioning EdgeX environment. docker-compose up -d consul docker-compose up -d redis docker-compose up -d notifications docker-compose up -d metadata docker-compose up -d data docker-compose up -d command Note These notes assume you are working with the EdgeX Genva release. Some versions of EdgeX may require other or additional containers to run. Run the command below to confirm that all the containers have started. docker-compose ps","title":"Get and Run the EdgeX Docker Containers"},{"location":"getting-started/Ch-GettingStartedHybrid/#get-build-and-run-the-non-docker-service","text":"With the EdgeX containers running, you can now download, build and run natively (outside of a container) the service you want to work on. In this example, the virtual device service is used to exemplify the steps necessary to get, build and run the native service with the EdgeX containerized services. However, the practice could be applied to any service.","title":"Get, Build and Run the (non-Docker) Service"},{"location":"getting-started/Ch-GettingStartedHybrid/#get-the-service-code","text":"Per Getting Started Go Developers , pull the micro service code you want to work on from GitHub. In this example, we assume you want to get the device-virtual-go. git clone https://github.com/edgexfoundry/device-virtual-go.git","title":"Get the service code"},{"location":"getting-started/Ch-GettingStartedHybrid/#build-the-service-code","text":"At this time, you can add or modify the code to make the service changes you need. Once ready, you must compile and build the service into an executable. Change folders to the cloned micro service directory and build the service. cd device-virtual-go/ make build","title":"Build the service code"},{"location":"getting-started/Ch-GettingStartedHybrid/#change-the-configuration","text":"Depending on the service you are working on, you may need to change the configuration of the service to point to and use the other services that are containerized (running in Docker). In particular, if the service you are working on is not on the same host as the Docker Engine running the containerized services, you will likely need to change the configuration. Examine the configuration.toml file in the cmd/res folder of the device-virtual-go. Note that the Registry (located in the [Registry] section of the configuration) and all the \"clients\" (located in the [clients] section of the configuration file) suggest that the \"Host\" of these services is \"localhost\". These and other host configuration elements need to change when the services are not running on the same host. If you do have to change the configuration, save the configuration.toml file after making changes.","title":"Change the configuration"},{"location":"getting-started/Ch-GettingStartedHybrid/#run-the-service-code-natively","text":"The executable created by the make command is usually found in the cmd folder of the service. cd cmd ./device-virtual","title":"Run the service code natively."},{"location":"getting-started/Ch-GettingStartedHybrid/#check-the-results","text":"At this time, your virtual device micro service should be communicating with the other EdgeX micro services running in their Docker containers. Give the virtual device a few seconds or so to initialize itself and start sending data to Core Data. To check that it is working properly, open a browser and point your browser to Core Data to check that events are being deposited. You can do this by calling on the Core Data API that checks the count of events in Core Data http://[host].48080/api/v1/event/count. Note If you choose, you can also import the service into GoLand and then code and run the service from GoLand. Follow the instructions in the Getting Started - Go Developers to learn how to import, build and run a service in GoLand.","title":"Check the results"},{"location":"getting-started/Ch-GettingStartedSDK-C/","text":"C SDK In this guide, you create a simple device service in C that generates a random number in place of getting data from an actual sensor. In this way, you get to explore some of the scaffolding and work necessary to complete a device service without actually having a device to talk to. Install dependencies To build a device service using the EdgeX C SDK you'll need the following: - libmicrohttpd - libcurl - libyaml - libcbor You can install these on Ubuntu by running: sudo apt install libcurl4-openssl-dev libmicrohttpd-dev libyaml-dev libcbor-dev Get the EdgeX Device SDK for C The next step is to download and build the EdgeX Device SDK for C. You always want to use the release of the SDK that matches the release of EdgeX you are targeting. As of this writing the fuji release is the current stable release of EdgeX, so we will be using the fuji branch of the C SDK. First, clone the fuji branch of device-sdk-c from Github: git clone -b fuji https://github.com/edgexfoundry/device-sdk-c.git cd ./device-sdk-c Then, build the device-sdk-c: ./scripts/build.sh Starting a new Device Service project For this guide we're going to use the example template provided by the C SDK as a starting point, and will modify it to generate random integer values. Begin by copying the template example source into a new directory named example-device-c : mkdir -p ../example-device-c/res cp ./src/c/examples/template.c ../example-device-c cd ../example-device-c Build your Device Service Now you are ready to build your new device service using the C SDK you compiled in an earlier step. Tell the compiler where to find the C SDK files: export CSDK_DIR=../device-sdk-c/build/release/_CPack_Packages/Linux/TGZ/csdk-1.0.0 Note The exact path to your compiled CSDK_DIR may differ, depending on the tagged version number on the SDK Now you can build your device service executable: gcc -I$CSDK_DIR/include -L$CSDK_DIR/lib -o device-example-c template.c -lcsdk Customize your Device Service Up to now you've been building the example device service provided by the C SDK. In order to change it to a device service that generates random numbers, you need to modify your template.c method template_get_handler so that it reads as follows: for ( uint32_t i = 0 ; i < nreadings ; i ++ ) { const edgex_nvpairs * current = requests [ i ]. attributes ; while ( current != NULL ) { if ( strcmp ( current -> name , \"type\" ) == 0 ) { /* Set the resulting reading type as Uint64 */ readings [ i ]. type = Uint64 ; if ( strcmp ( current -> value , \"random\" ) == 0 ) { /* Set the reading as a random value between 0 and 100 */ readings [ i ]. value . ui64_result = rand () % 100 ; } } current = current -> next ; } } return true ; Creating your Device Profile A Device Profile is a YAML file that describes a class of device to EdgeX. General characteristics about the type of device, the data these devices provide, and how to command the device is all provided in a Device Profile. Device Services use the Device Profile to understand what data is being collected from the Device (in some cases providing information used by the Device Service to know how to communicate with the device and get the desired sensor readings). A Device Profile is needed to describe the data that will be collected from the simple random number generating Device Service. Explore the files in the src/c/examples/res folder. Take note of the example Device Profile YAML file that is already there (TemplateProfile.yaml). You can explore the contents of this file to see how devices are represented by YAML. In particular, note how fields or properties of a sensor are represented by \"deviceResources\". Commands to be issued to the device are represented by \"coreCommands\". Download this random-generator-device.yaml <random-generator-device.yaml> {.interpreted-text role=\"download\"} into the ./res folder. You can open random-generator-device.yaml in a text editor. In this Device Profile, you are suggesting that the device you are describing to EdgeX has a single property (or deviceResource) which EdgeX should know about - in this case, the property is the \"randomnumber\". Note how the deviceResource is typed. In more real world IoT situations, this deviceResource list could be extensive and could be filled with all different types of data. Note also how the Device Profile describes REST commands that can be used by others to call on (or \"get\") the random number from the Device Service. Configuring your Device Service You will now update the configuration for your new Device Service -- changing the port it operates on (so as not to conflict with other Device Services), altering the scheduled times of when the data is collected from the Device Service (every 10 seconds), and setting up the initial provisioning of the random number generating device when the service starts. Download this configuration.toml <configuration.toml> {.interpreted-text role=\"download\"} to the ./res folder. If you will be running EdgeX inside of Docker containers (which you will at the bottom of this guide) you need to tell your new Device Service to listen on the Docker host IP address (172.17.0.1) instead of localhost . To do that, modify the configuration.toml file so that the top section looks like this: [Service] Host = \"172.17.0.1\" Port = 49992 Rebuild your Device Service Now you have your new Device Service, modified to return a random number, a Device Profile that will tell EdgeX how to read that random number, as well as a configuration file that will let your Device Service register itself and it's Device Profile with EdgeX, and begin taking readings every 10 seconds. Rebuild your Device Service to reflect the changes that you have made: gcc -I$CSDK_DIR/include -L$CSDK_DIR/lib -o device-example-c template.c -lcsdk Run your Device Service Allow your newly created Device Service, which was formed out of the Device Service C SDK, to create sensor mimicking data which it then sends to EdgeX. Follow the Getting Started Users guide to start all of the EdgeX services in Docker. From the folder containing the docker-compose file, start EdgeX with a call to: docker-compose up -d Back in your custom Device Service directory, tell your device service where to find the libcsdk.so : export LD_LIBRARY_PATH=$CSDK_DIR/lib Run your device service: ./device-example-c You should now see your Device Service having it's /Random command called every 10 seconds. You can verify that it is sending data into EdgeX by watching the logs of the edgex-core-data service: docker logs -f edgex-core-data Which would print an Event record every time your Device Service is called. You can manually generate an event using curl to query the device service directly: curl 0:49992/api/v1/device/name/RandNum-Device01/Random Note that the value of the \"randomnumber\" reading is an integer between 0 and 100: { \"device\" : \"RandNum-Device01\" , \"origin\" : 1559317102457 , \"readings\" :[{ \"name\" : \"randomnumber\" , \"value\" : \"63\" }]}","title":"C SDK"},{"location":"getting-started/Ch-GettingStartedSDK-C/#c-sdk","text":"In this guide, you create a simple device service in C that generates a random number in place of getting data from an actual sensor. In this way, you get to explore some of the scaffolding and work necessary to complete a device service without actually having a device to talk to.","title":"C SDK"},{"location":"getting-started/Ch-GettingStartedSDK-C/#install-dependencies","text":"To build a device service using the EdgeX C SDK you'll need the following: - libmicrohttpd - libcurl - libyaml - libcbor You can install these on Ubuntu by running: sudo apt install libcurl4-openssl-dev libmicrohttpd-dev libyaml-dev libcbor-dev","title":"Install dependencies"},{"location":"getting-started/Ch-GettingStartedSDK-C/#get-the-edgex-device-sdk-for-c","text":"The next step is to download and build the EdgeX Device SDK for C. You always want to use the release of the SDK that matches the release of EdgeX you are targeting. As of this writing the fuji release is the current stable release of EdgeX, so we will be using the fuji branch of the C SDK. First, clone the fuji branch of device-sdk-c from Github: git clone -b fuji https://github.com/edgexfoundry/device-sdk-c.git cd ./device-sdk-c Then, build the device-sdk-c: ./scripts/build.sh","title":"Get the EdgeX Device SDK for C"},{"location":"getting-started/Ch-GettingStartedSDK-C/#starting-a-new-device-service-project","text":"For this guide we're going to use the example template provided by the C SDK as a starting point, and will modify it to generate random integer values. Begin by copying the template example source into a new directory named example-device-c : mkdir -p ../example-device-c/res cp ./src/c/examples/template.c ../example-device-c cd ../example-device-c","title":"Starting a new Device Service project"},{"location":"getting-started/Ch-GettingStartedSDK-C/#build-your-device-service","text":"Now you are ready to build your new device service using the C SDK you compiled in an earlier step. Tell the compiler where to find the C SDK files: export CSDK_DIR=../device-sdk-c/build/release/_CPack_Packages/Linux/TGZ/csdk-1.0.0 Note The exact path to your compiled CSDK_DIR may differ, depending on the tagged version number on the SDK Now you can build your device service executable: gcc -I$CSDK_DIR/include -L$CSDK_DIR/lib -o device-example-c template.c -lcsdk","title":"Build your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-C/#customize-your-device-service","text":"Up to now you've been building the example device service provided by the C SDK. In order to change it to a device service that generates random numbers, you need to modify your template.c method template_get_handler so that it reads as follows: for ( uint32_t i = 0 ; i < nreadings ; i ++ ) { const edgex_nvpairs * current = requests [ i ]. attributes ; while ( current != NULL ) { if ( strcmp ( current -> name , \"type\" ) == 0 ) { /* Set the resulting reading type as Uint64 */ readings [ i ]. type = Uint64 ; if ( strcmp ( current -> value , \"random\" ) == 0 ) { /* Set the reading as a random value between 0 and 100 */ readings [ i ]. value . ui64_result = rand () % 100 ; } } current = current -> next ; } } return true ;","title":"Customize your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-C/#creating-your-device-profile","text":"A Device Profile is a YAML file that describes a class of device to EdgeX. General characteristics about the type of device, the data these devices provide, and how to command the device is all provided in a Device Profile. Device Services use the Device Profile to understand what data is being collected from the Device (in some cases providing information used by the Device Service to know how to communicate with the device and get the desired sensor readings). A Device Profile is needed to describe the data that will be collected from the simple random number generating Device Service. Explore the files in the src/c/examples/res folder. Take note of the example Device Profile YAML file that is already there (TemplateProfile.yaml). You can explore the contents of this file to see how devices are represented by YAML. In particular, note how fields or properties of a sensor are represented by \"deviceResources\". Commands to be issued to the device are represented by \"coreCommands\". Download this random-generator-device.yaml <random-generator-device.yaml> {.interpreted-text role=\"download\"} into the ./res folder. You can open random-generator-device.yaml in a text editor. In this Device Profile, you are suggesting that the device you are describing to EdgeX has a single property (or deviceResource) which EdgeX should know about - in this case, the property is the \"randomnumber\". Note how the deviceResource is typed. In more real world IoT situations, this deviceResource list could be extensive and could be filled with all different types of data. Note also how the Device Profile describes REST commands that can be used by others to call on (or \"get\") the random number from the Device Service.","title":"Creating your Device Profile"},{"location":"getting-started/Ch-GettingStartedSDK-C/#configuring-your-device-service","text":"You will now update the configuration for your new Device Service -- changing the port it operates on (so as not to conflict with other Device Services), altering the scheduled times of when the data is collected from the Device Service (every 10 seconds), and setting up the initial provisioning of the random number generating device when the service starts. Download this configuration.toml <configuration.toml> {.interpreted-text role=\"download\"} to the ./res folder. If you will be running EdgeX inside of Docker containers (which you will at the bottom of this guide) you need to tell your new Device Service to listen on the Docker host IP address (172.17.0.1) instead of localhost . To do that, modify the configuration.toml file so that the top section looks like this: [Service] Host = \"172.17.0.1\" Port = 49992","title":"Configuring your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-C/#rebuild-your-device-service","text":"Now you have your new Device Service, modified to return a random number, a Device Profile that will tell EdgeX how to read that random number, as well as a configuration file that will let your Device Service register itself and it's Device Profile with EdgeX, and begin taking readings every 10 seconds. Rebuild your Device Service to reflect the changes that you have made: gcc -I$CSDK_DIR/include -L$CSDK_DIR/lib -o device-example-c template.c -lcsdk","title":"Rebuild your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-C/#run-your-device-service","text":"Allow your newly created Device Service, which was formed out of the Device Service C SDK, to create sensor mimicking data which it then sends to EdgeX. Follow the Getting Started Users guide to start all of the EdgeX services in Docker. From the folder containing the docker-compose file, start EdgeX with a call to: docker-compose up -d Back in your custom Device Service directory, tell your device service where to find the libcsdk.so : export LD_LIBRARY_PATH=$CSDK_DIR/lib Run your device service: ./device-example-c You should now see your Device Service having it's /Random command called every 10 seconds. You can verify that it is sending data into EdgeX by watching the logs of the edgex-core-data service: docker logs -f edgex-core-data Which would print an Event record every time your Device Service is called. You can manually generate an event using curl to query the device service directly: curl 0:49992/api/v1/device/name/RandNum-Device01/Random Note that the value of the \"randomnumber\" reading is an integer between 0 and 100: { \"device\" : \"RandNum-Device01\" , \"origin\" : 1559317102457 , \"readings\" :[{ \"name\" : \"randomnumber\" , \"value\" : \"63\" }]}","title":"Run your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-Go/","text":"Golang SDK The EdgeX Device Service SDK helps developers quickly create new device connectors for EdgeX by providing the common framework that each Device Service needs. The framework provides a pattern for provisioning devices. It provides common template code to receive and react to command (a.k.a. actuation) requests. Finally, the framework provides the common code to help get the data coming from the sensor into EdgeX Core Data (often referred to as data ingestion). With the SDK, developers are left to focus on the code that is specific to the communications with the device via the protocol of the device. In this guide, you create a simple Device Service that generates a random number in place of getting data from an actual sensor. In this way, you get to explore some of the framework and work necessary to complete a Device Service without actually having a device to talk to. Install dependencies Creating a Device Service requires a little programming in Go. Go Lang (version 1.11 or better) must be installed on your system to complete this lab. Follow the instructions in the link below to install Go if it is not already installed on your platform: https://golang.org/doc/install You need a Git tool to pull the Device Service Go SDK code from the EdgeX Foundry Git repository. Follow the instructions in the link below to install Git for your platform: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git You will also need a \"make\" program. On Ubuntu Linux environments, this can be accomplished with the following command: sudo apt install build-essential Finally, you need a simple text editor (or Go Lang IDE). Get the EdgeX Device SDK for Go Complete the following steps to create a folder on your file system, download the Device SDK , then you pull the SDK to your system, and finally create the new EdgeX Device Service from the SDK templating code. Create a collection of nested folders, ~/go/src/github.com/edgexfoundry on your file system. This folder will eventually hold your new Device Service. In Linux, this can be done with a single mkdir (with -p switch) command: mkdir -p ~/go/src/github.com/edgexfoundry In a terminal window, change directories to the folder you created: cd ~/go/src/github.com/edgexfoundry Enter the following command to pull down the EdgeX Device Service SDK in Go (there is also a Device Service SDK in C): git clone https://github.com/edgexfoundry/device-sdk-go.git Create a folder for the Device Service that we are going to develop. In this step, you are naming the folder to the name you want to give your new Device Service. Standard practice in EdgeX is to prefix the name of a Device Service with device- : mkdir device-simple Copy the example code from device-sdk-go to device-simple : cp -rf ./device-sdk-go/example/* ./device-simple/ Copy Makefile to device-simple: cp ./device-sdk-go/Makefile ./device-simple Copy VERSION to device-simple: cp ./device-sdk-go/VERSION ./device-simple/ Copy version.go to device-simple: cp ./device-sdk-go/version.go ./device-simple/ Starting a new Device Service project The device-sdk-go comes with example code to create a new Device Service. Complete the following steps to modify the copy of the example code to use in your new service. Edit the main.go file in the cmd/device-simple folder. Modify the import statements to replace \"device-sdk-go/example/driver\" to \"device-simple/driver\" from the paths in the import statements. Save the file when you have finished editing. Open Makefile in your favorite text editor and make the following changes Replace MICROSERVICES : MICROSERVICES=example/cmd/device-simple/device-simple with : MICROSERVICES=cmd/device-simple/device-simple Modify GOFLAGS : GOFLAGS=-ldflags \"-X github.com/edgexfoundry/device-sdk-go.Version=$(VERSION)\" line to refer to the new service with: GOFLAGS=-ldflags \"-X github.com/edgexfoundry/device-simple.Version=$(VERSION)\" Modify build : example/cmd/device-simple/device-simple: $(GO) build $(GOFLAGS) -o $@ ./example/cmd/device-simple to: cmd/device-simple/device-simple: $(GO) build $(GOFLAGS) -o $@ ./cmd/device-simple Save the file. Enter the following command to create the initial module definition and write it to the go.mod file: GO111MODULE=on go mod init Build your Device Service To ensure that the code you have moved and updated still works, build the current Device Service. In a terminal window, change directories to the device-simple folder (the folder containing the Makefile): device-simple \u251c\u2500\u2500 cmd \u2502 \u2514\u2500\u2500 device-simple \u2502 \u251c\u2500\u2500 Dockerfile \u2502 \u251c\u2500\u2500 main.go \u2502 \u2514\u2500\u2500 res \u2502 \u251c\u2500\u2500 Simple-Driver.yaml \u2502 \u251c\u2500\u2500 configuration.toml \u2502 \u251c\u2500\u2500 docker \u2502 \u2502 \u2514\u2500\u2500 configuration.toml \u2502 \u251c\u2500\u2500 off.jpg \u2502 \u2514\u2500\u2500 on.png \u251c\u2500\u2500 driver \u2502 \u2514\u2500\u2500 simpledriver.go \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 Version \u251c\u2500\u2500 version.go \u251c\u2500\u2500 go.mod \u2514\u2500\u2500 go.sum Build the service by issuing the following command: make build If there are no errors, your service is ready for you to add customizations to generate data values as if there was a sensor attached. If there are errors, retrace your steps to correct the error and try to build again. Ask you instructor for help in finding the issue if you are unable to locate it given the error messages you receive from the build process. Customize your Device Service The Device Service you are creating isn't going to talk to a real device. Instead, it is going to generate a random number where the service would make a call to get sensor data from the actual device. By so doing, you see where the EdgeX Device Service would make a call to a local device (using its protocol and device drivers under the covers) to provide EdgeX with its sensor readings: Locate the simpledriver.go file in the /driver folder and open it with your favorite editor. In the import() area at the top of the file, add \"math/rand\" under \"time\". Locate the HandleReadCommands() function in this file. Notice the following line of code in this file: cv, _ := dsModels.NewBoolValue(reqs[0].DeviceResourceName, now, s.switchButton) Replace the two lines of code with the following: if reqs[0].DeviceResourceName == \"randomnumber\" { cv, _ := dsModels.NewInt32Value(reqs[0].DeviceResourceName, now, int32(rand.Intn(100))) The first line of code to confirmed request is for the customized resource \"randomnumber\". Also, the second line of code generates an integer (between 0 and 100) and uses that as the value the Device Service sends to EdgeX -- mimicking the collection of data from a real device. It is here that the Device Service would normally capture some sensor reading from a device and send the data to EdgeX. The line of code you just added is where you'd need to do some customization work to talk to the sensor, get the sensor's latest sensor values and send them into EdgeX. Save the simpledriver.go file Creating your Device Profile A Device Profile is a YAML file that describes a class of device to EdgeX. General characteristics about the type of device, the data these devices provide, and how to command the device is all provided in a Device Profile. Device Services use the Device Profile to understand what data is being collected from the Device (in some cases providing information used by the Device Service to know how to communicate with the device and get the desired sensor readings). A Device Profile is needed to describe the data to collect from the simple random number generating Device Service. Do the following: Explore the files in the cmd/device-simple/res folder. Take note of the example Device Profile YAML file that is already there (Simple-Driver.yml). You can explore the contents of this file to see how devices are represented by YAML. In particular, note how fields or properties of a sensor are represented by \"deviceResources\". Command to be issued to the device are represented by \"deviceCommands\". Download random-generator-device.yaml <random-generator-device.yaml> {.interpreted-text role=\"download\"} to the cmd/device-simple/res folder. Open the random-generator-device.yaml file in a text editor. In this Device Profile, you define that the device you are describing to EdgeX has a single property (or deviceResource) that EdgeX needs to know about - in this case, the property is the \"randomnumber\". Note how the deviceResource is typed. In real world IoT situations, this deviceResource list could be extensive and be filled with all different types of data. Note also how the Device Profile describes REST commands that can be used by others to call on (or \"get\") the random number from the Device Service. Configuring your Device Service Now update the configuration for your new Device Service -- changing the port it operates on (so as not to conflict with other Device Services), altering the auto event frequency of when the data is collected from the Device Service (every 10 seconds in this example), and setting up the initial provisioning of the random number generating device when the service starts. Download configuration.toml <configuration.toml> {.interpreted-text role=\"download\"} to the cmd/device-simple/res folder (this will overwrite an existing file -- that's ok). Rebuild your Device Service Just as you did before, you are ready to build the device-simple service -- creating the executable program that is your Device Service: In a terminal window, change directories to the base device-simple folder (containing the Makefile). Build the Device Service by issuing the following command: make build If there are no errors, your service has now been created and is available in the cmd/device-simple folder (look for the device-simple file). Run your Device Service Allow your newly created Device Service, which was formed out of the Device Service Go SDK, to create sensor-mimicking data that it then sends to EdgeX: As described in the ./Ch-GettingStartedUsers {.interpreted-text role=\"doc\"} guide, use Docker Compose to start all of EdgeX. From the folder containing the docker-compose file, start EdgeX with the following call: docker-compose up -d In a terminal window, change directories to the device-simple's cmd/device-simple folder. The executable device-simple is located there. Execute the Device Service with the ./device-simple command, as shown below: This starts the service and immediately displays log entries in the terminal. Using a browser, enter the following URL to see the Event/Reading data that the service is generating and sending to EdgeX: http://localhost:48080/api/v1/event/device/RandNum-Device-01/100 This request asks for the last 100 Events/Readings from Core Data associated to the RandNum-Device-01. Note : If you are running the other EdgeX services somewhere other than localhost, use that hostname in the above URL.","title":"Golang SDK"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#golang-sdk","text":"The EdgeX Device Service SDK helps developers quickly create new device connectors for EdgeX by providing the common framework that each Device Service needs. The framework provides a pattern for provisioning devices. It provides common template code to receive and react to command (a.k.a. actuation) requests. Finally, the framework provides the common code to help get the data coming from the sensor into EdgeX Core Data (often referred to as data ingestion). With the SDK, developers are left to focus on the code that is specific to the communications with the device via the protocol of the device. In this guide, you create a simple Device Service that generates a random number in place of getting data from an actual sensor. In this way, you get to explore some of the framework and work necessary to complete a Device Service without actually having a device to talk to.","title":"Golang SDK"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#install-dependencies","text":"Creating a Device Service requires a little programming in Go. Go Lang (version 1.11 or better) must be installed on your system to complete this lab. Follow the instructions in the link below to install Go if it is not already installed on your platform: https://golang.org/doc/install You need a Git tool to pull the Device Service Go SDK code from the EdgeX Foundry Git repository. Follow the instructions in the link below to install Git for your platform: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git You will also need a \"make\" program. On Ubuntu Linux environments, this can be accomplished with the following command: sudo apt install build-essential Finally, you need a simple text editor (or Go Lang IDE).","title":"Install dependencies"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#get-the-edgex-device-sdk-for-go","text":"Complete the following steps to create a folder on your file system, download the Device SDK , then you pull the SDK to your system, and finally create the new EdgeX Device Service from the SDK templating code. Create a collection of nested folders, ~/go/src/github.com/edgexfoundry on your file system. This folder will eventually hold your new Device Service. In Linux, this can be done with a single mkdir (with -p switch) command: mkdir -p ~/go/src/github.com/edgexfoundry In a terminal window, change directories to the folder you created: cd ~/go/src/github.com/edgexfoundry Enter the following command to pull down the EdgeX Device Service SDK in Go (there is also a Device Service SDK in C): git clone https://github.com/edgexfoundry/device-sdk-go.git Create a folder for the Device Service that we are going to develop. In this step, you are naming the folder to the name you want to give your new Device Service. Standard practice in EdgeX is to prefix the name of a Device Service with device- : mkdir device-simple Copy the example code from device-sdk-go to device-simple : cp -rf ./device-sdk-go/example/* ./device-simple/ Copy Makefile to device-simple: cp ./device-sdk-go/Makefile ./device-simple Copy VERSION to device-simple: cp ./device-sdk-go/VERSION ./device-simple/ Copy version.go to device-simple: cp ./device-sdk-go/version.go ./device-simple/","title":"Get the EdgeX Device SDK for Go"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#starting-a-new-device-service-project","text":"The device-sdk-go comes with example code to create a new Device Service. Complete the following steps to modify the copy of the example code to use in your new service. Edit the main.go file in the cmd/device-simple folder. Modify the import statements to replace \"device-sdk-go/example/driver\" to \"device-simple/driver\" from the paths in the import statements. Save the file when you have finished editing. Open Makefile in your favorite text editor and make the following changes Replace MICROSERVICES : MICROSERVICES=example/cmd/device-simple/device-simple with : MICROSERVICES=cmd/device-simple/device-simple Modify GOFLAGS : GOFLAGS=-ldflags \"-X github.com/edgexfoundry/device-sdk-go.Version=$(VERSION)\" line to refer to the new service with: GOFLAGS=-ldflags \"-X github.com/edgexfoundry/device-simple.Version=$(VERSION)\" Modify build : example/cmd/device-simple/device-simple: $(GO) build $(GOFLAGS) -o $@ ./example/cmd/device-simple to: cmd/device-simple/device-simple: $(GO) build $(GOFLAGS) -o $@ ./cmd/device-simple Save the file. Enter the following command to create the initial module definition and write it to the go.mod file: GO111MODULE=on go mod init","title":"Starting a new Device Service project"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#build-your-device-service","text":"To ensure that the code you have moved and updated still works, build the current Device Service. In a terminal window, change directories to the device-simple folder (the folder containing the Makefile): device-simple \u251c\u2500\u2500 cmd \u2502 \u2514\u2500\u2500 device-simple \u2502 \u251c\u2500\u2500 Dockerfile \u2502 \u251c\u2500\u2500 main.go \u2502 \u2514\u2500\u2500 res \u2502 \u251c\u2500\u2500 Simple-Driver.yaml \u2502 \u251c\u2500\u2500 configuration.toml \u2502 \u251c\u2500\u2500 docker \u2502 \u2502 \u2514\u2500\u2500 configuration.toml \u2502 \u251c\u2500\u2500 off.jpg \u2502 \u2514\u2500\u2500 on.png \u251c\u2500\u2500 driver \u2502 \u2514\u2500\u2500 simpledriver.go \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 Version \u251c\u2500\u2500 version.go \u251c\u2500\u2500 go.mod \u2514\u2500\u2500 go.sum Build the service by issuing the following command: make build If there are no errors, your service is ready for you to add customizations to generate data values as if there was a sensor attached. If there are errors, retrace your steps to correct the error and try to build again. Ask you instructor for help in finding the issue if you are unable to locate it given the error messages you receive from the build process.","title":"Build your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#customize-your-device-service","text":"The Device Service you are creating isn't going to talk to a real device. Instead, it is going to generate a random number where the service would make a call to get sensor data from the actual device. By so doing, you see where the EdgeX Device Service would make a call to a local device (using its protocol and device drivers under the covers) to provide EdgeX with its sensor readings: Locate the simpledriver.go file in the /driver folder and open it with your favorite editor. In the import() area at the top of the file, add \"math/rand\" under \"time\". Locate the HandleReadCommands() function in this file. Notice the following line of code in this file: cv, _ := dsModels.NewBoolValue(reqs[0].DeviceResourceName, now, s.switchButton) Replace the two lines of code with the following: if reqs[0].DeviceResourceName == \"randomnumber\" { cv, _ := dsModels.NewInt32Value(reqs[0].DeviceResourceName, now, int32(rand.Intn(100))) The first line of code to confirmed request is for the customized resource \"randomnumber\". Also, the second line of code generates an integer (between 0 and 100) and uses that as the value the Device Service sends to EdgeX -- mimicking the collection of data from a real device. It is here that the Device Service would normally capture some sensor reading from a device and send the data to EdgeX. The line of code you just added is where you'd need to do some customization work to talk to the sensor, get the sensor's latest sensor values and send them into EdgeX. Save the simpledriver.go file","title":"Customize your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#creating-your-device-profile","text":"A Device Profile is a YAML file that describes a class of device to EdgeX. General characteristics about the type of device, the data these devices provide, and how to command the device is all provided in a Device Profile. Device Services use the Device Profile to understand what data is being collected from the Device (in some cases providing information used by the Device Service to know how to communicate with the device and get the desired sensor readings). A Device Profile is needed to describe the data to collect from the simple random number generating Device Service. Do the following: Explore the files in the cmd/device-simple/res folder. Take note of the example Device Profile YAML file that is already there (Simple-Driver.yml). You can explore the contents of this file to see how devices are represented by YAML. In particular, note how fields or properties of a sensor are represented by \"deviceResources\". Command to be issued to the device are represented by \"deviceCommands\". Download random-generator-device.yaml <random-generator-device.yaml> {.interpreted-text role=\"download\"} to the cmd/device-simple/res folder. Open the random-generator-device.yaml file in a text editor. In this Device Profile, you define that the device you are describing to EdgeX has a single property (or deviceResource) that EdgeX needs to know about - in this case, the property is the \"randomnumber\". Note how the deviceResource is typed. In real world IoT situations, this deviceResource list could be extensive and be filled with all different types of data. Note also how the Device Profile describes REST commands that can be used by others to call on (or \"get\") the random number from the Device Service.","title":"Creating your Device Profile"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#configuring-your-device-service","text":"Now update the configuration for your new Device Service -- changing the port it operates on (so as not to conflict with other Device Services), altering the auto event frequency of when the data is collected from the Device Service (every 10 seconds in this example), and setting up the initial provisioning of the random number generating device when the service starts. Download configuration.toml <configuration.toml> {.interpreted-text role=\"download\"} to the cmd/device-simple/res folder (this will overwrite an existing file -- that's ok).","title":"Configuring your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#rebuild-your-device-service","text":"Just as you did before, you are ready to build the device-simple service -- creating the executable program that is your Device Service: In a terminal window, change directories to the base device-simple folder (containing the Makefile). Build the Device Service by issuing the following command: make build If there are no errors, your service has now been created and is available in the cmd/device-simple folder (look for the device-simple file).","title":"Rebuild your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK-Go/#run-your-device-service","text":"Allow your newly created Device Service, which was formed out of the Device Service Go SDK, to create sensor-mimicking data that it then sends to EdgeX: As described in the ./Ch-GettingStartedUsers {.interpreted-text role=\"doc\"} guide, use Docker Compose to start all of EdgeX. From the folder containing the docker-compose file, start EdgeX with the following call: docker-compose up -d In a terminal window, change directories to the device-simple's cmd/device-simple folder. The executable device-simple is located there. Execute the Device Service with the ./device-simple command, as shown below: This starts the service and immediately displays log entries in the terminal. Using a browser, enter the following URL to see the Event/Reading data that the service is generating and sending to EdgeX: http://localhost:48080/api/v1/event/device/RandNum-Device-01/100 This request asks for the last 100 Events/Readings from Core Data associated to the RandNum-Device-01. Note : If you are running the other EdgeX services somewhere other than localhost, use that hostname in the above URL.","title":"Run your Device Service"},{"location":"getting-started/Ch-GettingStartedSDK/","text":"Device Service SDK The EdgeX Device Service SDK helps developers quickly create new device connectors for EdgeX because it provides the common scaffolding that each Device Service needs to have. The scaffolding provides a pattern for provisioning devices. It provides common template code to receive and react to command (a.k.a. actuation) requests. Finally, the scaffolding provides the common code to help get the data coming from the sensor into EdgeX Core Data (often referred to as data ingestion). With the SDK, developers are left to focus on the code that is specific to the communications with the device via the protocol of the device. In these guides, you will create a simple device service that generates a random number in place of getting data from an actual sensor. In this way, you get to explore some of the scaffolding and work necessary to complete a device service without actually having a device to talk to.","title":"Device Service SDK"},{"location":"getting-started/Ch-GettingStartedSDK/#device-service-sdk","text":"The EdgeX Device Service SDK helps developers quickly create new device connectors for EdgeX because it provides the common scaffolding that each Device Service needs to have. The scaffolding provides a pattern for provisioning devices. It provides common template code to receive and react to command (a.k.a. actuation) requests. Finally, the scaffolding provides the common code to help get the data coming from the sensor into EdgeX Core Data (often referred to as data ingestion). With the SDK, developers are left to focus on the code that is specific to the communications with the device via the protocol of the device. In these guides, you will create a simple device service that generates a random number in place of getting data from an actual sensor. In this way, you get to explore some of the scaffolding and work necessary to complete a device service without actually having a device to talk to.","title":"Device Service SDK"},{"location":"getting-started/Ch-GettingStartedSnapUsers/","text":"Getting Started with Snap Introduction Just like Docker containers, the EdgeX project creates and publishes a snap for each release to the snap store . The snap currently supports running on both amd64 and arm64 platforms. See snap documents for help on using or installing snap. Installing EdgeX Foundry as a snap The snap is published in the snap store at https://snapcraft.io/edgexfoundry. You can see the current revisions available for your machine's architecture by running the command: $ snap info edgexfoundry The snap can be installed using snap install. To install the snap from the edge channel: $ sudo snap install edgexfoundry --edge You can install a specific release using the --channel option. For example to install the Fuji release of the snap: $ sudo snap install edgexfoundry --channel=fuji Lastly, on a system supporting it, the snap may be installed using GNOME (or Ubuntu) Software Center by searching for edgexfoundry. Note The snap has only been tested on Ubuntu Desktop/Server versions 18.04 and 16.04, as well as Ubuntu Core versions 16 and 18. Warning Running the EdgeX snap on a machine setup for EdgeX development can create conflicts and result in the platform errors/issues. Using the EdgeX snap Upon installation, the following EdgeX services are automatically and immediately started: - consul - redis - core-data - core-command - core-metadata - security-services (see note below) The following services are disabled by default: - app-service-configurable (required for Kuiper and support-rulesengine) - device-virtual - kuiper - support-logging - support-notifications - support-rulesengine (deprecated) - support-scheduler - sys-mgmt-agent Any disabled services can be enabled and started up using snap set: $ sudo snap set edgexfoundry support-notifications=on To turn a service off (thereby disabling and immediately stopping it) set the service to off: $ sudo snap set edgexfoundry support-notifications=off All services which are installed on the system as systemd units, which if enabled will automatically start running when the system boots or reboots. Configuring individual services All default configuration files are shipped with the snap inside $SNAP/config, however because $SNAP isn't writable, all of the config files are copied during snap installation (specifically during the install hook, see snap/hooks/install in this repository) to $SNAP_DATA/config. $ sudo snap restart edgexfoundry Viewing logs Currently, all log files for the snap's can be found inside $SNAP_COMMON, which is usually /var/snap/edgexfoundry/common. Once all the services are supported as daemons, you can also use sudo snap logs edgexfoundry to view logs. Additionally, logs can be viewed using the system journal or snap logs. To view the logs for all services in the edgexfoundry snap use: $ sudo snap logs edgexfoundry Individual service logs may be viewed by specifying the service name: $ sudo snap logs edgexfoundry.consul Or by using the systemd unit name and journalctl: $ journalctl -u snap.edgexfoundry.consul Security services Currently, the security services are enabled by default. The security services consitute the following components: - Kong - PostgreSQL - Vault - security-secrets-setup - security-secretstore-setup - security-proxy-setup Vault is used for secret management, and Kong is used as an HTTPS proxy for all the services. Kong can be disabled by using the following command: $ sudo snap set edgexfoundry security-proxy=off Vault can be also be disabled, but doing so will also disable Kong, as it depends on Vault. Thus the following command will disable both: Note Kong is currently not supported in the snap when installed on an arm64-based device, so it will be disabled on install.","title":"Getting Started with Snap"},{"location":"getting-started/Ch-GettingStartedSnapUsers/#getting-started-with-snap","text":"","title":"Getting Started with Snap"},{"location":"getting-started/Ch-GettingStartedSnapUsers/#introduction","text":"Just like Docker containers, the EdgeX project creates and publishes a snap for each release to the snap store . The snap currently supports running on both amd64 and arm64 platforms. See snap documents for help on using or installing snap.","title":"Introduction"},{"location":"getting-started/Ch-GettingStartedSnapUsers/#installing-edgex-foundry-as-a-snap","text":"The snap is published in the snap store at https://snapcraft.io/edgexfoundry. You can see the current revisions available for your machine's architecture by running the command: $ snap info edgexfoundry The snap can be installed using snap install. To install the snap from the edge channel: $ sudo snap install edgexfoundry --edge You can install a specific release using the --channel option. For example to install the Fuji release of the snap: $ sudo snap install edgexfoundry --channel=fuji Lastly, on a system supporting it, the snap may be installed using GNOME (or Ubuntu) Software Center by searching for edgexfoundry. Note The snap has only been tested on Ubuntu Desktop/Server versions 18.04 and 16.04, as well as Ubuntu Core versions 16 and 18. Warning Running the EdgeX snap on a machine setup for EdgeX development can create conflicts and result in the platform errors/issues.","title":"Installing EdgeX Foundry as a snap"},{"location":"getting-started/Ch-GettingStartedSnapUsers/#using-the-edgex-snap","text":"Upon installation, the following EdgeX services are automatically and immediately started: - consul - redis - core-data - core-command - core-metadata - security-services (see note below) The following services are disabled by default: - app-service-configurable (required for Kuiper and support-rulesengine) - device-virtual - kuiper - support-logging - support-notifications - support-rulesengine (deprecated) - support-scheduler - sys-mgmt-agent Any disabled services can be enabled and started up using snap set: $ sudo snap set edgexfoundry support-notifications=on To turn a service off (thereby disabling and immediately stopping it) set the service to off: $ sudo snap set edgexfoundry support-notifications=off All services which are installed on the system as systemd units, which if enabled will automatically start running when the system boots or reboots.","title":"Using the EdgeX snap"},{"location":"getting-started/Ch-GettingStartedSnapUsers/#configuring-individual-services","text":"All default configuration files are shipped with the snap inside $SNAP/config, however because $SNAP isn't writable, all of the config files are copied during snap installation (specifically during the install hook, see snap/hooks/install in this repository) to $SNAP_DATA/config. $ sudo snap restart edgexfoundry","title":"Configuring individual services"},{"location":"getting-started/Ch-GettingStartedSnapUsers/#viewing-logs","text":"Currently, all log files for the snap's can be found inside $SNAP_COMMON, which is usually /var/snap/edgexfoundry/common. Once all the services are supported as daemons, you can also use sudo snap logs edgexfoundry to view logs. Additionally, logs can be viewed using the system journal or snap logs. To view the logs for all services in the edgexfoundry snap use: $ sudo snap logs edgexfoundry Individual service logs may be viewed by specifying the service name: $ sudo snap logs edgexfoundry.consul Or by using the systemd unit name and journalctl: $ journalctl -u snap.edgexfoundry.consul","title":"Viewing logs"},{"location":"getting-started/Ch-GettingStartedSnapUsers/#security-services","text":"Currently, the security services are enabled by default. The security services consitute the following components: - Kong - PostgreSQL - Vault - security-secrets-setup - security-secretstore-setup - security-proxy-setup Vault is used for secret management, and Kong is used as an HTTPS proxy for all the services. Kong can be disabled by using the following command: $ sudo snap set edgexfoundry security-proxy=off Vault can be also be disabled, but doing so will also disable Kong, as it depends on Vault. Thus the following command will disable both: Note Kong is currently not supported in the snap when installed on an arm64-based device, so it will be disabled on install.","title":"Security services"},{"location":"getting-started/Ch-GettingStartedUsers/","text":"Getting Started with Docker Introduction These instructions are for Users to get and run EdgeX Foundry. (Developers should read: Getting Started Developers ) EdgeX is a collection of more than a dozen micro services that are deployed to provide a minimal edge platform capability. You can download EdgeX micro service source code and build your own micro services. However, if you do not have a need to change or add to EdgeX, then you do not need to download source code. Instead, Users run EdgeX micro service Docker containers. The EdgeX community builds and creates Docker container images with each release. Get & Run EdgeX Foundry Install Docker & Docker Compose To run Dockerized EdgeX, you need to install Docker. See https://docs.docker.com/install/ to learn how to install Docker. If you are new to Docker, the same web site provides you educational information. The following short video is also very informative https://www.youtube.com/watch?time_continue=3&v=VhabrYF1nms Use Docker Compose to orchestrate the fetch (or pull), install, and start the EdgeX micro service containers. Also use Docker Compose to stop the micro service containers. See: https://docs.docker.com/compose/ to learn more about Docker Compose. You do not need to be an expert with Docker (or Docker Compose) to get and run EdgeX. This guide provides the steps to get EdgeX running in your environment. Some knowledge of Docker and Docker Compose are nice to have, but not required. Basic Docker and Docker Compose commands provided here enable you to run, update, and diagnose issues within EdgeX. Select a EdgeX Foundry Compose File After installing Docker and Docker Compose, you need a EdgeX Docker Compose file. EdgeX Foundry has over a dozen micro services, each deployed in its own Docker container. This file is a manifest of all the EdgeX Foundry micro services to run. The Docker Compose file provides details about how to run each of the services. Specifically, a Docker Compose file is a manifest file, which lists: The Docker container images that should be downloaded, The order in which the containers should be started, The parameters (such as ports) under which the containers should be run The EdgeX development team provides Docker Compose files for each release. Visit the project GitHub and locate the EdgeX Docker Compose file for the version of EdgeX you want to run. The EdgeX Developer Scripts repository contains a folder for each release. In the folder, find the Docker Compose files for each release. Note At the GitHub location specified above there is a folder for each EdgeX release. The nightly-build folder contains Docker Compose files that use artifacts created from the latest code submitted by contributors. Most end users should avoid using these Docker Compose files. They are work-in-progress. Users should use the Docker Compose files for the latest version of EdgeX. In each folder, you will find several Docker Compose files (all with a .yml extension). The name of the file will suggest the type of EdgeX instance the Compose file will help setup. The table below provides a list of the Docker Compose filenames for the latest release (Geneva). Find the Docker Compose file that matches: your hardware (x86 or ARM) the database you want to use (Mongo or Redis) your desire to have security services on or off filename Docker Compose contents docker-compose-geneva-mongo-arm64.yml Specifies ARM 64 containers, uses Mongo database for persistence, and includes security services docker-compose-geneva-mongo-no-secty-arm64.yml Specifies x86 containers, uses Mongo database for persistence, but does not include security services docker-compose-geneva-mongo-no-secty.yml Specifies x86 containers, uses Mongo database for persistence, but does not include security services docker-compose-geneva-mongo.yml Specifies x86 containers, uses Mongo database for persistence, and includes security services docker-compose-geneva-redis-arm64.yml Specifies x86 containers, uses Redis database for persistence, and includes security services docker-compose-geneva-redis-no-secty-arm64.yml Specifies ARM 64 containers, uses Redis database for persistence, but does not include security services docker-compose-geneva-redis-no-secty.yml Specifies x86 containers, uses Redis database for persistence, but does not include security services docker-compose-geneva-redis.yml Specifies x86 containers, uses Redis database for persistence, and includes security services docker-compose-geneva-ui-arm64. Specifies the EdgeX user interface extension to be used with the ARM 64 EdgeX platform docker-compose-geneva-ui.yml Specifies the EdgeX user interface extension to be used with the x86 EdgeX platform docker-compose-portainer.yml Specifies the Portainer user interface extension (to be used with the x86 or ARM EdgeX platform) Info Unsure which Docker Compose file to use? The EdgeX community recommends you use the Reds, no security Docker Compose file for your architecture to start. As you learn about EdgeX, you can incorporate security elements. The Mongo database is being archived with the next release. Download a EdgeX Foundry Compose File Once you have selected the EdgeX Compose file you want to use, download it using your favorite tool. The examples below uses wget to fetch Docker Compose for the Geneva release, no security, Redis database. x86 wget https://raw.githubusercontent.com/edgexfoundry/developer-scripts/master/releases/geneva/compose-files/docker-compose-geneva-redis-no-secty.yml -O docker-compose.yml ARM wget https://raw.githubusercontent.com/edgexfoundry/developer-scripts/master/releases/geneva/compose-files/docker-compose-geneva-mongo-no-secty-arm64.yml -O docker-compose.yml Note The commands above fetch the Docker Compose to a file named 'docker-compose.yml' in the current directory. Docker Compose commands look for a file named 'docker-compose.yml' by default. You can use an alternate file name but then must specify that file name when issuing Docker Compose commands. See Compose reference documentation for help. Run EdgeX Foundry Now that you have the EdgeX Docker Compose file, you are ready to run EdgeX. Follow these steps to get the container images and start EdgeX! In a command terminal, change directories to the location of your docker-compose.yml. Run the following command in the terminal to pull (fetch) and then start the EdgeX containers. docker-compose up -d Info If you wish, you can fetch the images first and then run them. This allows you to make sure the EdgeX images you need are all available before trying to run. docker-compose pull docker-compose up -d Note The -d option indicates you want the Docker Compose to run the EdgeX containers in detached mode - that is to run the containers in the background. Without -d, the containers will all start in the terminal and to use the terminal further you have to stop the containers. Verify EdgeX Foundry Running In the same terminal, run the process status command shown below to confirm that all the containers downloaded and started. docker-compose ps If all EdgeX containers pulled and started correctly and without error, you should see a process status (ps) that looks similar to the image above. Checking the Status of EdgeX Foundry In addition to the process status of the EdgeX containers, there are a number of other tools to check on the healt and status of your EdgeX instance. EdgeX Foundry Container Logs Use the command below to see log of any service. # see the logs of a service docker-compose logs -f [ compose-service-name ] # example - core data docker-compose logs -f data See EdgeX Container Names for a list of the EdgeX Docker Compose service names. A check of an EdgeX service log usually indicates if the service is running normally or has errors. When you are done reviewing the content of the log, select Control-c to stop the output to your terminal. Ping Check Each EdgeX micro service has a built-in respond to a \"ping\" HTTP request. In networking environments, use a ping request to check the reach-ability of a network resource. EdgeX uses the same concept to check the availability or reach-ability of a micro service. After the EdgeX micro service containers are running, you can \"ping\" any one of the micro services to check that it is running. Open a browser or HTTP REST client tool and use the service's ping address (outlined below) to check that is available. http://localhost:[port]/api/v1/ping See EdgeX Device Service Ports for a list of the EdgeX default service ports. \"Pinging\" an EdgeX micro service allows you to check on its availability. If the service does not respond to ping, the service is down or having issues. Consul Registry Check EdgeX uses the open source Consul project as its registry service. All EdgeX micro services are expected to register with Consul as they start. Going to Consul's dashboard UI enables you to see which services are up. Find the Consul UI at http://localhost:8500/ui .","title":"Getting Started with Docker"},{"location":"getting-started/Ch-GettingStartedUsers/#getting-started-with-docker","text":"","title":"Getting Started with Docker"},{"location":"getting-started/Ch-GettingStartedUsers/#introduction","text":"These instructions are for Users to get and run EdgeX Foundry. (Developers should read: Getting Started Developers ) EdgeX is a collection of more than a dozen micro services that are deployed to provide a minimal edge platform capability. You can download EdgeX micro service source code and build your own micro services. However, if you do not have a need to change or add to EdgeX, then you do not need to download source code. Instead, Users run EdgeX micro service Docker containers. The EdgeX community builds and creates Docker container images with each release.","title":"Introduction"},{"location":"getting-started/Ch-GettingStartedUsers/#get-run-edgex-foundry","text":"","title":"Get &amp; Run EdgeX Foundry"},{"location":"getting-started/Ch-GettingStartedUsers/#install-docker-docker-compose","text":"To run Dockerized EdgeX, you need to install Docker. See https://docs.docker.com/install/ to learn how to install Docker. If you are new to Docker, the same web site provides you educational information. The following short video is also very informative https://www.youtube.com/watch?time_continue=3&v=VhabrYF1nms Use Docker Compose to orchestrate the fetch (or pull), install, and start the EdgeX micro service containers. Also use Docker Compose to stop the micro service containers. See: https://docs.docker.com/compose/ to learn more about Docker Compose. You do not need to be an expert with Docker (or Docker Compose) to get and run EdgeX. This guide provides the steps to get EdgeX running in your environment. Some knowledge of Docker and Docker Compose are nice to have, but not required. Basic Docker and Docker Compose commands provided here enable you to run, update, and diagnose issues within EdgeX.","title":"Install Docker &amp; Docker Compose"},{"location":"getting-started/Ch-GettingStartedUsers/#select-a-edgex-foundry-compose-file","text":"After installing Docker and Docker Compose, you need a EdgeX Docker Compose file. EdgeX Foundry has over a dozen micro services, each deployed in its own Docker container. This file is a manifest of all the EdgeX Foundry micro services to run. The Docker Compose file provides details about how to run each of the services. Specifically, a Docker Compose file is a manifest file, which lists: The Docker container images that should be downloaded, The order in which the containers should be started, The parameters (such as ports) under which the containers should be run The EdgeX development team provides Docker Compose files for each release. Visit the project GitHub and locate the EdgeX Docker Compose file for the version of EdgeX you want to run. The EdgeX Developer Scripts repository contains a folder for each release. In the folder, find the Docker Compose files for each release. Note At the GitHub location specified above there is a folder for each EdgeX release. The nightly-build folder contains Docker Compose files that use artifacts created from the latest code submitted by contributors. Most end users should avoid using these Docker Compose files. They are work-in-progress. Users should use the Docker Compose files for the latest version of EdgeX. In each folder, you will find several Docker Compose files (all with a .yml extension). The name of the file will suggest the type of EdgeX instance the Compose file will help setup. The table below provides a list of the Docker Compose filenames for the latest release (Geneva). Find the Docker Compose file that matches: your hardware (x86 or ARM) the database you want to use (Mongo or Redis) your desire to have security services on or off filename Docker Compose contents docker-compose-geneva-mongo-arm64.yml Specifies ARM 64 containers, uses Mongo database for persistence, and includes security services docker-compose-geneva-mongo-no-secty-arm64.yml Specifies x86 containers, uses Mongo database for persistence, but does not include security services docker-compose-geneva-mongo-no-secty.yml Specifies x86 containers, uses Mongo database for persistence, but does not include security services docker-compose-geneva-mongo.yml Specifies x86 containers, uses Mongo database for persistence, and includes security services docker-compose-geneva-redis-arm64.yml Specifies x86 containers, uses Redis database for persistence, and includes security services docker-compose-geneva-redis-no-secty-arm64.yml Specifies ARM 64 containers, uses Redis database for persistence, but does not include security services docker-compose-geneva-redis-no-secty.yml Specifies x86 containers, uses Redis database for persistence, but does not include security services docker-compose-geneva-redis.yml Specifies x86 containers, uses Redis database for persistence, and includes security services docker-compose-geneva-ui-arm64. Specifies the EdgeX user interface extension to be used with the ARM 64 EdgeX platform docker-compose-geneva-ui.yml Specifies the EdgeX user interface extension to be used with the x86 EdgeX platform docker-compose-portainer.yml Specifies the Portainer user interface extension (to be used with the x86 or ARM EdgeX platform) Info Unsure which Docker Compose file to use? The EdgeX community recommends you use the Reds, no security Docker Compose file for your architecture to start. As you learn about EdgeX, you can incorporate security elements. The Mongo database is being archived with the next release.","title":"Select a EdgeX Foundry Compose File"},{"location":"getting-started/Ch-GettingStartedUsers/#download-a-edgex-foundry-compose-file","text":"Once you have selected the EdgeX Compose file you want to use, download it using your favorite tool. The examples below uses wget to fetch Docker Compose for the Geneva release, no security, Redis database. x86 wget https://raw.githubusercontent.com/edgexfoundry/developer-scripts/master/releases/geneva/compose-files/docker-compose-geneva-redis-no-secty.yml -O docker-compose.yml ARM wget https://raw.githubusercontent.com/edgexfoundry/developer-scripts/master/releases/geneva/compose-files/docker-compose-geneva-mongo-no-secty-arm64.yml -O docker-compose.yml Note The commands above fetch the Docker Compose to a file named 'docker-compose.yml' in the current directory. Docker Compose commands look for a file named 'docker-compose.yml' by default. You can use an alternate file name but then must specify that file name when issuing Docker Compose commands. See Compose reference documentation for help.","title":"Download a EdgeX Foundry Compose File"},{"location":"getting-started/Ch-GettingStartedUsers/#run-edgex-foundry","text":"Now that you have the EdgeX Docker Compose file, you are ready to run EdgeX. Follow these steps to get the container images and start EdgeX! In a command terminal, change directories to the location of your docker-compose.yml. Run the following command in the terminal to pull (fetch) and then start the EdgeX containers. docker-compose up -d Info If you wish, you can fetch the images first and then run them. This allows you to make sure the EdgeX images you need are all available before trying to run. docker-compose pull docker-compose up -d Note The -d option indicates you want the Docker Compose to run the EdgeX containers in detached mode - that is to run the containers in the background. Without -d, the containers will all start in the terminal and to use the terminal further you have to stop the containers.","title":"Run EdgeX Foundry"},{"location":"getting-started/Ch-GettingStartedUsers/#verify-edgex-foundry-running","text":"In the same terminal, run the process status command shown below to confirm that all the containers downloaded and started. docker-compose ps If all EdgeX containers pulled and started correctly and without error, you should see a process status (ps) that looks similar to the image above.","title":"Verify EdgeX Foundry Running"},{"location":"getting-started/Ch-GettingStartedUsers/#checking-the-status-of-edgex-foundry","text":"In addition to the process status of the EdgeX containers, there are a number of other tools to check on the healt and status of your EdgeX instance.","title":"Checking the Status of EdgeX Foundry"},{"location":"getting-started/Ch-GettingStartedUsers/#edgex-foundry-container-logs","text":"Use the command below to see log of any service. # see the logs of a service docker-compose logs -f [ compose-service-name ] # example - core data docker-compose logs -f data See EdgeX Container Names for a list of the EdgeX Docker Compose service names. A check of an EdgeX service log usually indicates if the service is running normally or has errors. When you are done reviewing the content of the log, select Control-c to stop the output to your terminal.","title":"EdgeX Foundry Container Logs"},{"location":"getting-started/Ch-GettingStartedUsers/#ping-check","text":"Each EdgeX micro service has a built-in respond to a \"ping\" HTTP request. In networking environments, use a ping request to check the reach-ability of a network resource. EdgeX uses the same concept to check the availability or reach-ability of a micro service. After the EdgeX micro service containers are running, you can \"ping\" any one of the micro services to check that it is running. Open a browser or HTTP REST client tool and use the service's ping address (outlined below) to check that is available. http://localhost:[port]/api/v1/ping See EdgeX Device Service Ports for a list of the EdgeX default service ports. \"Pinging\" an EdgeX micro service allows you to check on its availability. If the service does not respond to ping, the service is down or having issues.","title":"Ping Check"},{"location":"getting-started/Ch-GettingStartedUsers/#consul-registry-check","text":"EdgeX uses the open source Consul project as its registry service. All EdgeX micro services are expected to register with Consul as they start. Going to Consul's dashboard UI enables you to see which services are up. Find the Consul UI at http://localhost:8500/ui .","title":"Consul Registry Check"},{"location":"getting-started/Ch-GettingStartedUsersNexus/","text":"Getting Docker Images from EdgeX Nexus Repository Released EdgeX Docker container images are available from Docker Hub . In some cases, it may be necessary to get your EdgeX container images from the Nexus repository. The Linux Foundation manages the Nexus repository for the project. Nexus contains the EdgeX project staging and development container images. In other words, Nexus contains work-in-progress or pre-release images. These, pre-release/work-in-progress Docker images are built nightly and made available at the following Nexus location: nexus3.edgexfoundry.org:10004 Rationale To Use Nexus Images Reasons you might want to use container images from Nexus include: The container is not available from Docker Hub (or Docker Hub is down temporarily) You need the latest development container image (the work in progress) You are working in a Windows or non-Linux environment and you are unable to build a container without some issues. A set of Docker Compose files have been created to allow you to get and use the latest EdgeX service images from Nexus. Find these Nexus \"Nightly Build\" Compose files in GitHub. The EdgeX development team provides these Docker Compose files. As with the EdgeX release Compose files, you will find several different Docker Compose files that allow you to get the type of EdgeX instance setup based on: your hardware (x86 or ARM) the database you want to use (Mongo or Redis) your desire to have security services on or off Warning The \"Nightly Build\" images are provided as is and may not always function properly or with other EdgeX services. Use with caution and typically only if you are a developer/contributor to EdgeX. These images represent the latest development work and may not have been thoroughly tested or integrated. Using Nexus Images The operations to pull the images and run the Nexus Repository is the same as when using EdgeX images from Docker Hub (see Getting Started with Docker ). To get containers from the Nexus Repository, in a command terminal, change directories to the location of your downloaded Nexus Docker Compose yaml. Rename the file to docker-compose.yml. Then run the following command in the terminal to pull (fetch) and then start the EdgeX Nexus-image containers. docker-compose up -d Using a Single Nexus Image In some cases, you may only need to use a single image from Nexus while other EdgeX services are created from the Docker Hub images. In this case, you can simply replace the image location for the selected image in your original Docker Compose file. The address of Nexus is nexus3.edgexfoundry.org at port 10004 . So, if you wished to use the EdgeX core data image from Nexus, you would replace the name and location of the core data image \"edgexfoundry/docker-core-data-go:1.2.1\" with \"nexus3.edgexfoundry.org:10004/docker-core-data-go:master\" in the Compose file. Note The example above replaces the Geneva core data service from Docker Hub with the latest core data image in Nexus.","title":"Getting Docker Images from EdgeX Nexus Repository"},{"location":"getting-started/Ch-GettingStartedUsersNexus/#getting-docker-images-from-edgex-nexus-repository","text":"Released EdgeX Docker container images are available from Docker Hub . In some cases, it may be necessary to get your EdgeX container images from the Nexus repository. The Linux Foundation manages the Nexus repository for the project. Nexus contains the EdgeX project staging and development container images. In other words, Nexus contains work-in-progress or pre-release images. These, pre-release/work-in-progress Docker images are built nightly and made available at the following Nexus location: nexus3.edgexfoundry.org:10004","title":"Getting Docker Images from EdgeX Nexus Repository"},{"location":"getting-started/Ch-GettingStartedUsersNexus/#rationale-to-use-nexus-images","text":"Reasons you might want to use container images from Nexus include: The container is not available from Docker Hub (or Docker Hub is down temporarily) You need the latest development container image (the work in progress) You are working in a Windows or non-Linux environment and you are unable to build a container without some issues. A set of Docker Compose files have been created to allow you to get and use the latest EdgeX service images from Nexus. Find these Nexus \"Nightly Build\" Compose files in GitHub. The EdgeX development team provides these Docker Compose files. As with the EdgeX release Compose files, you will find several different Docker Compose files that allow you to get the type of EdgeX instance setup based on: your hardware (x86 or ARM) the database you want to use (Mongo or Redis) your desire to have security services on or off Warning The \"Nightly Build\" images are provided as is and may not always function properly or with other EdgeX services. Use with caution and typically only if you are a developer/contributor to EdgeX. These images represent the latest development work and may not have been thoroughly tested or integrated.","title":"Rationale To Use Nexus Images"},{"location":"getting-started/Ch-GettingStartedUsersNexus/#using-nexus-images","text":"The operations to pull the images and run the Nexus Repository is the same as when using EdgeX images from Docker Hub (see Getting Started with Docker ). To get containers from the Nexus Repository, in a command terminal, change directories to the location of your downloaded Nexus Docker Compose yaml. Rename the file to docker-compose.yml. Then run the following command in the terminal to pull (fetch) and then start the EdgeX Nexus-image containers. docker-compose up -d","title":"Using Nexus Images"},{"location":"getting-started/Ch-GettingStartedUsersNexus/#using-a-single-nexus-image","text":"In some cases, you may only need to use a single image from Nexus while other EdgeX services are created from the Docker Hub images. In this case, you can simply replace the image location for the selected image in your original Docker Compose file. The address of Nexus is nexus3.edgexfoundry.org at port 10004 . So, if you wished to use the EdgeX core data image from Nexus, you would replace the name and location of the core data image \"edgexfoundry/docker-core-data-go:1.2.1\" with \"nexus3.edgexfoundry.org:10004/docker-core-data-go:master\" in the Compose file. Note The example above replaces the Geneva core data service from Docker Hub with the latest core data image in Nexus.","title":"Using a Single Nexus Image"},{"location":"getting-started/quick-start/","text":"Quick Start This guide will get EdgeX up and running on your machine in as little as 5 minutes. We will skip over lengthy descriptions for now. The goal here is to get you a working IoT Edge stack, from device to cloud, as simply as possible. When you need more detailed instructions or a breakdown of some of the commands you see in this quick start, see either the Getting Started- Users or Getting Started - Developers guides. Setup The fastest way to start running EdgeX is by using our pre-built Docker images. To use them you'll need to install the following: Docker https://docs.docker.com/install/ Docker Compose https://docs.docker.com/compose/install/ Running EdgeX Once you have Docker and Docker Compose installed, you need to: download / save the latest docker-compose file issue command to download and run the EdgeX Foundry Docker images from Docker Hub This can be accomplished with a single command as shown below (please note the tabs for x86 vs ARM architectures). x86 curl https://raw.githubusercontent.com/edgexfoundry/developer-scripts/master/releases/geneva/compose-files/docker-compose-geneva-redis-no-secty.yml -o docker-compose.yml; docker-compose up ARM curl https://raw.githubusercontent.com/edgexfoundry/developer-scripts/master/releases/geneva/compose-files/docker-compose-geneva-redis-no-secty-arm64.yml -o docker-compose.yml; docker-compose up Verify that the EdgeX containers have started: docker-compose ps If all EdgeX containers pulled and started correctly and without error, you should see a process status (ps) that looks similar to the image above. Connecting a Device EdgeX Foundry provides a Random Number device service which is useful to testing, it returns a random number within a configurable range. Configuration for running this service is in the docker-compose.yml file you downloaded at the start of this guide, but it is disabled by default. To enable it, uncomment the following lines in your docker-compose.yml : device-random : image : edgexfoundry/docker-device-random-go:1.2.1 ports : - \"127.0.0.1:49988:49988\" container_name : edgex-device-random hostname : edgex-device-random networks : - edgex-network environment : << : *common-variables Service_Host : edgex-device-random depends_on : - data - command Then you can start the Random device service with: docker-compose up -d device-random The device service will register a device named Random-Integer-Generator01 , which will start sending its random number readings into EdgeX. You can verify that those readings are being sent by querying the EdgeX core data service for the last 10 event records sent for Random-Integer-Generator01: curl http://localhost:48080/api/v1/event/device/Random-Integer-Generator01/10 Verify the random device service is operating correctly by requesting the last 10 event records received by core data for the Random-Integer-Generator device. Controlling the Device Reading data from devices is only part of what EdgeX is capable of. You can also use it to control your devices - this is termed 'actuating' the device. When a device registers with the EdgeX services, it provides a Device Profile that describes both the data readings available from that device, and also the commands that control it. When our Random Number device service registered the device Random-Integer-Generator01 , it used a profile which defines commands for changing the minimum and maximum values for the random numbers it will generate. Note The URLs won't be exactly the same for you, as the generated unique IDs for both the Device and the Command will be different. So be sure to use your values for the following steps. Warning Notice that localhost replaces edgex-core-command here. That's because the EdgeX Foundry services are running in Docker. Docker recognizes the internal hostname edgex-core-command , but when calling the service from outside of Docker, you have to use localhost to reach it. This command will return a JSON result that looks like this: { \"device\" : \"Random-Integer-Generator01\" , \"origin\" : 1592231895237359000 , \"readings\" : [ { \"origin\" : 1592231895237098000 , \"device\" : \"Random-Integer-Generator01\" , \"name\" : \"RandomValue_Int8\" , \"value\" : \"-45\" , \"valueType\" : \"Int8\" } ], \"EncodedEvent\" : null } A call to GET of the Random-Integer-Generator01 device's GenerateRandomValue_Int8 operation through the command service results in the next random value produced by the device in JSON format. Warning Again, also notice that localhost replaces edgex-core-command . There is no visible result of calling PUT if the call is successful. A call to the device's PUT command through the command service will return no results. Now every time we call GET on this command, the returned value will be between 0 and 100. Exporting Data EdgeX provides exporters (called application services) for a variety of cloud services and applications. To keep this guide simple, we're going to use the community provided 'application service configurable' to send the EdgeX data to a public MQTT broker hosted by HiveMQ. You can then watch for the EdgeX event data via HiveMQ provided MQTT browser client. First add the following application service to your docker-compose.yml file right after the 'rulesengine' service (around line 255). Spacing is important in YAML, so make sure to copy and paste it correctly. app-service-mqtt : image : edgexfoundry/docker-app-service-configurable:1.1.0 ports : - \"127.0.0.1:48101:48101\" container_name : edgex-app-service-configurable-mqtt hostname : edgex-app-service-configurable-mqtt networks : - edgex-network environment : << : *common-variables edgex_profile : mqtt-export Service_Host : edgex-app-service-configurable-mqtt Service_Port : 48101 MessageBus_SubscribeHost_Host : edgex-core-data Binding_PublishTopic : events Writable_Pipeline_Functions_MQTTSend_Addressable_Address : broker.mqttdashboard.com Writable_Pipeline_Functions_MQTTSend_Addressable_Port : 1883 Writable_Pipeline_Functions_MQTTSend_Addressable_Protocol : tcp Writable_Pipeline_Functions_MQTTSend_Addressable_Publisher : edgex Writable_Pipeline_Functions_MQTTSend_Addressable_Topic : EdgeXEvents depends_on : - consul - data Note This adds the application service configurable to your EdgeX system. The application service configurable allows you to configure (versus program) new exports - in this case exporting the EdgeX sensor data to the HiveMQ broker at broker.mqttdashboard.com port 1883. You will be publishing to EdgeXEvents topic. Save the compose file and then execute another compose up command to have Docker Compose pull and start the configurable application service. docker-compose up -d You can connect to this broker with any MQTT client to watch the sent data. HiveMQ provides a web-based client that you can use. Use a browser to go to the client's URL. Once there, hit the Connect button to connect to the HiveMQ public broker. Using the HiveMQ provided client tool, connect to the same public HiveMQ broker your configurable application service is sending EdgeX data to. Then, use the Subscriptions area to subscribe to the \"EdgeXEvents\" topic. You must subscribe to the same topic - EdgeXEvents - to see the EdgeX data sent by the configurable application service. You will begin seeing your random number readings appear in the Messages area on the screen. Once subscribed, the EdgeX event data will begin to appear in the Messages area on the browser screen. Next Steps Congratulations! You now have a full EdgeX deployment reading data from a (virtual) device and publishing it to an MQTT broker in the cloud, and you were able to control your device through commands into EdgeX. It's time to continue your journey by reading the Introduction to EdgeX Foundry, what it is and how it's built. From there you can take the Walkthrough to learn how the microservices work together to control devices and read data from them as you just did. REFERENCE Platform Requirements EdgeX Foundry is an operating system (OS)-agnostic and hardware (HW)-agnostic IoT edge platform. At this time the following platform minimums are recommended: Memory Memory: minimum of 1 GB Storage Hard drive space: minimum of 3 GB of space to run the EdgeX Foundry containers, but you may want more depending on how long sensor and device data is to be retained. Approximately 32GB of storage is minimumally recommended to start. Operating Systems EdgeX Foundry has been run successfully on many systems, including, but not limited to the following systems Windows (ver 7 - 10) Ubuntu Desktop (ver 14-20) Ubuntu Server (ver 14-20) Ubuntu Core (ver 16-18) Mac OS X 10 Info EdgeX is agnostic with regards to hardware (x86 and ARM), but only release artifacts for x86 and ARM 64 systems. EdgeX has been successfuly run on ARM 32 platforms but has required users to build their own executables from source. EdgeX does not officially support ARM 32. REFERENCE EdgeX Container Names The following table provies the list of the default EdgeX Docker image names to the Docker container name and Docker Compose names. Core Docker image name Docker container name Docker Compose service name docker-core-data-go edgex-core-data data docker-core-metadata-go edgex-core-metadata metadata docker-core-command-go edgex-core-command command Supporting Docker image name Docker container name Docker Compose service name docker-support-notifications-go edgex-support-notifications notifications docker-support-logging-go edgex-support-logging logging docker-support-scheduler-go edgex-support-scheduler scheduler Application & Analytics Docker image name Docker container name Docker Compose service name docker-app-service-configurable edgex-app-service-configurable-rules app-service-rules emqx/kuiper edgex-kuiper rulesengine Device Docker image name Docker container name Docker Compose service name docker-device-virtual-go edgex-device-virtual device-virtual docker-device-random-go edgex-device-random device-random docker-device-mqtt-go edgex-device-mqtt device-mqtt docker-device-rest-go edgex-device-rest device-rest docker-device-modbus-go edgex-device-modbus device-modbus docker-device-snmp-go edgex-device-snmp device-snmp Security Docker image name Docker container name Docker Compose service name vault edgex-vault vault postgress kong-db kong-db kong kong kong docker-edgex-security-proxy-setup-go edgex-proxy edgex-proxy Miscellaneous Docker image name Docker container name Docker Compose service name docker-edgex-consul edgex-core-consul consul mongo edgex-mongo mongo redis edgex-redis redis docker-sys-mgmt-agent-go edgex-sys-mgmt-agent system REFERENCE Default Service Ports The following tables (organized by type of service) capture the default service ports. These default ports are also used in the EdgeX provided service routes defined in the Kong API Gateway for access control. Core Services Name Port Definition core-data 48080 5563 core-metadata 48001 core-command 48082 Supporting Services Name Port Definition support-notifications 48060 support-logging 48061 support-scheduler 48085 Application & Analytics Services Name Port Definition app-service-rules 48095 rules engine/Kuiper 48075 20498 Device Services Name Port Definition device-virtual 49990 device-random 49988 device-mqtt 49982 device-rest 49986 device-modbus 49991 device-snmp 49993 Security Services Name Port Definition vault 8200 kong-db 5432 kong 8000 8001 8443 8444 Miscellaneous Services Name Port Definition consul 8400 8500 8600 mongo 27017 redis 6379 system management 48090","title":"Quick Start"},{"location":"getting-started/quick-start/#quick-start","text":"This guide will get EdgeX up and running on your machine in as little as 5 minutes. We will skip over lengthy descriptions for now. The goal here is to get you a working IoT Edge stack, from device to cloud, as simply as possible. When you need more detailed instructions or a breakdown of some of the commands you see in this quick start, see either the Getting Started- Users or Getting Started - Developers guides.","title":"Quick Start"},{"location":"getting-started/quick-start/#setup","text":"The fastest way to start running EdgeX is by using our pre-built Docker images. To use them you'll need to install the following: Docker https://docs.docker.com/install/ Docker Compose https://docs.docker.com/compose/install/","title":"Setup"},{"location":"getting-started/quick-start/#running-edgex","text":"Once you have Docker and Docker Compose installed, you need to: download / save the latest docker-compose file issue command to download and run the EdgeX Foundry Docker images from Docker Hub This can be accomplished with a single command as shown below (please note the tabs for x86 vs ARM architectures). x86 curl https://raw.githubusercontent.com/edgexfoundry/developer-scripts/master/releases/geneva/compose-files/docker-compose-geneva-redis-no-secty.yml -o docker-compose.yml; docker-compose up ARM curl https://raw.githubusercontent.com/edgexfoundry/developer-scripts/master/releases/geneva/compose-files/docker-compose-geneva-redis-no-secty-arm64.yml -o docker-compose.yml; docker-compose up Verify that the EdgeX containers have started: docker-compose ps If all EdgeX containers pulled and started correctly and without error, you should see a process status (ps) that looks similar to the image above.","title":"Running EdgeX"},{"location":"getting-started/quick-start/#connecting-a-device","text":"EdgeX Foundry provides a Random Number device service which is useful to testing, it returns a random number within a configurable range. Configuration for running this service is in the docker-compose.yml file you downloaded at the start of this guide, but it is disabled by default. To enable it, uncomment the following lines in your docker-compose.yml : device-random : image : edgexfoundry/docker-device-random-go:1.2.1 ports : - \"127.0.0.1:49988:49988\" container_name : edgex-device-random hostname : edgex-device-random networks : - edgex-network environment : << : *common-variables Service_Host : edgex-device-random depends_on : - data - command Then you can start the Random device service with: docker-compose up -d device-random The device service will register a device named Random-Integer-Generator01 , which will start sending its random number readings into EdgeX. You can verify that those readings are being sent by querying the EdgeX core data service for the last 10 event records sent for Random-Integer-Generator01: curl http://localhost:48080/api/v1/event/device/Random-Integer-Generator01/10 Verify the random device service is operating correctly by requesting the last 10 event records received by core data for the Random-Integer-Generator device.","title":"Connecting a Device"},{"location":"getting-started/quick-start/#controlling-the-device","text":"Reading data from devices is only part of what EdgeX is capable of. You can also use it to control your devices - this is termed 'actuating' the device. When a device registers with the EdgeX services, it provides a Device Profile that describes both the data readings available from that device, and also the commands that control it. When our Random Number device service registered the device Random-Integer-Generator01 , it used a profile which defines commands for changing the minimum and maximum values for the random numbers it will generate. Note The URLs won't be exactly the same for you, as the generated unique IDs for both the Device and the Command will be different. So be sure to use your values for the following steps. Warning Notice that localhost replaces edgex-core-command here. That's because the EdgeX Foundry services are running in Docker. Docker recognizes the internal hostname edgex-core-command , but when calling the service from outside of Docker, you have to use localhost to reach it. This command will return a JSON result that looks like this: { \"device\" : \"Random-Integer-Generator01\" , \"origin\" : 1592231895237359000 , \"readings\" : [ { \"origin\" : 1592231895237098000 , \"device\" : \"Random-Integer-Generator01\" , \"name\" : \"RandomValue_Int8\" , \"value\" : \"-45\" , \"valueType\" : \"Int8\" } ], \"EncodedEvent\" : null } A call to GET of the Random-Integer-Generator01 device's GenerateRandomValue_Int8 operation through the command service results in the next random value produced by the device in JSON format. Warning Again, also notice that localhost replaces edgex-core-command . There is no visible result of calling PUT if the call is successful. A call to the device's PUT command through the command service will return no results. Now every time we call GET on this command, the returned value will be between 0 and 100.","title":"Controlling the Device"},{"location":"getting-started/quick-start/#exporting-data","text":"EdgeX provides exporters (called application services) for a variety of cloud services and applications. To keep this guide simple, we're going to use the community provided 'application service configurable' to send the EdgeX data to a public MQTT broker hosted by HiveMQ. You can then watch for the EdgeX event data via HiveMQ provided MQTT browser client. First add the following application service to your docker-compose.yml file right after the 'rulesengine' service (around line 255). Spacing is important in YAML, so make sure to copy and paste it correctly. app-service-mqtt : image : edgexfoundry/docker-app-service-configurable:1.1.0 ports : - \"127.0.0.1:48101:48101\" container_name : edgex-app-service-configurable-mqtt hostname : edgex-app-service-configurable-mqtt networks : - edgex-network environment : << : *common-variables edgex_profile : mqtt-export Service_Host : edgex-app-service-configurable-mqtt Service_Port : 48101 MessageBus_SubscribeHost_Host : edgex-core-data Binding_PublishTopic : events Writable_Pipeline_Functions_MQTTSend_Addressable_Address : broker.mqttdashboard.com Writable_Pipeline_Functions_MQTTSend_Addressable_Port : 1883 Writable_Pipeline_Functions_MQTTSend_Addressable_Protocol : tcp Writable_Pipeline_Functions_MQTTSend_Addressable_Publisher : edgex Writable_Pipeline_Functions_MQTTSend_Addressable_Topic : EdgeXEvents depends_on : - consul - data Note This adds the application service configurable to your EdgeX system. The application service configurable allows you to configure (versus program) new exports - in this case exporting the EdgeX sensor data to the HiveMQ broker at broker.mqttdashboard.com port 1883. You will be publishing to EdgeXEvents topic. Save the compose file and then execute another compose up command to have Docker Compose pull and start the configurable application service. docker-compose up -d You can connect to this broker with any MQTT client to watch the sent data. HiveMQ provides a web-based client that you can use. Use a browser to go to the client's URL. Once there, hit the Connect button to connect to the HiveMQ public broker. Using the HiveMQ provided client tool, connect to the same public HiveMQ broker your configurable application service is sending EdgeX data to. Then, use the Subscriptions area to subscribe to the \"EdgeXEvents\" topic. You must subscribe to the same topic - EdgeXEvents - to see the EdgeX data sent by the configurable application service. You will begin seeing your random number readings appear in the Messages area on the screen. Once subscribed, the EdgeX event data will begin to appear in the Messages area on the browser screen.","title":"Exporting Data"},{"location":"getting-started/quick-start/#next-steps","text":"Congratulations! You now have a full EdgeX deployment reading data from a (virtual) device and publishing it to an MQTT broker in the cloud, and you were able to control your device through commands into EdgeX. It's time to continue your journey by reading the Introduction to EdgeX Foundry, what it is and how it's built. From there you can take the Walkthrough to learn how the microservices work together to control devices and read data from them as you just did.","title":"Next Steps"},{"location":"getting-started/quick-start/#reference-platform-requirements","text":"EdgeX Foundry is an operating system (OS)-agnostic and hardware (HW)-agnostic IoT edge platform. At this time the following platform minimums are recommended: Memory Memory: minimum of 1 GB Storage Hard drive space: minimum of 3 GB of space to run the EdgeX Foundry containers, but you may want more depending on how long sensor and device data is to be retained. Approximately 32GB of storage is minimumally recommended to start. Operating Systems EdgeX Foundry has been run successfully on many systems, including, but not limited to the following systems Windows (ver 7 - 10) Ubuntu Desktop (ver 14-20) Ubuntu Server (ver 14-20) Ubuntu Core (ver 16-18) Mac OS X 10 Info EdgeX is agnostic with regards to hardware (x86 and ARM), but only release artifacts for x86 and ARM 64 systems. EdgeX has been successfuly run on ARM 32 platforms but has required users to build their own executables from source. EdgeX does not officially support ARM 32.","title":"REFERENCE Platform Requirements"},{"location":"getting-started/quick-start/#reference-edgex-container-names","text":"The following table provies the list of the default EdgeX Docker image names to the Docker container name and Docker Compose names. Core Docker image name Docker container name Docker Compose service name docker-core-data-go edgex-core-data data docker-core-metadata-go edgex-core-metadata metadata docker-core-command-go edgex-core-command command Supporting Docker image name Docker container name Docker Compose service name docker-support-notifications-go edgex-support-notifications notifications docker-support-logging-go edgex-support-logging logging docker-support-scheduler-go edgex-support-scheduler scheduler Application & Analytics Docker image name Docker container name Docker Compose service name docker-app-service-configurable edgex-app-service-configurable-rules app-service-rules emqx/kuiper edgex-kuiper rulesengine Device Docker image name Docker container name Docker Compose service name docker-device-virtual-go edgex-device-virtual device-virtual docker-device-random-go edgex-device-random device-random docker-device-mqtt-go edgex-device-mqtt device-mqtt docker-device-rest-go edgex-device-rest device-rest docker-device-modbus-go edgex-device-modbus device-modbus docker-device-snmp-go edgex-device-snmp device-snmp Security Docker image name Docker container name Docker Compose service name vault edgex-vault vault postgress kong-db kong-db kong kong kong docker-edgex-security-proxy-setup-go edgex-proxy edgex-proxy Miscellaneous Docker image name Docker container name Docker Compose service name docker-edgex-consul edgex-core-consul consul mongo edgex-mongo mongo redis edgex-redis redis docker-sys-mgmt-agent-go edgex-sys-mgmt-agent system","title":"REFERENCE EdgeX Container Names"},{"location":"getting-started/quick-start/#reference-default-service-ports","text":"The following tables (organized by type of service) capture the default service ports. These default ports are also used in the EdgeX provided service routes defined in the Kong API Gateway for access control. Core Services Name Port Definition core-data 48080 5563 core-metadata 48001 core-command 48082 Supporting Services Name Port Definition support-notifications 48060 support-logging 48061 support-scheduler 48085 Application & Analytics Services Name Port Definition app-service-rules 48095 rules engine/Kuiper 48075 20498 Device Services Name Port Definition device-virtual 49990 device-random 49988 device-mqtt 49982 device-rest 49986 device-modbus 49991 device-snmp 49993 Security Services Name Port Definition vault 8200 kong-db 5432 kong 8000 8001 8443 8444 Miscellaneous Services Name Port Definition consul 8400 8500 8600 mongo 27017 redis 6379 system management 48090","title":"REFERENCE Default Service Ports"},{"location":"microservices/application/AdvancedTopics/","text":"Advanced Topics The following items discuss topics that are a bit beyond the basic use cases of the Application Functions SDK when interacting with EdgeX. Configurable Functions Pipeline This SDK provides the capability to define the functions pipeline via configuration rather than code by using the app-service-configurable application service. See the App Service Configurable section for more details. Using The Webserver It is not uncommon to require your own API endpoints when building an app service. Rather than spin up your own webserver inside of your app (alongside the already existing running webserver), we've exposed a method that allows you add your own routes to the existing webserver. A few routes are reserved and cannot be used: /api/version /api/v1/ping /api/v1/metrics /api/v1/config /api/v1/trigger /api/v1/secrets To add your own route, use the AddRoute(route string, handler func(nethttp.ResponseWriter, *nethttp.Request), methods ...string) error function provided on the SDK. Here's an example: edgexSdk . AddRoute ( \"/myroute\" , func ( writer http . ResponseWriter , req * http . Request ) { context := req . Context (). Value ( appsdk . SDKKey ).( * appsdk . AppFunctionsSDK ) context . LoggingClient . Info ( \"TEST\" ) // alternative to edgexSdk.LoggingClient.Info(\"TEST\") writer . Header (). Set ( \"Content-Type\" , \"text/plain\" ) writer . Write ([] byte ( \"hello\" )) writer . WriteHeader ( 200 ) }, \"GET\" ) Under the hood, this simply adds the provided route, handler, and method to the gorilla mux.Router we use in the SDK. For more information on gorilla mux you can check out the github repo here . You can access the resources such as the logging client by accessing the context as shown above -- this is useful for when your routes might not be defined in your main.go where you have access to the edgexSdk instance. Target Type The target type is the object type of the incoming data that is sent to the first function in the function pipeline. By default this is an EdgeX Event since typical usage is receiving events from Core Data via Message Bus. For other usages where the data is not events coming from Core Data, the TargetType of the accepted incoming data can be set when the SDK instance is created. There are scenarios where the incoming data is not an EdgeX Event . One example scenario is 2 application services are chained via the Message Bus. The output of the first service back to the Message Bus is inference data from analyzing the original input Event data. The second service needs to be able to let the SDK know the target type of the input data it is expecting. For usages where the incoming data is not events , the TargetType of the excepted incoming data can be set when the SDK instance is created. Example: type Person struct { FirstName string `json:\"first_name\"` LastName string `json:\"last_name\"` } edgexSdk := & appsdk . AppFunctionsSDK { ServiceKey : serviceKey , TargetType : & Person {}, } TargetType must be set to a pointer to an instance of your target type such as &Person{} . The first function in your function pipeline will be passed an instance of your target type, not a pointer to it. In the example above, the first function in the pipeline would start something like: func MyPersonFunction ( edgexcontext * appcontext . Context , params ... interface {}) ( bool , interface {}) { edgexcontext . LoggingClient . Debug ( \"MyPersonFunction\" ) if len ( params ) < 1 { // We didn't receive a result return false , nil } person , ok := params [ 0 ].( Person ) if ! ok { return false , errors . New ( \"type received is not a Person\" ) } // .... The SDK supports un-marshaling JSON or CBOR encoded data into an instance of the target type. If your incoming data is not JSON or CBOR encoded, you then need to set the TargetType to &[]byte . If the target type is set to &[]byte the incoming data will not be un-marshaled. The content type, if set, will be passed as the second parameter to the first function in your pipeline. Your first function will be responsible for decoding the data or not. Command Line Options The following command line options are available -c=<path> --confdir=<path> Specify an alternate configuration directory. -p=<profile> --profile=<profile> Specify a profile other than default. -f, --file <name> Indicates name of the local configuration file. Defaults to configuration.toml -cp=<url> --configProvider=<url> Indicates to use Configuration Provider service at specified URL. URL Format: {type}.{protocol}://{host}:{port} ex: consul.http://localhost:8500 No url, i.e. -cp, defaults to consul.http://localhost:8500 -o -overwrite Force overwrite configuration in the Configuration Provider with local values. -r --registry Indicates the service should use the service Registry. -s -skipVersionCheck Indicates the service should skip the Core Service's version compatibility check. -sk --serviceKey Overrides the service key used with Registry and/or Configuration Providers. If the name provided contains the text `<profile>`, this text will be replaced with the name of the profile used. Examples: simple-filter-xml -c = ./res -p = http-export or simple-filter-xml --confdir = ./res -p = http-export -cp = consul.http://localhost:8500 --registry Environment Variable Overrides All the configuration settings from the configuration.toml file can be overridden by environment variables. The environment variable names have the following format: < TOML KEY > < TOML SECTION > _ < TOML KEY > < TOML SECTION > _ < TOML SUB-SECTION > _ < TOML KEY > Note With the Geneva release CamelCase environment variable names are deprecated. Instead use all uppercase environment variable names as in the example below. Examples: TOML : FailLimit = 30 ENVVAR : FAILLIMIT = 100 TOML : [Logging] EnableRemote = false ENVVAR : LOGGING_ENABLEREMOTE = true TOML : [Clients] [Clients.CoreData] Host = 'localhost' ENVVAR : CLIENTS_COREDATA_HOST = edgex-core-data EDGEX_SERVICE_KEY This environment variable overrides the service key used with the Configuration and/or Registry providers. Default is set by the application service. Also overrides any value set with the -sk/--serviceKey command-line option. Note If the name provided contains the text <profile> , this text will be replaced with the name of the profile used. Example EDGEX_SERVICE_KEY: AppService-<profile>-mycloud and if profile: http-export then service key will be \"AppService-http-export-mycloud\" EDGEX_CONFIGURATION_PROVIDER This environment variable overrides the Configuration Provider connection information. The value is in the format of a URL. EDGEX_CONFIGURATION_PROVIDER=consul.http://edgex-core-consul:8500 This sets the Configration Provider information fields as follows: Type: consul Host: edgex-core-consul Port: 8500 edgex_registry (DEPRECATED) This environment variable overrides the Registry connection information and occurs every time the application service starts. The value is in the format of a URL. Note This environment variable override has been deprecated in the Geneva Release. Instead, use configuration overrides of REGISTRY_PROTOCOL and/or REGISTRY_HOST and/or REGISTRY_PORT EDGEX_REGISTRY=consul://edgex-core-consul:8500 This sets the Registry information fields as follows: Type: consul Host: edgex-core-consul Port: 8500 edgex_service (DEPRECATED) This environment variable overrides the Service connection information. The value is in the format of a URL. Note This environment variable override has been deprecated in the Geneva Release. Instead, use configuration overrides of SERVICE_PROTOCOL and/or SERVICE_HOST and/or SERVICE_PORT EDGEX_SERVICE=http://192.168.1.2:4903 This sets the Service information fields as follows: Protocol: http Host: 192.168.1.2 Port: 4903 EDGEX_PROFILE This environment variable overrides the command line profile argument. It will set the profile or replace the value passed via the -p or --profile , if one exists. This is useful when running the service via docker-compose. Note The lower case version has been deprecated in the Geneva release. Instead use upper case version EDGEX_PROFILE Using docker-compose: app-service-configurable-rules: image: edgexfoundry/docker-app-service-configurable:1.1.0 environment: - EDGEX_PROFILE : \"rules-engine\" ports: - \"48095:48095\" container_name: edgex-app-service-configurable hostname: edgex-app-service-configurable networks: edgex-network: aliases: - edgex-app-service-configurable depends_on: - data - command This sets the profile so that the application service uses the rules-engine configuration profile which resides at /res/rules-engine/configuration.toml Note EdgeX services no longer use docker profiles. They use Environment Overrides in the docker compose file to make the necessary changes to the configuration for running in Docker. See the Environment Variable Overrides For Docker * section in the App Service Configurable section for more details and an example. EDGEX_STARTUP_DURATION This environment variable overrides the default duration, 30 seconds, for a service to complete the start-up, aka bootstrap, phase of execution EDGEX_STARTUP_INTERVAL This environment variable overrides the retry interval or sleep time before a failure is retried during the start-up, aka bootstrap, phase of execution. EDGEX_CONF_DIR This environment variable overrides the configuration directory where the configuration file resides. Default is ./res and also overrides any value set with the -c/--confdir command-line option. EDGEX_CONFIG_FILE This environment variable overrides the configuration file name. Default is configutation.toml and also overrides any value set with the -f/--file command-line option. Store and Forward The Store and Forward capability allows for export functions to persist data on failure and for the export of the data to be retried at a later time. Note The order the data exported via this retry mechanism is not guaranteed to be the same order in which the data was initial received from Core Data Configuration Two sections of configuration have been added for Store and Forward. Writable.StoreAndForward allows enabling, setting the interval between retries and the max number of retries. If running with Configuration Provider, these setting can be changed on the fly without having to restart the service. [Writable.StoreAndForward] Enabled = false RetryInterval = '5m' MaxRetryCount = 10 Note RetryInterval should be at least 1 second (eg. '1s') or greater. If a value less than 1 second is specified, 1 second will be used. Endless retries will occur when MaxRetryCount is set to 0. If MaxRetryCount is set to less than 0, a default of 1 retry will be used. Database describes which database type to use, mongodb (DEPRECATED) or redisdb , and the information required to connect to the database. This section is required if Store and Forward is enabled, otherwise it is currently optional. [Database] Type = \"redisdb\" Host = \"localhost\" Port = 6379 Timeout = '30s' Username = \"\" Password = \"\" How it works When an export function encounters an error sending data it can call SetRetryData(payload []byte) on the Context. This will store the data for later retry. If the application service is stopped and then restarted while stored data hasn't been successfully exported, the export retry will resume once the service is up and running again. Note It is important that export functions return an error and stop pipeline execution after the call to SetRetryData . See HTTPPost function in SDK as an example When the RetryInterval expires, the function pipeline will be re-executed starting with the export function that saved the data. The saved data will be passed to the export function which can then attempt to resend the data. Note The export function will receive the data as it was stored, so it is important that any transformation of the data occur in functions prior to the export function. The export function should only export the data that it receives. One of three out comes can occur after the export retried has completed. Export retry was successful In this case, the stored data is removed from the database and the execution of the pipeline functions after the export function, if any, continues. Export retry fails and retry count has not been exceeded In this case, the stored data is updated in the database with the incremented retry count Export retry fails and retry count has been exceeded In this case, the stored data is removed from the database and never retried again. Note Changing Writable.Pipeline.ExecutionOrder will invalidate all currently stored data and result in it all being removed from the database on the next retry. This is because the position of the export function can no longer be guaranteed and no way to ensure it is properly executed on the retry. Secrets Configuration All instances of App Services share the same database and database credentials. However, there are secrets for each App Service that are exclusive to the instance running. As a result, two separate configurations for secret store clients are used to manage shared and exclusive application service secrets. The GetSecrets() and StoreSecrets() calls use the exclusive secret store client to manage application secrets. An example of configuration settings for each secret store client is below: # Shared Secret Store [SecretStore] Host = 'localhost' Port = 8200 Path = '/v1/secret/edgex/appservice/' Protocol = 'https' RootCaCertPath = '/tmp/edgex/secrets/ca/ca.pem' ServerName = 'edgex-vault' TokenFile = '/tmp/edgex/secrets/edgex-appservice/secrets-token.json' # Number of attempts to retry retrieving secrets before failing to start the service. AdditionalRetryAttempts = 10 # Amount of time to wait before attempting another retry RetryWaitPeriod = \"1s\" [SecretStore.Authentication] AuthType = 'X-Vault-Token' # Exclusive Secret Store [SecretStoreExclusive] Host = 'localhost' Port = 8200 Path = '/v1/secret/edgex/<app service key>/' Protocol = 'https' ServerName = 'edgex-vault' TokenFile = '/tmp/edgex/secrets/<app service key>/secrets-token.json' # Number of attempts to retry retrieving secrets before failing to start the service. AdditionalRetryAttempts = 10 # Amount of time to wait before attempting another retry RetryWaitPeriod = \"1s\" [SecretStoreExclusive.Authentication] AuthType = 'X-Vault-Token' Storing Secrets Secure Mode When running an application service in secure mode, secrets can be stored in the secret store (Vault) by making an HTTP POST call to the secrets API route in the application service, http://[host]:[port]/api/v1/secrets . The secrets are stored and retrieved from the secret store based on values in the SecretStoreExclusive section of the configuration file. Once a secret is stored, only the service that added the secret will be able to retrieve it. For secret retrieval see Getting Secrets . An example of the JSON message body is below. { \"path\" : \"MyPath\" , \"secrets\" : [ { \"key\" : \"MySecretKey\" , \"value\" : \"MySecretValue\" } ] } Note Path specifies the type or location of the secrets to store. It is appended to the base path from the SecretStoreExclusive configuration. An empty path is a valid configuration for a secret's location. Insecure Mode When running in insecure mode, the secrets are stored and retrieved from the Writable.InsecureSecrets section of the service's configuration toml file. Insecure secrets and their paths can be configured as below. [ Writable . InsecureSecrets ] [Writable.InsecureSecrets.AWS] Path = 'aws' [Writable.InsecureSecrets.AWS.Secrets] username = 'aws-user' password = 'aws-pw' [Writable.InsecureSecrets.DB] Path = 'redisdb' [Writable.InsecureSecrets.DB.Secrets] username = '' password = '' Note An empty path is a valid configuration for a secret's location Getting Secrets Application Services can retrieve their secrets from the underlying secret store using the GetSecrets() API in the SDK. If in secure mode, the secrets are retrieved from the secret store based on the SecretStoreExclusive configuration values. If running in insecure mode, the secrets are retrieved from the Writable.InsecureSecrets configuration.","title":"Advanced Topics"},{"location":"microservices/application/AdvancedTopics/#advanced-topics","text":"The following items discuss topics that are a bit beyond the basic use cases of the Application Functions SDK when interacting with EdgeX.","title":"Advanced Topics"},{"location":"microservices/application/AdvancedTopics/#configurable-functions-pipeline","text":"This SDK provides the capability to define the functions pipeline via configuration rather than code by using the app-service-configurable application service. See the App Service Configurable section for more details.","title":"Configurable Functions Pipeline"},{"location":"microservices/application/AdvancedTopics/#using-the-webserver","text":"It is not uncommon to require your own API endpoints when building an app service. Rather than spin up your own webserver inside of your app (alongside the already existing running webserver), we've exposed a method that allows you add your own routes to the existing webserver. A few routes are reserved and cannot be used: /api/version /api/v1/ping /api/v1/metrics /api/v1/config /api/v1/trigger /api/v1/secrets To add your own route, use the AddRoute(route string, handler func(nethttp.ResponseWriter, *nethttp.Request), methods ...string) error function provided on the SDK. Here's an example: edgexSdk . AddRoute ( \"/myroute\" , func ( writer http . ResponseWriter , req * http . Request ) { context := req . Context (). Value ( appsdk . SDKKey ).( * appsdk . AppFunctionsSDK ) context . LoggingClient . Info ( \"TEST\" ) // alternative to edgexSdk.LoggingClient.Info(\"TEST\") writer . Header (). Set ( \"Content-Type\" , \"text/plain\" ) writer . Write ([] byte ( \"hello\" )) writer . WriteHeader ( 200 ) }, \"GET\" ) Under the hood, this simply adds the provided route, handler, and method to the gorilla mux.Router we use in the SDK. For more information on gorilla mux you can check out the github repo here . You can access the resources such as the logging client by accessing the context as shown above -- this is useful for when your routes might not be defined in your main.go where you have access to the edgexSdk instance.","title":"Using The Webserver"},{"location":"microservices/application/AdvancedTopics/#target-type","text":"The target type is the object type of the incoming data that is sent to the first function in the function pipeline. By default this is an EdgeX Event since typical usage is receiving events from Core Data via Message Bus. For other usages where the data is not events coming from Core Data, the TargetType of the accepted incoming data can be set when the SDK instance is created. There are scenarios where the incoming data is not an EdgeX Event . One example scenario is 2 application services are chained via the Message Bus. The output of the first service back to the Message Bus is inference data from analyzing the original input Event data. The second service needs to be able to let the SDK know the target type of the input data it is expecting. For usages where the incoming data is not events , the TargetType of the excepted incoming data can be set when the SDK instance is created. Example: type Person struct { FirstName string `json:\"first_name\"` LastName string `json:\"last_name\"` } edgexSdk := & appsdk . AppFunctionsSDK { ServiceKey : serviceKey , TargetType : & Person {}, } TargetType must be set to a pointer to an instance of your target type such as &Person{} . The first function in your function pipeline will be passed an instance of your target type, not a pointer to it. In the example above, the first function in the pipeline would start something like: func MyPersonFunction ( edgexcontext * appcontext . Context , params ... interface {}) ( bool , interface {}) { edgexcontext . LoggingClient . Debug ( \"MyPersonFunction\" ) if len ( params ) < 1 { // We didn't receive a result return false , nil } person , ok := params [ 0 ].( Person ) if ! ok { return false , errors . New ( \"type received is not a Person\" ) } // .... The SDK supports un-marshaling JSON or CBOR encoded data into an instance of the target type. If your incoming data is not JSON or CBOR encoded, you then need to set the TargetType to &[]byte . If the target type is set to &[]byte the incoming data will not be un-marshaled. The content type, if set, will be passed as the second parameter to the first function in your pipeline. Your first function will be responsible for decoding the data or not.","title":"Target Type"},{"location":"microservices/application/AdvancedTopics/#command-line-options","text":"The following command line options are available -c=<path> --confdir=<path> Specify an alternate configuration directory. -p=<profile> --profile=<profile> Specify a profile other than default. -f, --file <name> Indicates name of the local configuration file. Defaults to configuration.toml -cp=<url> --configProvider=<url> Indicates to use Configuration Provider service at specified URL. URL Format: {type}.{protocol}://{host}:{port} ex: consul.http://localhost:8500 No url, i.e. -cp, defaults to consul.http://localhost:8500 -o -overwrite Force overwrite configuration in the Configuration Provider with local values. -r --registry Indicates the service should use the service Registry. -s -skipVersionCheck Indicates the service should skip the Core Service's version compatibility check. -sk --serviceKey Overrides the service key used with Registry and/or Configuration Providers. If the name provided contains the text `<profile>`, this text will be replaced with the name of the profile used. Examples: simple-filter-xml -c = ./res -p = http-export or simple-filter-xml --confdir = ./res -p = http-export -cp = consul.http://localhost:8500 --registry","title":"Command Line Options"},{"location":"microservices/application/AdvancedTopics/#environment-variable-overrides","text":"All the configuration settings from the configuration.toml file can be overridden by environment variables. The environment variable names have the following format: < TOML KEY > < TOML SECTION > _ < TOML KEY > < TOML SECTION > _ < TOML SUB-SECTION > _ < TOML KEY > Note With the Geneva release CamelCase environment variable names are deprecated. Instead use all uppercase environment variable names as in the example below. Examples: TOML : FailLimit = 30 ENVVAR : FAILLIMIT = 100 TOML : [Logging] EnableRemote = false ENVVAR : LOGGING_ENABLEREMOTE = true TOML : [Clients] [Clients.CoreData] Host = 'localhost' ENVVAR : CLIENTS_COREDATA_HOST = edgex-core-data","title":"Environment Variable Overrides"},{"location":"microservices/application/AdvancedTopics/#edgex_service_key","text":"This environment variable overrides the service key used with the Configuration and/or Registry providers. Default is set by the application service. Also overrides any value set with the -sk/--serviceKey command-line option. Note If the name provided contains the text <profile> , this text will be replaced with the name of the profile used. Example EDGEX_SERVICE_KEY: AppService-<profile>-mycloud and if profile: http-export then service key will be \"AppService-http-export-mycloud\"","title":"EDGEX_SERVICE_KEY"},{"location":"microservices/application/AdvancedTopics/#edgex_configuration_provider","text":"This environment variable overrides the Configuration Provider connection information. The value is in the format of a URL. EDGEX_CONFIGURATION_PROVIDER=consul.http://edgex-core-consul:8500 This sets the Configration Provider information fields as follows: Type: consul Host: edgex-core-consul Port: 8500","title":"EDGEX_CONFIGURATION_PROVIDER"},{"location":"microservices/application/AdvancedTopics/#edgex_registry-deprecated","text":"This environment variable overrides the Registry connection information and occurs every time the application service starts. The value is in the format of a URL. Note This environment variable override has been deprecated in the Geneva Release. Instead, use configuration overrides of REGISTRY_PROTOCOL and/or REGISTRY_HOST and/or REGISTRY_PORT EDGEX_REGISTRY=consul://edgex-core-consul:8500 This sets the Registry information fields as follows: Type: consul Host: edgex-core-consul Port: 8500","title":"edgex_registry (DEPRECATED)"},{"location":"microservices/application/AdvancedTopics/#edgex_service-deprecated","text":"This environment variable overrides the Service connection information. The value is in the format of a URL. Note This environment variable override has been deprecated in the Geneva Release. Instead, use configuration overrides of SERVICE_PROTOCOL and/or SERVICE_HOST and/or SERVICE_PORT EDGEX_SERVICE=http://192.168.1.2:4903 This sets the Service information fields as follows: Protocol: http Host: 192.168.1.2 Port: 4903","title":"edgex_service (DEPRECATED)"},{"location":"microservices/application/AdvancedTopics/#edgex_profile","text":"This environment variable overrides the command line profile argument. It will set the profile or replace the value passed via the -p or --profile , if one exists. This is useful when running the service via docker-compose. Note The lower case version has been deprecated in the Geneva release. Instead use upper case version EDGEX_PROFILE Using docker-compose: app-service-configurable-rules: image: edgexfoundry/docker-app-service-configurable:1.1.0 environment: - EDGEX_PROFILE : \"rules-engine\" ports: - \"48095:48095\" container_name: edgex-app-service-configurable hostname: edgex-app-service-configurable networks: edgex-network: aliases: - edgex-app-service-configurable depends_on: - data - command This sets the profile so that the application service uses the rules-engine configuration profile which resides at /res/rules-engine/configuration.toml Note EdgeX services no longer use docker profiles. They use Environment Overrides in the docker compose file to make the necessary changes to the configuration for running in Docker. See the Environment Variable Overrides For Docker * section in the App Service Configurable section for more details and an example.","title":"EDGEX_PROFILE"},{"location":"microservices/application/AdvancedTopics/#edgex_startup_duration","text":"This environment variable overrides the default duration, 30 seconds, for a service to complete the start-up, aka bootstrap, phase of execution","title":"EDGEX_STARTUP_DURATION"},{"location":"microservices/application/AdvancedTopics/#edgex_startup_interval","text":"This environment variable overrides the retry interval or sleep time before a failure is retried during the start-up, aka bootstrap, phase of execution.","title":"EDGEX_STARTUP_INTERVAL"},{"location":"microservices/application/AdvancedTopics/#edgex_conf_dir","text":"This environment variable overrides the configuration directory where the configuration file resides. Default is ./res and also overrides any value set with the -c/--confdir command-line option.","title":"EDGEX_CONF_DIR"},{"location":"microservices/application/AdvancedTopics/#edgex_config_file","text":"This environment variable overrides the configuration file name. Default is configutation.toml and also overrides any value set with the -f/--file command-line option.","title":"EDGEX_CONFIG_FILE"},{"location":"microservices/application/AdvancedTopics/#store-and-forward","text":"The Store and Forward capability allows for export functions to persist data on failure and for the export of the data to be retried at a later time. Note The order the data exported via this retry mechanism is not guaranteed to be the same order in which the data was initial received from Core Data","title":"Store and Forward"},{"location":"microservices/application/AdvancedTopics/#configuration","text":"Two sections of configuration have been added for Store and Forward. Writable.StoreAndForward allows enabling, setting the interval between retries and the max number of retries. If running with Configuration Provider, these setting can be changed on the fly without having to restart the service. [Writable.StoreAndForward] Enabled = false RetryInterval = '5m' MaxRetryCount = 10 Note RetryInterval should be at least 1 second (eg. '1s') or greater. If a value less than 1 second is specified, 1 second will be used. Endless retries will occur when MaxRetryCount is set to 0. If MaxRetryCount is set to less than 0, a default of 1 retry will be used. Database describes which database type to use, mongodb (DEPRECATED) or redisdb , and the information required to connect to the database. This section is required if Store and Forward is enabled, otherwise it is currently optional. [Database] Type = \"redisdb\" Host = \"localhost\" Port = 6379 Timeout = '30s' Username = \"\" Password = \"\"","title":"Configuration"},{"location":"microservices/application/AdvancedTopics/#how-it-works","text":"When an export function encounters an error sending data it can call SetRetryData(payload []byte) on the Context. This will store the data for later retry. If the application service is stopped and then restarted while stored data hasn't been successfully exported, the export retry will resume once the service is up and running again. Note It is important that export functions return an error and stop pipeline execution after the call to SetRetryData . See HTTPPost function in SDK as an example When the RetryInterval expires, the function pipeline will be re-executed starting with the export function that saved the data. The saved data will be passed to the export function which can then attempt to resend the data. Note The export function will receive the data as it was stored, so it is important that any transformation of the data occur in functions prior to the export function. The export function should only export the data that it receives. One of three out comes can occur after the export retried has completed. Export retry was successful In this case, the stored data is removed from the database and the execution of the pipeline functions after the export function, if any, continues. Export retry fails and retry count has not been exceeded In this case, the stored data is updated in the database with the incremented retry count Export retry fails and retry count has been exceeded In this case, the stored data is removed from the database and never retried again. Note Changing Writable.Pipeline.ExecutionOrder will invalidate all currently stored data and result in it all being removed from the database on the next retry. This is because the position of the export function can no longer be guaranteed and no way to ensure it is properly executed on the retry.","title":"How it works"},{"location":"microservices/application/AdvancedTopics/#secrets","text":"","title":"Secrets"},{"location":"microservices/application/AdvancedTopics/#configuration_1","text":"All instances of App Services share the same database and database credentials. However, there are secrets for each App Service that are exclusive to the instance running. As a result, two separate configurations for secret store clients are used to manage shared and exclusive application service secrets. The GetSecrets() and StoreSecrets() calls use the exclusive secret store client to manage application secrets. An example of configuration settings for each secret store client is below: # Shared Secret Store [SecretStore] Host = 'localhost' Port = 8200 Path = '/v1/secret/edgex/appservice/' Protocol = 'https' RootCaCertPath = '/tmp/edgex/secrets/ca/ca.pem' ServerName = 'edgex-vault' TokenFile = '/tmp/edgex/secrets/edgex-appservice/secrets-token.json' # Number of attempts to retry retrieving secrets before failing to start the service. AdditionalRetryAttempts = 10 # Amount of time to wait before attempting another retry RetryWaitPeriod = \"1s\" [SecretStore.Authentication] AuthType = 'X-Vault-Token' # Exclusive Secret Store [SecretStoreExclusive] Host = 'localhost' Port = 8200 Path = '/v1/secret/edgex/<app service key>/' Protocol = 'https' ServerName = 'edgex-vault' TokenFile = '/tmp/edgex/secrets/<app service key>/secrets-token.json' # Number of attempts to retry retrieving secrets before failing to start the service. AdditionalRetryAttempts = 10 # Amount of time to wait before attempting another retry RetryWaitPeriod = \"1s\" [SecretStoreExclusive.Authentication] AuthType = 'X-Vault-Token'","title":"Configuration"},{"location":"microservices/application/AdvancedTopics/#storing-secrets","text":"","title":"Storing Secrets"},{"location":"microservices/application/AdvancedTopics/#secure-mode","text":"When running an application service in secure mode, secrets can be stored in the secret store (Vault) by making an HTTP POST call to the secrets API route in the application service, http://[host]:[port]/api/v1/secrets . The secrets are stored and retrieved from the secret store based on values in the SecretStoreExclusive section of the configuration file. Once a secret is stored, only the service that added the secret will be able to retrieve it. For secret retrieval see Getting Secrets . An example of the JSON message body is below. { \"path\" : \"MyPath\" , \"secrets\" : [ { \"key\" : \"MySecretKey\" , \"value\" : \"MySecretValue\" } ] } Note Path specifies the type or location of the secrets to store. It is appended to the base path from the SecretStoreExclusive configuration. An empty path is a valid configuration for a secret's location.","title":"Secure Mode"},{"location":"microservices/application/AdvancedTopics/#insecure-mode","text":"When running in insecure mode, the secrets are stored and retrieved from the Writable.InsecureSecrets section of the service's configuration toml file. Insecure secrets and their paths can be configured as below. [ Writable . InsecureSecrets ] [Writable.InsecureSecrets.AWS] Path = 'aws' [Writable.InsecureSecrets.AWS.Secrets] username = 'aws-user' password = 'aws-pw' [Writable.InsecureSecrets.DB] Path = 'redisdb' [Writable.InsecureSecrets.DB.Secrets] username = '' password = '' Note An empty path is a valid configuration for a secret's location","title":"Insecure Mode"},{"location":"microservices/application/AdvancedTopics/#getting-secrets","text":"Application Services can retrieve their secrets from the underlying secret store using the GetSecrets() API in the SDK. If in secure mode, the secrets are retrieved from the secret store based on the SecretStoreExclusive configuration values. If running in insecure mode, the secrets are retrieved from the Writable.InsecureSecrets configuration.","title":"Getting Secrets"},{"location":"microservices/application/AppServiceConfigurable/","text":"App Service Configurable Getting Started App-Service-Configurable is provided as an easy way to get started with processing data flowing through EdgeX. This service leverages the App Functions SDK and provides a way for developers to use configuration instead of having to compile standalone services to utilize built in functions in the SDK. Please refer to Available Configurable Pipeline Functions section below for full list of built in functions that can be used in the configurable pipeline. To get started with the App-Service-Configurable, you'll want to start by determining which functions are required in your pipeline. Using a simple example, let's assume you wish to use the following functions from the SDK: FilterByDeviceName - to filter events for a specific device. TransformToXML - to transform the data to XML HTTPPost - to send the data to an HTTP endpoint that takes our XML data MarkAsPushed - to call Core Data API to mark the event as having been pushed Once the functions have been identified, we'll go ahead and build out the configuration in the configuration.toml file under the [Writable.Pipeline] section: [Writable] LogLevel = 'DEBUG' [Writable.Pipeline] ExecutionOrder = \"FilterByDeviceName, TransformToXML, HTTPPost, MarkAsPushed\" [Writable.Pipeline.Functions.FilterByDeviceName] [Writable.Pipeline.Functions.FilterByDeviceName.Parameters] FilterValues = \"Random-Float-Device, Random-Integer-Device\" [Writable.Pipeline.Functions.TransformToXML] [Writable.Pipeline.Functions.MarkAsPushed] [Writable.Pipeline.Functions.HTTPPost] [Writable.Pipeline.Functions.HTTPPost.Parameters] url = \"http://my.api.net/edgexdata\" mimeType = \"\" #OPTIONAL - default application/json The first line of note is ExecutionOrder = \"FilterByDeviceName, TransformToXML, HTTPPost,MarkAsPushed\" . This specifies the order in which to execute your functions. Each function specified here must also be placed in the [Writeable.Pipeline.Functions] section. Next, each function and its required information is listed. Each function typically has associated Parameters that must be configured to properly execute the function as designated by [Writable.Pipeline.Functions.{FunctionName}.Parameters] . Knowing which parameters are required for each function, can be referenced by taking a look at the Available Configurable Pipeline Functions section below. In a few cases, such as TransformToXML , TransformToJSON , SetOutputData`, etc. there are no parameters required. Note By default, the configuration provided is set to use MessageBus as a trigger from CoreData. This means you must have EdgeX Running with devices sending data in order to trigger the pipeline. You can also change the trigger to be HTTP. For more on triggers, view the Triggers documentation located in the Triggers section. That's it! Now we can run/deploy this service and the functions pipeline will process the data with functions we've defined. Environment Variable Overrides For Docker EdgeX services no longer have docker specific profiles. They now rely on environment variable overrides in the docker compose files for the docker specific differences. The following environment settings are required in the compose files when using App Service Configurable. EDGEX_PROFILE : [ target profile ] SERVICE_HOST : [ service name ] SERVICE_PORT : [ service port ] MESSAGEBUS_SUBSCRIBEHOST_HOST : edgex-core-data CLIENTS_COREDATA_HOST : edgex-core-data The following is an example docker compose entry for App Service Configurable : app-service-configurable-rules : image : edgexfoundry/docker-app-service-configurable:1.1.0 environment : EDGEX_PROFILE : rules-engine SERVICE_HOST : edgex-app-service-configurable-rules SERVICE_PORT : 48096 MESSAGEBUS_SUBSCRIBEHOST_HOST : edgex-core-data CLIENTS_COREDATA_HOST : edgex-core-data ports : - \"48096:48096\" container_name : edgex-app-service-configurable-rules hostname : edgex-app-service-configurable-rules networks : - edgex-network depends_on : - data Note App Service Configurable is designed to be run multiple times each with different profiles. This is why in the above example the name edgex-app-service-configurable-rules is used for the instance running the rules-engine profile. Deploying Multiple Instances using profiles App Service Configurable was designed to be deployed as multiple instances with different purposes. Since the function pipeline is specified in the configuration.toml file, we can use this as a way to run each instance with a different function pipeline. App Service Configurable does not have the standard default configuration at /res/configuration.toml . This default configuration has been moved to the sample profile. This forces you to specify the profile for the configuration you would like to run. The profile is specified using the -p/--profile=[profilename] command line option or the EDGEX_PROFILE=[profilename] environment variable override. The profile name selected is used in the service key ( AppService-[profile name] ) to make each instance unique, e.g. AppService-sample when specifying sample as the profile. Note If you need to run multiple instances with the same profile, e.g. http-export , but configured differently, you will need to override the service key with a custom name for one or more of the services. This is done with the -sk/-serviceKey command-line option or the EDGEX_SERVICE_KEY environment variable. See the Command-line Options and Environment Overrides sections for more detail. The following profiles and their purposes are provided with App Service Configurable. blackbox-tests - Profile used for black box testing http-export - Starter profile used for exporting data via HTTP. Requires further configuration which can easily be accomplished using environment variable overrides Required: WRITABLE_PIPELINE_FUNCTIONS_HTTPPOSTJSON_PARAMETERS_URL: [Your URL] Optional: environment : - WRITABLE_PIPELINE_FUNCTIONS_HTTPPOSTJSON_PARAMETERS_PERSISTONERROR : [ \"true\" /\"false\" ] - WRITABLE_PIPELINE_FUNCTIONS_FILTERBYDEVICENAME_PARAMETERS_DEVICENAMES : \"[comma separated list]\" - WRITABLE_PIPELINE_FUNCTIONS_FILTERBYVALUEDESCRIPTOR_PARAMETERS_VALUEDESCRIPTORS : \"[comma separated list]\" - WRITABLE_PIPELINE_FUNCTIONS_FILTERBYVALUEDESCRIPTOR_PARAMETERS_FILTEROUT : [ \"true\" /\"false\" ] mqtt-export - Starter profile used for exporting data via MQTT. Requires further configuration which can easily be accomplished using environment variable overrides Required: WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_ADDRESS: [Your Address] Optional: environment : - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_PORT : [ \"your port\" ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_PROTOCOL : [ tcp or tcps ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_PUBLISHER : [ your name ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_USER : [ your username ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_PASSWORD : [ your password ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_TOPIC : [ your topic ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_PARAMETERS_QOS : [ \"your quality or service\" ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_PARAMETERS_KEY : [ your Key ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_PARAMETERS_CERT : [ your Certificate ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_PARAMETERS_AUTORECONNECT : [ \"true\" or \"false\" ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_PARAMETERS_RETAIN : [ \"true\" or \"false\" ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_PARAMETERS_PERSISTONERROR : [ \"true\" or \"false\" ] rules-engine - Profile used to push Event messages to the Rules Engine via ZMQ Message Bus. rules-engine-mqtt - Profile used to push Event messages to the Rules Engine via MQTT Message Bus. rules-engine-redis Profile used to push Event messages to the Rules Engine via RedisStreams Message Bus. sample - Sample profile with all available functions declared and a sample pipeline. Provided as a sample that can be copied and modified to create new custom profiles. Note Functions can be declared in a profile but not used in the pipeline ExecutionOrder allowing them to be added to the pipeline ExecutionOrder later at runtime if needed. What if my input data isn't an EdgeX Event ? The default TargetType for data flowing into the functions pipeline is an EdgeX event. There are cases when this incoming data might not be an EdgeX event. In these cases the Pipeline can be configured using UseTargetTypeOfByteArray=true to set the TargetType to be a byte array, i.e. byte[] . The first function in the pipeline must then be one that can handle the byte[] data. The compression , encryption and export functions are examples of pipeline functions that will take input data that is byte[] . Here is an example of how to configure the functions pipeline to compress , encrypt and then export the byte[] data via HTTP. [Writable] LogLevel = 'DEBUG' [Writable.Pipeline] UseTargetTypeOfByteArray = true ExecutionOrder = \"CompressWithGZIP, EncryptWithAES, HTTPPost\" [Writable.Pipeline.Functions.CompressWithGZIP] [Writable.Pipeline.Functions.EncryptWithAES] [Writable.Pipeline.Functions.EncryptWithAES.Parameters] Key = \"aquqweoruqwpeoruqwpoeruqwpoierupqoweiurpoqwiuerpqowieurqpowieurpoqiweuroipwqure\" InitVector = \"123456789012345678901234567890\" [Writable.Pipeline.Functions.HTTPPost] [Writable.Pipeline.Functions.HTTPPost.Parameters] url = \"http://my.api.net/edgexdata\" If along with this pipeline configuration, you also configured the Binding to be http trigger, you could then send any data to the app-service-configurable' s /api/v1/trigger endpoint and have it compressed, encrypted and sent to your configured URL above. [Binding] Type = \"http\" Available Configurable Pipeline Functions Below are the functions that are available to use in the configurable functions pipeline ( [Writable.Pipeline] ) section of the configuration. The function names below can be added to the Writable.Pipeline.ExecutionOrder setting (comma separated list) and must also be present or added to the [Writable.Pipeline.Functions] section as [Writable.Pipeline.Functions.{FunctionName}] . Certain functions will also have the [Writable.Pipeline.Functions.{FunctionName}.Parameters] section where the function's parameters are configured. Please refer to the Getting Started section above for an example. Note The Parameters section for each function is a key/value map of string values. So even tough the parameter is referred to as an Integer or Boolean, it has to be specified as a string, e.g. \"20\" or \"true\". Please refer to the function's detailed documentation by clicking the function name below. BatchByCount Parameters BatchThreshold - Number of items to batch before sending batched items to the next function in the pipeline. Example [Writable.Pipeline.Functions.BatchByCount] [Writable.Pipeline.Functions.BatchByCount.Parameters] BatchThreshold = \"30\" BatchByTime Parameters TimeInterval - Time duration to batch before sending batched items to the next function in the pipeline. Example [Writable.Pipeline.Functions.BatchByTime] [Writable.Pipeline.Functions.BatchByTime.Parameters] TimeInterval = \"60s\" BatchByTimeAndCount Parameters BatchThreshold - The number of items to batch before sending batched items to the next function in the pipeline. TimeInterval - Time duration to batch before sending batched items to the next function in the pipeline. Example [Writable.Pipeline.Functions.BatchByTimeAndCount] [Writable.Pipeline.Functions.BatchByTimeAndCount.Parameters] BatchThreshold = \"30\" TimeInterval = \"60s\" CompressWithGZIP Parameters none Example [Writable.Pipeline.Functions.CompressWithGZIP] CompressWithZLIB Parameters none Example [Writable.Pipeline.Functions.CompressWithZLIB] EncryptWithAES Parameters Key - Encryption key used for the AES encryption. InitVector - Initialization vector used for the AES encryption. Example [Writable.Pipeline.Functions.EncryptWithAES] [Writable.Pipeline.Functions.EncryptWithAES.Parameters] Key = \"aquqweoruqwpeoruqwpoeruqwpoierupqoweiurpoqwiuerpqowieurqpowieurpoqiweuroipwqure\" InitVector = \"123456789012345678901234567890\" FilterByDeviceName Parameters DeviceNames - Comma separated list of device names for filtering FilterOut - Boolean indicating if the data matching the device names should be filtered out or filtered for. Example [Writable.Pipeline.Functions.FilterByDeviceName] [Writable.Pipeline.Functions.FilterByDeviceName.Parameters] DeviceNames = \"Random-Float-Device,Random-Integer-Device\" FilterOut = \"false\" FilterByValueDescriptor Parameters ValueDescriptors - Comma separated list of value descriptor (reading) names for filtering FilterOut - Boolean indicating if the data matching the value descriptor (reading) names should be filtered out or filtered for. Example [Writable.Pipeline.Functions.FilterByValueDescriptor] [Writable.Pipeline.Functions.FilterByValueDescriptor.Parameters] ValueDescriptors = \"RandomValue_Int8, RandomValue_Int64\" FilterOut = \"true\" HTTPPost Parameters Url - HTTP endpoint to POST the data. MimeType - Optional mime type for the data. Defaults to application/json. PersistOnError - Indicates to persist the data if the POST fails. Store and Forward must also be enabled if this is set to \"true\". SecretHeaderName - Optional HTTP header name used when POSTing with authorization token. If specified, the Secret Store (Vault or InsecureSecrets ) must contain the <name> secret at the specified SecretPath . SecretPath - Optional path in the secret store where to token is stored. Example [Writable.Pipeline.Functions.HTTPPost] [Writable.Pipeline.Functions.HTTPPost.Parameters] Url = \"http://my.api.net/edgexdata\" MimeType = \"\" #OPTIONAL - default application/json PersistOnError = \"false\" SecretHeaderName = \"\" # This is the name used in the HTTP header and also used as the secret key SecretPath = \"\" HTTPPostJSON Parameters Url - HTTP endpoint to POST the data. PersistOnError - Indicates to persist the data if the POST fails. Store and Forward must also be enabled if this is set to \"true\". SecretHeaderName - Optional HTTP header name used when POSTing with authorization token. If specified, the Secret Store (Vault or InsecureSecrets ) must contain the <name> secret at the specified SecretPath . SecretPath - Optional path in the secret store where to token is stored. Example [Writable.Pipeline.Functions.HTTPPostJSON] [Writable.Pipeline.Functions.HTTPPostJSON.Parameters] Url = \"https://my.api.net/edgexdata\" PersistOnError = \"true\" SecretHeaderName = \"Authorization\" # This is the name used in the HTTP header and also used as the secret key SecretPath = \"http\" HTTPPostXML Parameters Url - HTTP endpoint to POST the data. PersistOnError - Indicates to persist the data if the POST fails. Store and Forward must also be enabled if this is set to \"true\". SecretHeaderName - Optional HTTP header name used when POSTing with authorization token. If specified, the Secret Store (Vault or InsecureSecrets ) must contain the <name> secret at the specified SecretPath . SecretPath - Optional path in the secret store where to token is stored. Example [Writable.Pipeline.Functions.HTTPPostXML] [Writable.Pipeline.Functions.HTTPPostXML.Parameters] Url = \"http://my.api.net/edgexdata\" PersistOnError = \"false\" SecretHeaderName = \"\" # This is the name used in the HTTP header and also used as the secret key SecretPath = \"\" JSONLogic Parameters Rule - The JSON formatted rule that with be executed on the data by JSONLogic Example [Writable.Pipeline.Functions.JSONLogic] [Writable.Pipeline.Functions.JSONLogic.Parameters] Rule = \"{ \\\"and\\\" : [{\\\"<\\\" : [{ \\\"var\\\" : \\\"temp\\\" }, 110 ]}, {\\\"==\\\" : [{ \\\"var\\\" : \\\"sensor.type\\\" }, \\\"temperature\\\" ]} ] }\" MarkAsPushed Parameters none Example [Writable.Pipeline.Functions.MarkAsPushed] MQTTSecretSend Parameters BrokerAddress - URL specify the address of the MQTT Broker Topic - Topic to publish the data ClientId - Id to use when connection to the MQTT Broker Qos - MQTT Quality of Service setting to use (0, 1 or 2). Please refer here for more details on QOS values AutoReconnect - Boolean specifying if reconnect should be automatic if connection to MQTT broker is lost Retain - Boolean specifying if the MQTT Broker should save the last message published as the \u201cLast Good Message\u201d on that topic. SkipVerify - Boolean indicating if the certificate verification should be skipped. PersistOnError - Indicates to persist the data if the POST fails. Store and Forward must also be enabled if this is set to \"true\". AuthMode - Mode of authentication to use when connecting to the MQTT Broker none - No authentication required usernamepassword - Use username and password authentication. The Secret Store (Vault or InsecureSecrets ) must contain the username and password secrets. clientcert - Use Client Certificate authentication. The Secret Store (Vault or InsecureSecrets ) must contain the clientkey and clientcert secrets. cacert - Use CA Certificate authentication. The Secret Store (Vault or InsecureSecrets ) must contain the cacert secret. SecretPath - Path in the secret store where to authorization secrets are stored. Examples [Writable.Pipeline.Functions.MQTTSecretSend] [Writable.Pipeline.Functions.MQTTSecretSend.Parameters] BrokerAddress = \"tcps://localhost:8883\" Topic = \"mytopic\" ClientId = \"myclientid\" Qos = \"0\" AutoReconnect = \"false\" Retain = \"false\" SkipVerify = \"false\" PersistOnError = \"false\" AuthMode = \"\" SecretPath = \"\" [Writable.Pipeline.Functions.MQTTSecretSend] [Writable.Pipeline.Functions.MQTTSecretSend.Parameters] BrokerAddress = \"tcps://my-broker-host.com:8883\" Topic = \"mytopic\" ClientId = \"myclientid\" Qos = \"2\" AutoReconnect = \"true\" Retain = \"true\" SkipVerify = \"false\" PersistOnError = \"true\" AuthMode = \"usernamepassword\" SecretPath = \"mqtt\" MQTTSend MQTTSend has been deprecated. Please use MQTTSecretSend . PushToCore Parameters none Example [Writable.Pipeline.Functions.PushToCore] SetOutputData Parameters none Example [Writable.Pipeline.Functions.SetOutputData] TransformToJSON Parameters none Example [Writable.Pipeline.Functions.TransformToJSON] TransformToXML Parameters none Example [Writable.Pipeline.Functions.TransformToXML]","title":"App Service Configurable"},{"location":"microservices/application/AppServiceConfigurable/#app-service-configurable","text":"","title":"App Service Configurable"},{"location":"microservices/application/AppServiceConfigurable/#getting-started","text":"App-Service-Configurable is provided as an easy way to get started with processing data flowing through EdgeX. This service leverages the App Functions SDK and provides a way for developers to use configuration instead of having to compile standalone services to utilize built in functions in the SDK. Please refer to Available Configurable Pipeline Functions section below for full list of built in functions that can be used in the configurable pipeline. To get started with the App-Service-Configurable, you'll want to start by determining which functions are required in your pipeline. Using a simple example, let's assume you wish to use the following functions from the SDK: FilterByDeviceName - to filter events for a specific device. TransformToXML - to transform the data to XML HTTPPost - to send the data to an HTTP endpoint that takes our XML data MarkAsPushed - to call Core Data API to mark the event as having been pushed Once the functions have been identified, we'll go ahead and build out the configuration in the configuration.toml file under the [Writable.Pipeline] section: [Writable] LogLevel = 'DEBUG' [Writable.Pipeline] ExecutionOrder = \"FilterByDeviceName, TransformToXML, HTTPPost, MarkAsPushed\" [Writable.Pipeline.Functions.FilterByDeviceName] [Writable.Pipeline.Functions.FilterByDeviceName.Parameters] FilterValues = \"Random-Float-Device, Random-Integer-Device\" [Writable.Pipeline.Functions.TransformToXML] [Writable.Pipeline.Functions.MarkAsPushed] [Writable.Pipeline.Functions.HTTPPost] [Writable.Pipeline.Functions.HTTPPost.Parameters] url = \"http://my.api.net/edgexdata\" mimeType = \"\" #OPTIONAL - default application/json The first line of note is ExecutionOrder = \"FilterByDeviceName, TransformToXML, HTTPPost,MarkAsPushed\" . This specifies the order in which to execute your functions. Each function specified here must also be placed in the [Writeable.Pipeline.Functions] section. Next, each function and its required information is listed. Each function typically has associated Parameters that must be configured to properly execute the function as designated by [Writable.Pipeline.Functions.{FunctionName}.Parameters] . Knowing which parameters are required for each function, can be referenced by taking a look at the Available Configurable Pipeline Functions section below. In a few cases, such as TransformToXML , TransformToJSON , SetOutputData`, etc. there are no parameters required. Note By default, the configuration provided is set to use MessageBus as a trigger from CoreData. This means you must have EdgeX Running with devices sending data in order to trigger the pipeline. You can also change the trigger to be HTTP. For more on triggers, view the Triggers documentation located in the Triggers section. That's it! Now we can run/deploy this service and the functions pipeline will process the data with functions we've defined.","title":"Getting Started"},{"location":"microservices/application/AppServiceConfigurable/#environment-variable-overrides-for-docker","text":"EdgeX services no longer have docker specific profiles. They now rely on environment variable overrides in the docker compose files for the docker specific differences. The following environment settings are required in the compose files when using App Service Configurable. EDGEX_PROFILE : [ target profile ] SERVICE_HOST : [ service name ] SERVICE_PORT : [ service port ] MESSAGEBUS_SUBSCRIBEHOST_HOST : edgex-core-data CLIENTS_COREDATA_HOST : edgex-core-data The following is an example docker compose entry for App Service Configurable : app-service-configurable-rules : image : edgexfoundry/docker-app-service-configurable:1.1.0 environment : EDGEX_PROFILE : rules-engine SERVICE_HOST : edgex-app-service-configurable-rules SERVICE_PORT : 48096 MESSAGEBUS_SUBSCRIBEHOST_HOST : edgex-core-data CLIENTS_COREDATA_HOST : edgex-core-data ports : - \"48096:48096\" container_name : edgex-app-service-configurable-rules hostname : edgex-app-service-configurable-rules networks : - edgex-network depends_on : - data Note App Service Configurable is designed to be run multiple times each with different profiles. This is why in the above example the name edgex-app-service-configurable-rules is used for the instance running the rules-engine profile.","title":"Environment Variable Overrides For Docker"},{"location":"microservices/application/AppServiceConfigurable/#deploying-multiple-instances-using-profiles","text":"App Service Configurable was designed to be deployed as multiple instances with different purposes. Since the function pipeline is specified in the configuration.toml file, we can use this as a way to run each instance with a different function pipeline. App Service Configurable does not have the standard default configuration at /res/configuration.toml . This default configuration has been moved to the sample profile. This forces you to specify the profile for the configuration you would like to run. The profile is specified using the -p/--profile=[profilename] command line option or the EDGEX_PROFILE=[profilename] environment variable override. The profile name selected is used in the service key ( AppService-[profile name] ) to make each instance unique, e.g. AppService-sample when specifying sample as the profile. Note If you need to run multiple instances with the same profile, e.g. http-export , but configured differently, you will need to override the service key with a custom name for one or more of the services. This is done with the -sk/-serviceKey command-line option or the EDGEX_SERVICE_KEY environment variable. See the Command-line Options and Environment Overrides sections for more detail. The following profiles and their purposes are provided with App Service Configurable. blackbox-tests - Profile used for black box testing http-export - Starter profile used for exporting data via HTTP. Requires further configuration which can easily be accomplished using environment variable overrides Required: WRITABLE_PIPELINE_FUNCTIONS_HTTPPOSTJSON_PARAMETERS_URL: [Your URL] Optional: environment : - WRITABLE_PIPELINE_FUNCTIONS_HTTPPOSTJSON_PARAMETERS_PERSISTONERROR : [ \"true\" /\"false\" ] - WRITABLE_PIPELINE_FUNCTIONS_FILTERBYDEVICENAME_PARAMETERS_DEVICENAMES : \"[comma separated list]\" - WRITABLE_PIPELINE_FUNCTIONS_FILTERBYVALUEDESCRIPTOR_PARAMETERS_VALUEDESCRIPTORS : \"[comma separated list]\" - WRITABLE_PIPELINE_FUNCTIONS_FILTERBYVALUEDESCRIPTOR_PARAMETERS_FILTEROUT : [ \"true\" /\"false\" ] mqtt-export - Starter profile used for exporting data via MQTT. Requires further configuration which can easily be accomplished using environment variable overrides Required: WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_ADDRESS: [Your Address] Optional: environment : - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_PORT : [ \"your port\" ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_PROTOCOL : [ tcp or tcps ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_PUBLISHER : [ your name ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_USER : [ your username ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_PASSWORD : [ your password ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_ADDRESSABLE_TOPIC : [ your topic ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_PARAMETERS_QOS : [ \"your quality or service\" ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_PARAMETERS_KEY : [ your Key ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_PARAMETERS_CERT : [ your Certificate ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_PARAMETERS_AUTORECONNECT : [ \"true\" or \"false\" ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_PARAMETERS_RETAIN : [ \"true\" or \"false\" ] - WRITABLE_PIPELINE_FUNCTIONS_MQTTSEND_PARAMETERS_PERSISTONERROR : [ \"true\" or \"false\" ] rules-engine - Profile used to push Event messages to the Rules Engine via ZMQ Message Bus. rules-engine-mqtt - Profile used to push Event messages to the Rules Engine via MQTT Message Bus. rules-engine-redis Profile used to push Event messages to the Rules Engine via RedisStreams Message Bus. sample - Sample profile with all available functions declared and a sample pipeline. Provided as a sample that can be copied and modified to create new custom profiles. Note Functions can be declared in a profile but not used in the pipeline ExecutionOrder allowing them to be added to the pipeline ExecutionOrder later at runtime if needed.","title":"Deploying Multiple Instances using profiles"},{"location":"microservices/application/AppServiceConfigurable/#what-if-my-input-data-isnt-an-edgex-event","text":"The default TargetType for data flowing into the functions pipeline is an EdgeX event. There are cases when this incoming data might not be an EdgeX event. In these cases the Pipeline can be configured using UseTargetTypeOfByteArray=true to set the TargetType to be a byte array, i.e. byte[] . The first function in the pipeline must then be one that can handle the byte[] data. The compression , encryption and export functions are examples of pipeline functions that will take input data that is byte[] . Here is an example of how to configure the functions pipeline to compress , encrypt and then export the byte[] data via HTTP. [Writable] LogLevel = 'DEBUG' [Writable.Pipeline] UseTargetTypeOfByteArray = true ExecutionOrder = \"CompressWithGZIP, EncryptWithAES, HTTPPost\" [Writable.Pipeline.Functions.CompressWithGZIP] [Writable.Pipeline.Functions.EncryptWithAES] [Writable.Pipeline.Functions.EncryptWithAES.Parameters] Key = \"aquqweoruqwpeoruqwpoeruqwpoierupqoweiurpoqwiuerpqowieurqpowieurpoqiweuroipwqure\" InitVector = \"123456789012345678901234567890\" [Writable.Pipeline.Functions.HTTPPost] [Writable.Pipeline.Functions.HTTPPost.Parameters] url = \"http://my.api.net/edgexdata\" If along with this pipeline configuration, you also configured the Binding to be http trigger, you could then send any data to the app-service-configurable' s /api/v1/trigger endpoint and have it compressed, encrypted and sent to your configured URL above. [Binding] Type = \"http\"","title":"What if my input data isn't an EdgeX Event ?"},{"location":"microservices/application/AppServiceConfigurable/#available-configurable-pipeline-functions","text":"Below are the functions that are available to use in the configurable functions pipeline ( [Writable.Pipeline] ) section of the configuration. The function names below can be added to the Writable.Pipeline.ExecutionOrder setting (comma separated list) and must also be present or added to the [Writable.Pipeline.Functions] section as [Writable.Pipeline.Functions.{FunctionName}] . Certain functions will also have the [Writable.Pipeline.Functions.{FunctionName}.Parameters] section where the function's parameters are configured. Please refer to the Getting Started section above for an example. Note The Parameters section for each function is a key/value map of string values. So even tough the parameter is referred to as an Integer or Boolean, it has to be specified as a string, e.g. \"20\" or \"true\". Please refer to the function's detailed documentation by clicking the function name below.","title":"Available Configurable Pipeline Functions"},{"location":"microservices/application/AppServiceConfigurable/#batchbycount","text":"Parameters BatchThreshold - Number of items to batch before sending batched items to the next function in the pipeline. Example [Writable.Pipeline.Functions.BatchByCount] [Writable.Pipeline.Functions.BatchByCount.Parameters] BatchThreshold = \"30\"","title":"BatchByCount"},{"location":"microservices/application/AppServiceConfigurable/#batchbytime","text":"Parameters TimeInterval - Time duration to batch before sending batched items to the next function in the pipeline. Example [Writable.Pipeline.Functions.BatchByTime] [Writable.Pipeline.Functions.BatchByTime.Parameters] TimeInterval = \"60s\"","title":"BatchByTime"},{"location":"microservices/application/AppServiceConfigurable/#batchbytimeandcount","text":"Parameters BatchThreshold - The number of items to batch before sending batched items to the next function in the pipeline. TimeInterval - Time duration to batch before sending batched items to the next function in the pipeline. Example [Writable.Pipeline.Functions.BatchByTimeAndCount] [Writable.Pipeline.Functions.BatchByTimeAndCount.Parameters] BatchThreshold = \"30\" TimeInterval = \"60s\"","title":"BatchByTimeAndCount"},{"location":"microservices/application/AppServiceConfigurable/#compresswithgzip","text":"Parameters none Example [Writable.Pipeline.Functions.CompressWithGZIP]","title":"CompressWithGZIP"},{"location":"microservices/application/AppServiceConfigurable/#compresswithzlib","text":"Parameters none Example [Writable.Pipeline.Functions.CompressWithZLIB]","title":"CompressWithZLIB"},{"location":"microservices/application/AppServiceConfigurable/#encryptwithaes","text":"Parameters Key - Encryption key used for the AES encryption. InitVector - Initialization vector used for the AES encryption. Example [Writable.Pipeline.Functions.EncryptWithAES] [Writable.Pipeline.Functions.EncryptWithAES.Parameters] Key = \"aquqweoruqwpeoruqwpoeruqwpoierupqoweiurpoqwiuerpqowieurqpowieurpoqiweuroipwqure\" InitVector = \"123456789012345678901234567890\"","title":"EncryptWithAES"},{"location":"microservices/application/AppServiceConfigurable/#filterbydevicename","text":"Parameters DeviceNames - Comma separated list of device names for filtering FilterOut - Boolean indicating if the data matching the device names should be filtered out or filtered for. Example [Writable.Pipeline.Functions.FilterByDeviceName] [Writable.Pipeline.Functions.FilterByDeviceName.Parameters] DeviceNames = \"Random-Float-Device,Random-Integer-Device\" FilterOut = \"false\"","title":"FilterByDeviceName"},{"location":"microservices/application/AppServiceConfigurable/#filterbyvaluedescriptor","text":"Parameters ValueDescriptors - Comma separated list of value descriptor (reading) names for filtering FilterOut - Boolean indicating if the data matching the value descriptor (reading) names should be filtered out or filtered for. Example [Writable.Pipeline.Functions.FilterByValueDescriptor] [Writable.Pipeline.Functions.FilterByValueDescriptor.Parameters] ValueDescriptors = \"RandomValue_Int8, RandomValue_Int64\" FilterOut = \"true\"","title":"FilterByValueDescriptor"},{"location":"microservices/application/AppServiceConfigurable/#httppost","text":"Parameters Url - HTTP endpoint to POST the data. MimeType - Optional mime type for the data. Defaults to application/json. PersistOnError - Indicates to persist the data if the POST fails. Store and Forward must also be enabled if this is set to \"true\". SecretHeaderName - Optional HTTP header name used when POSTing with authorization token. If specified, the Secret Store (Vault or InsecureSecrets ) must contain the <name> secret at the specified SecretPath . SecretPath - Optional path in the secret store where to token is stored. Example [Writable.Pipeline.Functions.HTTPPost] [Writable.Pipeline.Functions.HTTPPost.Parameters] Url = \"http://my.api.net/edgexdata\" MimeType = \"\" #OPTIONAL - default application/json PersistOnError = \"false\" SecretHeaderName = \"\" # This is the name used in the HTTP header and also used as the secret key SecretPath = \"\"","title":"HTTPPost"},{"location":"microservices/application/AppServiceConfigurable/#httppostjson","text":"Parameters Url - HTTP endpoint to POST the data. PersistOnError - Indicates to persist the data if the POST fails. Store and Forward must also be enabled if this is set to \"true\". SecretHeaderName - Optional HTTP header name used when POSTing with authorization token. If specified, the Secret Store (Vault or InsecureSecrets ) must contain the <name> secret at the specified SecretPath . SecretPath - Optional path in the secret store where to token is stored. Example [Writable.Pipeline.Functions.HTTPPostJSON] [Writable.Pipeline.Functions.HTTPPostJSON.Parameters] Url = \"https://my.api.net/edgexdata\" PersistOnError = \"true\" SecretHeaderName = \"Authorization\" # This is the name used in the HTTP header and also used as the secret key SecretPath = \"http\"","title":"HTTPPostJSON"},{"location":"microservices/application/AppServiceConfigurable/#httppostxml","text":"Parameters Url - HTTP endpoint to POST the data. PersistOnError - Indicates to persist the data if the POST fails. Store and Forward must also be enabled if this is set to \"true\". SecretHeaderName - Optional HTTP header name used when POSTing with authorization token. If specified, the Secret Store (Vault or InsecureSecrets ) must contain the <name> secret at the specified SecretPath . SecretPath - Optional path in the secret store where to token is stored. Example [Writable.Pipeline.Functions.HTTPPostXML] [Writable.Pipeline.Functions.HTTPPostXML.Parameters] Url = \"http://my.api.net/edgexdata\" PersistOnError = \"false\" SecretHeaderName = \"\" # This is the name used in the HTTP header and also used as the secret key SecretPath = \"\"","title":"HTTPPostXML"},{"location":"microservices/application/AppServiceConfigurable/#jsonlogic","text":"Parameters Rule - The JSON formatted rule that with be executed on the data by JSONLogic Example [Writable.Pipeline.Functions.JSONLogic] [Writable.Pipeline.Functions.JSONLogic.Parameters] Rule = \"{ \\\"and\\\" : [{\\\"<\\\" : [{ \\\"var\\\" : \\\"temp\\\" }, 110 ]}, {\\\"==\\\" : [{ \\\"var\\\" : \\\"sensor.type\\\" }, \\\"temperature\\\" ]} ] }\"","title":"JSONLogic"},{"location":"microservices/application/AppServiceConfigurable/#markaspushed","text":"Parameters none Example [Writable.Pipeline.Functions.MarkAsPushed]","title":"MarkAsPushed"},{"location":"microservices/application/AppServiceConfigurable/#mqttsecretsend","text":"Parameters BrokerAddress - URL specify the address of the MQTT Broker Topic - Topic to publish the data ClientId - Id to use when connection to the MQTT Broker Qos - MQTT Quality of Service setting to use (0, 1 or 2). Please refer here for more details on QOS values AutoReconnect - Boolean specifying if reconnect should be automatic if connection to MQTT broker is lost Retain - Boolean specifying if the MQTT Broker should save the last message published as the \u201cLast Good Message\u201d on that topic. SkipVerify - Boolean indicating if the certificate verification should be skipped. PersistOnError - Indicates to persist the data if the POST fails. Store and Forward must also be enabled if this is set to \"true\". AuthMode - Mode of authentication to use when connecting to the MQTT Broker none - No authentication required usernamepassword - Use username and password authentication. The Secret Store (Vault or InsecureSecrets ) must contain the username and password secrets. clientcert - Use Client Certificate authentication. The Secret Store (Vault or InsecureSecrets ) must contain the clientkey and clientcert secrets. cacert - Use CA Certificate authentication. The Secret Store (Vault or InsecureSecrets ) must contain the cacert secret. SecretPath - Path in the secret store where to authorization secrets are stored. Examples [Writable.Pipeline.Functions.MQTTSecretSend] [Writable.Pipeline.Functions.MQTTSecretSend.Parameters] BrokerAddress = \"tcps://localhost:8883\" Topic = \"mytopic\" ClientId = \"myclientid\" Qos = \"0\" AutoReconnect = \"false\" Retain = \"false\" SkipVerify = \"false\" PersistOnError = \"false\" AuthMode = \"\" SecretPath = \"\" [Writable.Pipeline.Functions.MQTTSecretSend] [Writable.Pipeline.Functions.MQTTSecretSend.Parameters] BrokerAddress = \"tcps://my-broker-host.com:8883\" Topic = \"mytopic\" ClientId = \"myclientid\" Qos = \"2\" AutoReconnect = \"true\" Retain = \"true\" SkipVerify = \"false\" PersistOnError = \"true\" AuthMode = \"usernamepassword\" SecretPath = \"mqtt\"","title":"MQTTSecretSend"},{"location":"microservices/application/AppServiceConfigurable/#mqttsend","text":"MQTTSend has been deprecated. Please use MQTTSecretSend .","title":"MQTTSend"},{"location":"microservices/application/AppServiceConfigurable/#pushtocore","text":"Parameters none Example [Writable.Pipeline.Functions.PushToCore]","title":"PushToCore"},{"location":"microservices/application/AppServiceConfigurable/#setoutputdata","text":"Parameters none Example [Writable.Pipeline.Functions.SetOutputData]","title":"SetOutputData"},{"location":"microservices/application/AppServiceConfigurable/#transformtojson","text":"Parameters none Example [Writable.Pipeline.Functions.TransformToJSON]","title":"TransformToJSON"},{"location":"microservices/application/AppServiceConfigurable/#transformtoxml","text":"Parameters none Example [Writable.Pipeline.Functions.TransformToXML]","title":"TransformToXML"},{"location":"microservices/application/ApplicationFunctionsSDK/","text":"App Functions SDK Welcome the App Functions SDK for EdgeX. This SDK is meant to provide all the plumbing necessary for developers to get started in processing/transforming/exporting data out of EdgeX. If you're new to the SDK - checkout the Getting Started guide. If you're already familiar - checkout the various sections about the SDK: Section Description Built In Transforms Provides a list of the available transforms in the SDK Context API Provides a list of all available functions on the context that is available inside of an app function Error Handling Describes how to properly handle pipeline execution failures Advanced Topics Learn about other ways to leverage the SDK beyond basic use cases","title":"App Functions SDK"},{"location":"microservices/application/ApplicationFunctionsSDK/#app-functions-sdk","text":"Welcome the App Functions SDK for EdgeX. This SDK is meant to provide all the plumbing necessary for developers to get started in processing/transforming/exporting data out of EdgeX. If you're new to the SDK - checkout the Getting Started guide. If you're already familiar - checkout the various sections about the SDK: Section Description Built In Transforms Provides a list of the available transforms in the SDK Context API Provides a list of all available functions on the context that is available inside of an app function Error Handling Describes how to properly handle pipeline execution failures Advanced Topics Learn about other ways to leverage the SDK beyond basic use cases","title":"App Functions SDK"},{"location":"microservices/application/ApplicationServices/","text":"Application Services Application Services are a means to get data from EdgeX Foundry to external systems and process (be it analytics package, enterprise or on-prem application, cloud systems like Azure IoT, AWS IoT, or Google IoT Core, etc.). Application Services provide the means for data to be prepared (transformed, enriched, filtered, etc.) and groomed (formatted, compressed, encrypted, etc.) before being sent to an endpoint of choice. Endpoints supported out of the box today include HTTP and MQTT endpoints, but will include additional offerings in the future and could include a custom endpoints. Note Application Services has replaced Export Services Application Services are based on the idea of a \"Functions Pipeline\". A functions pipeline is a collection of functions that process messages (in this case EdgeX event/reading messages) in the order that you've specified. The first function in a pipeline is a trigger. A trigger begins the functions pipeline execution. A trigger is something like a message landing in a watched message queue. An Applications Functions Software Development Kit (or App Functions SDK) is available to help create Application Services. Currently the only SDK supported language is Golang, with the intention that community developed and supported SDKs will come in the future for other languages. It is currently available as a Golang module to remain operating system (OS) agnostic and to comply with the latest EdgeX guidelines on dependency management. Any application built on top of the Application Functions SDK is considered an App Service. This SDK is provided to help build Application Services by assembling triggers, pre-existing functions and custom functions of your making into a pipeline. Application Service Improvements Providing an SDK that connects directly to a message bus by which Core Data events are published eliminates performance issues as well as allow the developers extra control on what happens with that data as soon as it is available. Furthermore, it emphasizes configuration over registration for consuming the data. The application services can be customized to a client's needs and thereby also removing the need for client registration. Standard Functions As mentioned, an Application Service is a function pipeline. The SDK provides some standard functions that can be used in a functions pipeline. In the future, additional functions will be provided \"standard\" or in other words provided with the SDK. Additionally, developers can implement their own custom functions and add those to their Application Service functions pipeline. One of the most common use cases for working with data that comes from Core Data is to filter data down to what is relevant for a given application and to format it. To help facilitate this, four primary functions ported over from the existing services today are included in the SDK. The first is the DeviceNameFilter function which will remove events that do not match the specified IDs and will cease execution of the pipeline if no event matches. The second is the ValueDescriptorFilter which exhibits the same behavior as DeviceNameFilter except filtering event readings on Value Descriptor instead of DeviceID. The third and fourth provided functions in the SDK transform the data received to either XML or JSON by calling XMLTransform or JSONTransform . Typically, after filtering and transforming the data as needed, exporting is the last step in a pipeline to ship the data where it needs to go. There are two primary functions included in the SDK to help facilitate this. The first is the HTTPPost function that will POST the provided data to a specified endpoint, and the second is an MQTTSecretSend() function that will publish the provided data to an MQTT Broker as specified in the configuration. There are two primary triggers that have been included in the SDK that initiate the start of the function pipeline. First is via a POST HTTP Endpoint /api/v1/trigger with the EdgeX event data as the body. Second is the MessageBus subscription with connection details as specified in the configuration. Finally, data may be sent back to the message bus or HTTP response by calling .complete() on the context. If the trigger is HTTP, then it will be an HTTP Response. If the trigger is MessageBus, then it will be published to the configured host and topic. Unsupported existing export service functions Valid Event Check --The first component in the pipe and filter, before the copier (described in the previous section) is a filter that can be optionally turned on or off by configuration. This filter is a general purpose data checking filter which assesses the device- or sensor-provided Event, with associated Readings, and ensures the data conforms to the ValueDescriptor associated with the Readings. For example, if the data from a sensor is described by its metadata profile as adhering to a \"Temperature\" value descriptor of floating number type, with the value between -100\u00b0 F and 200\u00b0 F, but the data seen in the Event and Readings is not a floating point number, for example if the data in the reading is a word such as \"cold,\" instead of a number, then the Event is rejected (no client receives the data) and no further processing is accomplished on the Event by the Export Distro service.","title":"Introduction"},{"location":"microservices/application/ApplicationServices/#application-services","text":"Application Services are a means to get data from EdgeX Foundry to external systems and process (be it analytics package, enterprise or on-prem application, cloud systems like Azure IoT, AWS IoT, or Google IoT Core, etc.). Application Services provide the means for data to be prepared (transformed, enriched, filtered, etc.) and groomed (formatted, compressed, encrypted, etc.) before being sent to an endpoint of choice. Endpoints supported out of the box today include HTTP and MQTT endpoints, but will include additional offerings in the future and could include a custom endpoints. Note Application Services has replaced Export Services Application Services are based on the idea of a \"Functions Pipeline\". A functions pipeline is a collection of functions that process messages (in this case EdgeX event/reading messages) in the order that you've specified. The first function in a pipeline is a trigger. A trigger begins the functions pipeline execution. A trigger is something like a message landing in a watched message queue. An Applications Functions Software Development Kit (or App Functions SDK) is available to help create Application Services. Currently the only SDK supported language is Golang, with the intention that community developed and supported SDKs will come in the future for other languages. It is currently available as a Golang module to remain operating system (OS) agnostic and to comply with the latest EdgeX guidelines on dependency management. Any application built on top of the Application Functions SDK is considered an App Service. This SDK is provided to help build Application Services by assembling triggers, pre-existing functions and custom functions of your making into a pipeline.","title":"Application Services"},{"location":"microservices/application/ApplicationServices/#application-service-improvements","text":"Providing an SDK that connects directly to a message bus by which Core Data events are published eliminates performance issues as well as allow the developers extra control on what happens with that data as soon as it is available. Furthermore, it emphasizes configuration over registration for consuming the data. The application services can be customized to a client's needs and thereby also removing the need for client registration.","title":"Application Service Improvements"},{"location":"microservices/application/ApplicationServices/#standard-functions","text":"As mentioned, an Application Service is a function pipeline. The SDK provides some standard functions that can be used in a functions pipeline. In the future, additional functions will be provided \"standard\" or in other words provided with the SDK. Additionally, developers can implement their own custom functions and add those to their Application Service functions pipeline. One of the most common use cases for working with data that comes from Core Data is to filter data down to what is relevant for a given application and to format it. To help facilitate this, four primary functions ported over from the existing services today are included in the SDK. The first is the DeviceNameFilter function which will remove events that do not match the specified IDs and will cease execution of the pipeline if no event matches. The second is the ValueDescriptorFilter which exhibits the same behavior as DeviceNameFilter except filtering event readings on Value Descriptor instead of DeviceID. The third and fourth provided functions in the SDK transform the data received to either XML or JSON by calling XMLTransform or JSONTransform . Typically, after filtering and transforming the data as needed, exporting is the last step in a pipeline to ship the data where it needs to go. There are two primary functions included in the SDK to help facilitate this. The first is the HTTPPost function that will POST the provided data to a specified endpoint, and the second is an MQTTSecretSend() function that will publish the provided data to an MQTT Broker as specified in the configuration. There are two primary triggers that have been included in the SDK that initiate the start of the function pipeline. First is via a POST HTTP Endpoint /api/v1/trigger with the EdgeX event data as the body. Second is the MessageBus subscription with connection details as specified in the configuration. Finally, data may be sent back to the message bus or HTTP response by calling .complete() on the context. If the trigger is HTTP, then it will be an HTTP Response. If the trigger is MessageBus, then it will be published to the configured host and topic.","title":"Standard Functions"},{"location":"microservices/application/ApplicationServices/#unsupported-existing-export-service-functions","text":"Valid Event Check --The first component in the pipe and filter, before the copier (described in the previous section) is a filter that can be optionally turned on or off by configuration. This filter is a general purpose data checking filter which assesses the device- or sensor-provided Event, with associated Readings, and ensures the data conforms to the ValueDescriptor associated with the Readings. For example, if the data from a sensor is described by its metadata profile as adhering to a \"Temperature\" value descriptor of floating number type, with the value between -100\u00b0 F and 200\u00b0 F, but the data seen in the Event and Readings is not a floating point number, for example if the data in the reading is a word such as \"cold,\" instead of a number, then the Event is rejected (no client receives the data) and no further processing is accomplished on the Event by the Export Distro service.","title":"Unsupported existing export service functions"},{"location":"microservices/application/BuiltIn/","text":"Built-In Transforms/Functions All transforms define a type and a New function which is used to initialize an instance of the type with the required parameters. These instances returned by these New functions give access to their appropriate pipeline function pointers when setting up the function pipeline. Example NewFilter ([] { \"Device1\" , \"Device2\" }). FilterByDeviceName Filtering There are two basic types of filtering included in the SDK to add to your pipeline. There is also an option to Filter Out specific items. These provided filter functions return a type of events.Model. If filtering results in no remaining data, the pipeline execution for that pass is terminated. If no values are provided for filtering, then data flows through unfiltered. Factory Method Description NewFilter([]string filterValues) This function returns a Filter instance initialized with the passed in filter values. This Filter instance is used to access the following filter functions that will operate using the specified filter values. type Filter struct { // Holds the values to be filtered FilterValues [] string // Determines if items in FilterValues should be filtered out. If set to true all items found in the filter will be removed. If set to false all items found in the filter will be returned. If FilterValues is empty then all items will be returned. FilterOut bool } By Device Name FilterByDeviceName - This function will filter the event data down to the specified device names and return the filtered data to the pipeline. NewFilter ([] { \"Device1\" , \"Device2\" }). FilterByDeviceName By Value Descriptor FilterByValueDescriptor - This function will filter the event data down to the specified device value descriptor and return the filtered data to the pipeline. NewFilter ([] { \"ValueDescriptor1\" , \"ValueDescriptor2\" }). FilterByValueDescriptor JSON Logic Factory Method Description NewJSONLogic(rule string) This function returns a JSONLogic instance initialized with the passed in JSON rule. The rule passed in should be a JSON string conforming to the specification here: http://jsonlogic.com/operations.html. Evaluate - This is the function that will be used in the pipeline to apply the JSON rule to data coming in on the pipeline. If the condition of your rule is met, then the pipeline will continue and the data will continue to flow to the next function in the pipeline. If the condition of your rule is NOT met, then pipeline execution stops. NewJSONLogic ( \"{ \\\"in\\\" : [{ \\\"var\\\" : \\\"device\\\" }, [\\\"Random-Integer-Device\\\",\\\"Random-Float-Device\\\"] ] }\" ). Evaluate Note Only operations that return true or false are supported. See http://jsonlogic.com/operations.html# for the complete list of operations paying attention to return values. Any operator that returns manipulated data is currently not supported. For more advanced scenarios checkout EMQ X Kuiper . Tip Leverage http://jsonlogic.com/play.html to get your rule right before implementing in code. JSON can be a bit tricky to get right in code with all the escaped double quotes. Encryption There is one encryption transform included in the SDK that can be added to your pipeline. Factory Method Description NewEncryption(key string, initializationVector string) This function returns a Encryption instance initialized with the passed in key and initialization vector. This Encryption instance is used to access the following encryption function that will use the specified key and initialization vector. AES EncryptWithAES - This function receives a either a string , []byte , or json.Marshaller type and encrypts it using AES encryption and returns a []byte to the pipeline. NewEncryption ( \"key\" , \"initializationVector\" ). EncryptWithAES Batch Included in the SDK is an in-memory batch function that will hold on to your data before continuing the pipeline. There are three functions provided for batching each with their own strategy. Factory Method Description NewBatchByTime(timeInterval string) This function returns a BatchConfig instance with time being the strategy that is used for determining when to release the batched data and continue the pipeline. timeInterval is the duration to wait (i.e. 10s ). The time begins after the first piece of data is received. If no data has been received no data will be sent forward. // Example: NewBatchByTime ( \"10s\" ). Batch NewBatchByCount(batchThreshold int) This function returns a BatchConfig instance with count being the strategy that is used for determining when to release the batched data and continue the pipeline. batchThreshold is how many events to hold on to (i.e. 25 ). The count begins after the first piece of data is received and once the threshold is met, the batched data will continue forward and the counter will be reset. // Example: NewBatchByCount ( 10 ). Batch NewBatchByTimeAndCount(timeInterval string, batchThreshold int) This function returns a BatchConfig instance with a combination of both time and count being the strategy that is used for determining when to release the batched data and continue the pipeline. Whichever occurs first will trigger the data to continue and be reset. // Example: NewBatchByCount ( \"30s\" , 10 ). Batch Batch - This function will apply the selected strategy in your pipeline. Warning Keep memory usage in mind as you determine the thresholds for both time and count. The larger they are the more memory is required and could lead to performance issue. Conversion There are two conversions included in the SDK that can be added to your pipeline. These transforms return a string . Factory Method Description NewConversion() This function returns a Conversion instance that is used to access the conversion functions. XML TransformToXML - This function receives an events.Model type, converts it to XML format and returns the XML string to the pipeline. NewConversion (). TransformToXML JSON TransformToJSON - This function receives an events.Model type and converts it to JSON format and returns the JSON string to the pipeline. NewConversion (). TransformToJSON Compressions There are two compression types included in the SDK that can be added to your pipeline. These transforms return a []byte . Factory Method Description NewCompression() This function returns a Compression instance that is used to access the compression functions. GZIP CompressWithGZIP - This function receives either a string , []byte , or json.Marshaler type, GZIP compresses the data, converts result to base64 encoded string, which is returned as a []byte to the pipeline. NewCompression (). CompressWithGZIP ZLIB CompressWithZLIB - This function receives either a string , []byte , or json.Marshaler type, ZLIB compresses the data, converts result to base64 encoded string, which is returned as a []byte to the pipeline. NewCompression (). CompressWithZLIB CoreData Functions These are functions that enable interactions with the CoreData REST API. Factory Method Description NewCoreData() This function returns a CoreData instance. This CoreData instance is used to access core data functions. Mark As Pushed MarkAsPushed - This function provides the MarkAsPushed function from the context as a First-Class Transform that can be called in your pipeline. See Definition Above . The data passed into this function from the pipeline is passed along unmodified since all required information is provided on the context (EventId, CorrelationId,etc.. ) NewCoreData (). MarkAsPushed Push to Core PushToCore - This function provides the PushToCore function from the context as a First-Class Transform that can be called in your pipeline. See Definition Above . The data passed into this function from the pipeline is wrapped in an EdgeX event with the deviceName and readingName that were set upon the CoreData instance and then sent to Core Data service to be added as an event. Returns the new EdgeX event with ID populated. NewCoreData (). PushToCore Note If validation is turned on in Core Services then your deviceName and readingName must exist in the Core Metadata service and be properly registered in EdgeX. Export Functions There are a few export functions included in the SDK that can be added to your pipeline. HTTP HTTPPost - This function receives either a string , []byte , or json.Marshaler type from the previous function in the pipeline and posts it to the configured endpoint. If no previous function exists, then the event that triggered the pipeline, marshaled to json, will be used. If the post fails and persistOnError is true and Store and Forward is enabled, the data will be stored for later retry. See Store and Forward for more details. Factory Method Description NewHTTPSender(url string, mimeType string, persistOnError bool) This function returns a HTTPSender instance initialized with the passed in url, mime type and persistOnError values. NewHTTPSenderWithSecretHeader(url string, mimeType string, persistOnError bool, httpHeaderSecretName string, secretPath string) This function returns a HTTPSender instance similar to the above function however will set up the HTTPSender to add a header to the HTTP request using the httpHeaderSecretName as both the header key and the key to search for in the secret provider at secretPath leveraging secure storage of secrets. Example NewHTTPSender ( \"https://myendpoint.com\" , \"application/json\" , false ). HTTPPost //assumes TransformToJSON was used before this transform in the pipeline NewHTTPSenderWithSecretHeader ( \"https://myendpoint.com\" , \"application/json\" , false , \"Authentication\" , \"/jwt\" ). HTTPPost //assumes TransformToJSON was used before this transform in the pipeline and /jwt has been seeded into the secret provider with a key of Authentication MQTT Factory Method Description NewMQTTSecretSender(mqttConfig MQTTSecretConfig, persistOnError bool) This function returns a MQTTSecretSender instance initialized with the options specified in the MQTTSecretConfig . type MQTTSecretConfig struct { // BrokerAddress should be set to the complete broker address i.e. mqtts://mosquitto:8883/mybroker BrokerAddress string // ClientId to connect with the broker with. ClientId string // The name of the path in secret provider to retrieve your secrets SecretPath string // AutoReconnect indicated whether or not to retry connection if disconnected AutoReconnect bool // Topic that you wish to publish to Topic string // QoS for MQTT Connection QoS byte // Retain setting for MQTT Connection Retain bool // SkipCertVerify SkipCertVerify bool // AuthMode indicates what to use when connecting to the broker. // Options are \"none\", \"cacert\" , \"usernamepassword\", \"clientcert\". // If a CA Cert exists in the SecretPath then it will be used for // all modes except \"none\". AuthMode string } Secrets in the secret provider may be located at any path however they must have some or all the follow keys at the specified SecretPath . username - username to connect to the broker password - password used to connect to the broker clientkey - client private key in PEM format clientcert - client cert in PEM format cacert - ca cert in PEM format What AuthMode you choose depends on what values are used. For example, if \"none\" is specified as auth mode all keys will be ignored. Similarly, if AuthMode is set to \"clientcert\" username and password will be ignored. Factory Method Description DEPRECATED NewMQTTSender(logging logger.LoggingClient, addr models.Addressable, keyCertPair *KeyCertPair, mqttConfig MqttConfig, persistOnError bool) This function returns a MQTTSender instance initialized with the passed in MQTT configuration . This MQTTSender instance is used to access the following function that will use the specified MQTT configuration - `KeyCertPair` - This structure holds the Key and Certificate information for when using secure ** TLS ** connection to the broker . Can be `nil` if not using secure ** TLS ** connection . - `MqttConfig` - This structure holds addition MQTT configuration settings . Qos byte Retain bool AutoReconnect bool SkipCertVerify bool User string Password string Note The GO complier will default these to 0 , false and \"\" , so you only need to set the fields that your usage requires that differ from the default. MQTTSend - This function receives either a string , []byte , or json.Marshaler type from the previous function in the pipeline and sends it to the specified MQTT broker. If no previous function exists, then the event that triggered the pipeline, marshaled to json, will be used. If the send fails and persistOnError is true and Store and Forward is enabled, the data will be stored for later retry. See Store and Forward for more details. Output Functions There is one output function included in the SDK that can be added to your pipeline. Factory Method Description NewOutput() This function returns a Output instance that is used to access the following output function SetOutput - This function receives either a string , []byte , or json.Marshaler type from the previous function in the pipeline and sets it as the output data for the pipeline to return to the configured trigger. If configured to use message bus, the data will be published to the message bus as determined by the MessageBus and Binding configuration. If configured to use HTTP trigger the data is returned as the HTTP response. Note Calling Complete() from the Context API in a custom function can be used in place of adding this function to your pipeline","title":"Built-In Transforms/Functions"},{"location":"microservices/application/BuiltIn/#built-in-transformsfunctions","text":"All transforms define a type and a New function which is used to initialize an instance of the type with the required parameters. These instances returned by these New functions give access to their appropriate pipeline function pointers when setting up the function pipeline. Example NewFilter ([] { \"Device1\" , \"Device2\" }). FilterByDeviceName","title":"Built-In Transforms/Functions"},{"location":"microservices/application/BuiltIn/#filtering","text":"There are two basic types of filtering included in the SDK to add to your pipeline. There is also an option to Filter Out specific items. These provided filter functions return a type of events.Model. If filtering results in no remaining data, the pipeline execution for that pass is terminated. If no values are provided for filtering, then data flows through unfiltered. Factory Method Description NewFilter([]string filterValues) This function returns a Filter instance initialized with the passed in filter values. This Filter instance is used to access the following filter functions that will operate using the specified filter values. type Filter struct { // Holds the values to be filtered FilterValues [] string // Determines if items in FilterValues should be filtered out. If set to true all items found in the filter will be removed. If set to false all items found in the filter will be returned. If FilterValues is empty then all items will be returned. FilterOut bool }","title":"Filtering"},{"location":"microservices/application/BuiltIn/#by-device-name","text":"FilterByDeviceName - This function will filter the event data down to the specified device names and return the filtered data to the pipeline. NewFilter ([] { \"Device1\" , \"Device2\" }). FilterByDeviceName","title":"By Device Name"},{"location":"microservices/application/BuiltIn/#by-value-descriptor","text":"FilterByValueDescriptor - This function will filter the event data down to the specified device value descriptor and return the filtered data to the pipeline. NewFilter ([] { \"ValueDescriptor1\" , \"ValueDescriptor2\" }). FilterByValueDescriptor","title":"By Value Descriptor"},{"location":"microservices/application/BuiltIn/#json-logic","text":"Factory Method Description NewJSONLogic(rule string) This function returns a JSONLogic instance initialized with the passed in JSON rule. The rule passed in should be a JSON string conforming to the specification here: http://jsonlogic.com/operations.html. Evaluate - This is the function that will be used in the pipeline to apply the JSON rule to data coming in on the pipeline. If the condition of your rule is met, then the pipeline will continue and the data will continue to flow to the next function in the pipeline. If the condition of your rule is NOT met, then pipeline execution stops. NewJSONLogic ( \"{ \\\"in\\\" : [{ \\\"var\\\" : \\\"device\\\" }, [\\\"Random-Integer-Device\\\",\\\"Random-Float-Device\\\"] ] }\" ). Evaluate Note Only operations that return true or false are supported. See http://jsonlogic.com/operations.html# for the complete list of operations paying attention to return values. Any operator that returns manipulated data is currently not supported. For more advanced scenarios checkout EMQ X Kuiper . Tip Leverage http://jsonlogic.com/play.html to get your rule right before implementing in code. JSON can be a bit tricky to get right in code with all the escaped double quotes.","title":"JSON Logic"},{"location":"microservices/application/BuiltIn/#encryption","text":"There is one encryption transform included in the SDK that can be added to your pipeline. Factory Method Description NewEncryption(key string, initializationVector string) This function returns a Encryption instance initialized with the passed in key and initialization vector. This Encryption instance is used to access the following encryption function that will use the specified key and initialization vector.","title":"Encryption"},{"location":"microservices/application/BuiltIn/#aes","text":"EncryptWithAES - This function receives a either a string , []byte , or json.Marshaller type and encrypts it using AES encryption and returns a []byte to the pipeline. NewEncryption ( \"key\" , \"initializationVector\" ). EncryptWithAES","title":"AES"},{"location":"microservices/application/BuiltIn/#batch","text":"Included in the SDK is an in-memory batch function that will hold on to your data before continuing the pipeline. There are three functions provided for batching each with their own strategy. Factory Method Description NewBatchByTime(timeInterval string) This function returns a BatchConfig instance with time being the strategy that is used for determining when to release the batched data and continue the pipeline. timeInterval is the duration to wait (i.e. 10s ). The time begins after the first piece of data is received. If no data has been received no data will be sent forward. // Example: NewBatchByTime ( \"10s\" ). Batch NewBatchByCount(batchThreshold int) This function returns a BatchConfig instance with count being the strategy that is used for determining when to release the batched data and continue the pipeline. batchThreshold is how many events to hold on to (i.e. 25 ). The count begins after the first piece of data is received and once the threshold is met, the batched data will continue forward and the counter will be reset. // Example: NewBatchByCount ( 10 ). Batch NewBatchByTimeAndCount(timeInterval string, batchThreshold int) This function returns a BatchConfig instance with a combination of both time and count being the strategy that is used for determining when to release the batched data and continue the pipeline. Whichever occurs first will trigger the data to continue and be reset. // Example: NewBatchByCount ( \"30s\" , 10 ). Batch Batch - This function will apply the selected strategy in your pipeline. Warning Keep memory usage in mind as you determine the thresholds for both time and count. The larger they are the more memory is required and could lead to performance issue.","title":"Batch"},{"location":"microservices/application/BuiltIn/#conversion","text":"There are two conversions included in the SDK that can be added to your pipeline. These transforms return a string . Factory Method Description NewConversion() This function returns a Conversion instance that is used to access the conversion functions.","title":"Conversion"},{"location":"microservices/application/BuiltIn/#xml","text":"TransformToXML - This function receives an events.Model type, converts it to XML format and returns the XML string to the pipeline. NewConversion (). TransformToXML","title":"XML"},{"location":"microservices/application/BuiltIn/#json","text":"TransformToJSON - This function receives an events.Model type and converts it to JSON format and returns the JSON string to the pipeline. NewConversion (). TransformToJSON","title":"JSON"},{"location":"microservices/application/BuiltIn/#compressions","text":"There are two compression types included in the SDK that can be added to your pipeline. These transforms return a []byte . Factory Method Description NewCompression() This function returns a Compression instance that is used to access the compression functions.","title":"Compressions"},{"location":"microservices/application/BuiltIn/#gzip","text":"CompressWithGZIP - This function receives either a string , []byte , or json.Marshaler type, GZIP compresses the data, converts result to base64 encoded string, which is returned as a []byte to the pipeline. NewCompression (). CompressWithGZIP","title":"GZIP"},{"location":"microservices/application/BuiltIn/#zlib","text":"CompressWithZLIB - This function receives either a string , []byte , or json.Marshaler type, ZLIB compresses the data, converts result to base64 encoded string, which is returned as a []byte to the pipeline. NewCompression (). CompressWithZLIB","title":"ZLIB"},{"location":"microservices/application/BuiltIn/#coredata-functions","text":"These are functions that enable interactions with the CoreData REST API. Factory Method Description NewCoreData() This function returns a CoreData instance. This CoreData instance is used to access core data functions.","title":"CoreData Functions"},{"location":"microservices/application/BuiltIn/#mark-as-pushed","text":"MarkAsPushed - This function provides the MarkAsPushed function from the context as a First-Class Transform that can be called in your pipeline. See Definition Above . The data passed into this function from the pipeline is passed along unmodified since all required information is provided on the context (EventId, CorrelationId,etc.. ) NewCoreData (). MarkAsPushed","title":"Mark As Pushed"},{"location":"microservices/application/BuiltIn/#push-to-core","text":"PushToCore - This function provides the PushToCore function from the context as a First-Class Transform that can be called in your pipeline. See Definition Above . The data passed into this function from the pipeline is wrapped in an EdgeX event with the deviceName and readingName that were set upon the CoreData instance and then sent to Core Data service to be added as an event. Returns the new EdgeX event with ID populated. NewCoreData (). PushToCore Note If validation is turned on in Core Services then your deviceName and readingName must exist in the Core Metadata service and be properly registered in EdgeX.","title":"Push to Core"},{"location":"microservices/application/BuiltIn/#export-functions","text":"There are a few export functions included in the SDK that can be added to your pipeline.","title":"Export Functions"},{"location":"microservices/application/BuiltIn/#http","text":"HTTPPost - This function receives either a string , []byte , or json.Marshaler type from the previous function in the pipeline and posts it to the configured endpoint. If no previous function exists, then the event that triggered the pipeline, marshaled to json, will be used. If the post fails and persistOnError is true and Store and Forward is enabled, the data will be stored for later retry. See Store and Forward for more details. Factory Method Description NewHTTPSender(url string, mimeType string, persistOnError bool) This function returns a HTTPSender instance initialized with the passed in url, mime type and persistOnError values. NewHTTPSenderWithSecretHeader(url string, mimeType string, persistOnError bool, httpHeaderSecretName string, secretPath string) This function returns a HTTPSender instance similar to the above function however will set up the HTTPSender to add a header to the HTTP request using the httpHeaderSecretName as both the header key and the key to search for in the secret provider at secretPath leveraging secure storage of secrets. Example NewHTTPSender ( \"https://myendpoint.com\" , \"application/json\" , false ). HTTPPost //assumes TransformToJSON was used before this transform in the pipeline NewHTTPSenderWithSecretHeader ( \"https://myendpoint.com\" , \"application/json\" , false , \"Authentication\" , \"/jwt\" ). HTTPPost //assumes TransformToJSON was used before this transform in the pipeline and /jwt has been seeded into the secret provider with a key of Authentication","title":"HTTP"},{"location":"microservices/application/BuiltIn/#mqtt","text":"Factory Method Description NewMQTTSecretSender(mqttConfig MQTTSecretConfig, persistOnError bool) This function returns a MQTTSecretSender instance initialized with the options specified in the MQTTSecretConfig . type MQTTSecretConfig struct { // BrokerAddress should be set to the complete broker address i.e. mqtts://mosquitto:8883/mybroker BrokerAddress string // ClientId to connect with the broker with. ClientId string // The name of the path in secret provider to retrieve your secrets SecretPath string // AutoReconnect indicated whether or not to retry connection if disconnected AutoReconnect bool // Topic that you wish to publish to Topic string // QoS for MQTT Connection QoS byte // Retain setting for MQTT Connection Retain bool // SkipCertVerify SkipCertVerify bool // AuthMode indicates what to use when connecting to the broker. // Options are \"none\", \"cacert\" , \"usernamepassword\", \"clientcert\". // If a CA Cert exists in the SecretPath then it will be used for // all modes except \"none\". AuthMode string } Secrets in the secret provider may be located at any path however they must have some or all the follow keys at the specified SecretPath . username - username to connect to the broker password - password used to connect to the broker clientkey - client private key in PEM format clientcert - client cert in PEM format cacert - ca cert in PEM format What AuthMode you choose depends on what values are used. For example, if \"none\" is specified as auth mode all keys will be ignored. Similarly, if AuthMode is set to \"clientcert\" username and password will be ignored. Factory Method Description DEPRECATED NewMQTTSender(logging logger.LoggingClient, addr models.Addressable, keyCertPair *KeyCertPair, mqttConfig MqttConfig, persistOnError bool) This function returns a MQTTSender instance initialized with the passed in MQTT configuration . This MQTTSender instance is used to access the following function that will use the specified MQTT configuration - `KeyCertPair` - This structure holds the Key and Certificate information for when using secure ** TLS ** connection to the broker . Can be `nil` if not using secure ** TLS ** connection . - `MqttConfig` - This structure holds addition MQTT configuration settings . Qos byte Retain bool AutoReconnect bool SkipCertVerify bool User string Password string Note The GO complier will default these to 0 , false and \"\" , so you only need to set the fields that your usage requires that differ from the default. MQTTSend - This function receives either a string , []byte , or json.Marshaler type from the previous function in the pipeline and sends it to the specified MQTT broker. If no previous function exists, then the event that triggered the pipeline, marshaled to json, will be used. If the send fails and persistOnError is true and Store and Forward is enabled, the data will be stored for later retry. See Store and Forward for more details.","title":"MQTT"},{"location":"microservices/application/BuiltIn/#output-functions","text":"There is one output function included in the SDK that can be added to your pipeline. Factory Method Description NewOutput() This function returns a Output instance that is used to access the following output function SetOutput - This function receives either a string , []byte , or json.Marshaler type from the previous function in the pipeline and sets it as the output data for the pipeline to return to the configured trigger. If configured to use message bus, the data will be published to the message bus as determined by the MessageBus and Binding configuration. If configured to use HTTP trigger the data is returned as the HTTP response. Note Calling Complete() from the Context API in a custom function can be used in place of adding this function to your pipeline","title":"Output Functions"},{"location":"microservices/application/ContextAPI/","text":"The context parameter passed to each function/transform provides operations and data associated with each execution of the pipeline. Let's take a look at a few of the properties that are available: type Context struct { // ID of the EdgeX Event (will be filled for a received JSON Event) EventID string // Checksum of the EdgeX Event (will be filled for a received CBOR Event) EventChecksum string // This is the ID used to track the EdgeX event through entire EdgeX framework. CorrelationID string // OutputData is used for specifying the data that is to be outputted. Leverage the .Complete() function to set. OutputData [] byte // This holds the configuration for your service. This is the preferred way to access your custom application settings that have been set in the configuration. Configuration common . ConfigurationStruct // LoggingClient is exposed to allow logging following the preferred logging strategy within EdgeX. LoggingClient logger . LoggingClient // EventClient exposes Core Data's EventClient API EventClient coredata . EventClient // ValueDescriptorClient exposes Core Data's ValueDescriptor API ValueDescriptorClient coredata . ValueDescriptorClient // CommandClient exposes Core Commands's Command API CommandClient command . CommandClient // NotificationsClient exposes Support Notification's Notifications API NotificationsClient notifications . NotificationsClient // RetryData holds the data to be stored for later retry when the pipeline function returns an error RetryData [] byte // SecretProvider exposes the support for getting and storing secrets SecretProvider * security . SecretProvider } Clients LoggingClient The LoggingClient exposed on the context is available to leverage logging libraries/service utilized throughout the EdgeX framework. The SDK has initialized everything so it can be used to log Trace , Debug , Warn , Info , and Error messages as appropriate. See simple-filter-xml/main.go for an example of how to use the LoggingClient . EventClient The EventClient exposed on the context is available to leverage Core Data's Event API. See interface definition for more details. This client is useful for querying events and is used by the MarkAsPushed convenience API described below. ValueDescriptorClient The ValueDescriptorClient exposed on the context is available to leverage Core Data's ValueDescriptor API. See interface definition for more details. Useful for looking up the value descriptor for a reading received. CommandClient The CommandClient exposed on the context is available to leverage Core Command's Command API. See interface definition for more details. Useful for sending commands to devices. NotificationsClient The NotificationsClient exposed on the context is available to leverage Support Notifications' Notifications API. See README for more details. Useful for sending notifications. Note about Clients Each of the clients above is only initialized if the Clients section of the configuration contains an entry for the service associated with the Client API. If it isn't in the configuration the client will be nil . Your code must check for nil to avoid panic in case it is missing from the configuration. Only add the clients to your configuration that your Application Service will actually be using. All application services need Core-Data for version compatibility check done on start-up. The following is an example Clients section of a configuration.toml with all supported clients specified: [Clients] [Clients.Logging] Protocol = \"http\" Host = \"localhost\" Port = 48061 [Clients.CoreData] Protocol = 'http' Host = 'localhost' Port = 48080 [Clients.Command] Protocol = 'http' Host = 'localhost' Port = 48082 [Clients.Notifications] Protocol = 'http' Host = 'localhost' Port = 48060 .MarkAsPushed() .MarkAsPushed() is used to indicate to EdgeX Core Data that an event has been \"pushed\" and is no longer required to be stored. The scheduler service will purge all events that have been marked as pushed based on the configured schedule. By default, it is once daily at midnight. If you leverage the built in export functions (i.e. HTTP Export, or MQTT Export), then simply adding the MaskedAsPush function to you pipeline after the export function will take care of calling this API. .PushToCore() .PushToCore(string deviceName, string readingName, byte[] value) is used to push data to EdgeX Core Data so that it can be shared with other applications that are subscribed to the message bus that core-data publishes to. deviceName can be set as you like along with the readingName which will be set on the EdgeX event sent to CoreData. This function will return the new EdgeX Event with the ID populated, however the CorrelationId will not be available. Note If validation is turned on in CoreServices then your deviceName and readingName must exist in the CoreMetadata and be properly registered in EdgeX. Warning Be aware that without a filter in your pipeline, it is possible to create an infinite loop when the Message Bus trigger is used. Choose your device-name and reading name appropriately. .Complete() .Complete([]byte outputData) can be used to return data back to the configured trigger. In the case of an HTTP trigger, this would be an HTTP Response to the caller. In the case of a message bus trigger, this is how data can be published to a new topic per the configuration. .SetRetryData() .SetRetryData(payload []byte) can be used to store data for later retry. This is useful when creating a custom export function that needs to retry on failure when sending the data. The payload data will be stored for later retry based on Store and Forward configuration. When the retry is triggered, the function pipeline will be re-executed starting with the function that called this API. That function will be passed the stored data, so it is important that all transformations occur in functions prior to the export function. The Context will also be restored to the state when the function called this API. See Store and Forward for more details. Note Store and Forward be must enabled when calling this API. .GetSecrets() .GetSecrets(path string, keys ...string) is used to retrieve secrets from the secret store. path specifies the type or location of the secrets to retrieve. If specified, it is appended to the base path from the exclusive secret store configuration. keys specifies the list of secrets to be retrieved. If no keys are provided then all the keys associated with the specified path will be returned.","title":"Context API"},{"location":"microservices/application/ContextAPI/#clients","text":"","title":"Clients"},{"location":"microservices/application/ContextAPI/#loggingclient","text":"The LoggingClient exposed on the context is available to leverage logging libraries/service utilized throughout the EdgeX framework. The SDK has initialized everything so it can be used to log Trace , Debug , Warn , Info , and Error messages as appropriate. See simple-filter-xml/main.go for an example of how to use the LoggingClient .","title":"LoggingClient"},{"location":"microservices/application/ContextAPI/#eventclient","text":"The EventClient exposed on the context is available to leverage Core Data's Event API. See interface definition for more details. This client is useful for querying events and is used by the MarkAsPushed convenience API described below.","title":"EventClient"},{"location":"microservices/application/ContextAPI/#valuedescriptorclient","text":"The ValueDescriptorClient exposed on the context is available to leverage Core Data's ValueDescriptor API. See interface definition for more details. Useful for looking up the value descriptor for a reading received.","title":"ValueDescriptorClient"},{"location":"microservices/application/ContextAPI/#commandclient","text":"The CommandClient exposed on the context is available to leverage Core Command's Command API. See interface definition for more details. Useful for sending commands to devices.","title":"CommandClient"},{"location":"microservices/application/ContextAPI/#notificationsclient","text":"The NotificationsClient exposed on the context is available to leverage Support Notifications' Notifications API. See README for more details. Useful for sending notifications.","title":"NotificationsClient"},{"location":"microservices/application/ContextAPI/#note-about-clients","text":"Each of the clients above is only initialized if the Clients section of the configuration contains an entry for the service associated with the Client API. If it isn't in the configuration the client will be nil . Your code must check for nil to avoid panic in case it is missing from the configuration. Only add the clients to your configuration that your Application Service will actually be using. All application services need Core-Data for version compatibility check done on start-up. The following is an example Clients section of a configuration.toml with all supported clients specified: [Clients] [Clients.Logging] Protocol = \"http\" Host = \"localhost\" Port = 48061 [Clients.CoreData] Protocol = 'http' Host = 'localhost' Port = 48080 [Clients.Command] Protocol = 'http' Host = 'localhost' Port = 48082 [Clients.Notifications] Protocol = 'http' Host = 'localhost' Port = 48060","title":"Note about Clients"},{"location":"microservices/application/ContextAPI/#markaspushed","text":".MarkAsPushed() is used to indicate to EdgeX Core Data that an event has been \"pushed\" and is no longer required to be stored. The scheduler service will purge all events that have been marked as pushed based on the configured schedule. By default, it is once daily at midnight. If you leverage the built in export functions (i.e. HTTP Export, or MQTT Export), then simply adding the MaskedAsPush function to you pipeline after the export function will take care of calling this API.","title":".MarkAsPushed()"},{"location":"microservices/application/ContextAPI/#pushtocore","text":".PushToCore(string deviceName, string readingName, byte[] value) is used to push data to EdgeX Core Data so that it can be shared with other applications that are subscribed to the message bus that core-data publishes to. deviceName can be set as you like along with the readingName which will be set on the EdgeX event sent to CoreData. This function will return the new EdgeX Event with the ID populated, however the CorrelationId will not be available. Note If validation is turned on in CoreServices then your deviceName and readingName must exist in the CoreMetadata and be properly registered in EdgeX. Warning Be aware that without a filter in your pipeline, it is possible to create an infinite loop when the Message Bus trigger is used. Choose your device-name and reading name appropriately.","title":".PushToCore()"},{"location":"microservices/application/ContextAPI/#complete","text":".Complete([]byte outputData) can be used to return data back to the configured trigger. In the case of an HTTP trigger, this would be an HTTP Response to the caller. In the case of a message bus trigger, this is how data can be published to a new topic per the configuration.","title":".Complete()"},{"location":"microservices/application/ContextAPI/#setretrydata","text":".SetRetryData(payload []byte) can be used to store data for later retry. This is useful when creating a custom export function that needs to retry on failure when sending the data. The payload data will be stored for later retry based on Store and Forward configuration. When the retry is triggered, the function pipeline will be re-executed starting with the function that called this API. That function will be passed the stored data, so it is important that all transformations occur in functions prior to the export function. The Context will also be restored to the state when the function called this API. See Store and Forward for more details. Note Store and Forward be must enabled when calling this API.","title":".SetRetryData()"},{"location":"microservices/application/ContextAPI/#getsecrets","text":".GetSecrets(path string, keys ...string) is used to retrieve secrets from the secret store. path specifies the type or location of the secrets to retrieve. If specified, it is appended to the base path from the exclusive secret store configuration. keys specifies the list of secrets to be retrieved. If no keys are provided then all the keys associated with the specified path will be returned.","title":".GetSecrets()"},{"location":"microservices/application/ErrorHandling/","text":"Error Handling Each transform returns a true or false as part of the return signature. This is called the continuePipeline flag and indicates whether the SDK should continue calling successive transforms in the pipeline. return false, nil will stop the pipeline and stop processing the event. This is useful, for example, when filtering on values and nothing matches the criteria you've filtered on. return false, error , will stop the pipeline as well and the SDK will log the error you have returned. return true, nil tells the SDK to continue, and will call the next function in the pipeline with your result. The SDK will return control back to main when receiving a SIGTERM/SIGINT event to allow for custom clean up.","title":"Error Handling"},{"location":"microservices/application/ErrorHandling/#error-handling","text":"Each transform returns a true or false as part of the return signature. This is called the continuePipeline flag and indicates whether the SDK should continue calling successive transforms in the pipeline. return false, nil will stop the pipeline and stop processing the event. This is useful, for example, when filtering on values and nothing matches the criteria you've filtered on. return false, error , will stop the pipeline as well and the SDK will log the error you have returned. return true, nil tells the SDK to continue, and will call the next function in the pipeline with your result. The SDK will return control back to main when receiving a SIGTERM/SIGINT event to allow for custom clean up.","title":"Error Handling"},{"location":"microservices/application/GeneralAppServiceConfig/","text":"General App Service Configuration Similar to other EdgeX services, configuration is first determined by the configuration.toml file in the /res folder. If -cp is passed to the application on startup, the SDK will leverage the specific configuration provider (i.e Consul) to push configuration from the file into the registry and monitor configuration from there. You will find the configuration under the edgex/appservices/1.0/ key. This section describes the configuration elements that are unique to Application Services Please refer to the general Configuration documentation for configuration properties common across all services. Note * indicates the configuration value can be changed on the fly if using a configuration provider (like Consul). ** indicates the configuration value can be changed but the service must be restarted. Writable The following are additional entries in the Writable section which are applicable to Application Services. Writable StoreAndForward The section configures the Store and Forward capability. Please refer to Store and Forward section for more details. Configuration Default Value Description Writable StoreAndForward Enabled false* Indicates whether the Store and Forward capability enabled or disabled Writable StoreAndForward RetryInterval \"5m\"* Indicates the duration of time to wait before retries, aka Forward Writable StoreAndForward MaxRetryCount 10* Indicates whether maximum number of retries of failed data. The failed data is removed after the maximum retries has been exceeded. A value of 0 indicates endless retries. Writable Pipeline The section configures the Configurable Function Pipeline which is used only by App Service Configurable. Please refer to App Service Configurable - Getting Started section for more details Writable InsecureSecrets This section defines Insecure Secrets that are used when running is non-secure mode, i.e. when Vault isn't available. This is a dynamic map of configuration, so can empty if no secrets are used or can have as many or few user define secrets. Below are a few that are need if using the indicated capabilities. Configuration Default Value Description Writable InsecureSecrets DB --- This section defines a block of insecure secrets for database connection when Store and Forward is enabled and running is non-secure mode. This section is not required if Store and Forward is not enabled. Writable InsecureSecrets DB path redisdb* Indicates the type of database the insecure secrets are for. redisdb id the DB type name used internally and used to look up the credentials. Writable InsecureSecrets DB Secrets --- This section contains the Secrets key value pair map of database credentials Writable InsecureSecrets DB Secrets username blank* Indicates the value for the username when connecting to the database. When running in non-secure mode it is blank . Writable InsecureSecrets DB Secrets password blank* Indicates the value for the password when connecting to the database. When running in non-secure mode it is blank . Writable InsecureSecrets http --- This section defines a block of insecure secrets for HTTP Export, i.e HTTPPost function Writable InsecureSecrets http path http* Indicates the secrets path for HTTP Export. Must match the secretpath name configured for the HTTPPost function. Writable InsecureSecrets http Secrets --- This section contains the Secrets key value pair map for the HTTPPost function Writable InsecureSecrets http Secrets [headername] undefined* This indicates the HTTP header name and the value to set it to. I.e. the key name you choose is the actual HTTP Header name. The key name must match the secretheadername configured for HTTPPost . The value is what you need the header set to. Writable InsecureSecrets MQTT --- This section defines a block of insecure secrets for MQTT export, i.e. MQTTSecretSend function. Writable InsecureSecrets MQTT path mqtt* Indicates the secrets path for MQTT Export. Must match the secretpath name configured for the MQTTSecretSend function. Writable InsecureSecrets MQTT Secrets --- This section contains the Secrets key value pair map for the MQTTSecretSend function Writable InsecureSecrets MQTT Secrets username blank* Indicates the value for the username when connecting to the MQTT broker using usernamepassword authentication mode. Must be configured to the value the MQTT broker is expecting. Writable InsecureSecrets MQTT Secrets password blank* Indicates the value for the password when connecting to the MQTT broker using usernamepassword authentication mode. Must be configured to the value the MQTT broker is expecting. Writable InsecureSecrets MQTT Secrets cacert blank* Indicates the value (contents) for the CA Certificate when connecting to the MQTT broker using cacert authentication mode. Must be configured to the value the MQTT broker is expecting. Writable InsecureSecrets MQTT Secrets clientcert blank* Indicates the value (contents) for the Client Certificate when connecting to the MQTT broker using clientcert authentication mode. Must be configured to the value the MQTT broker is expecting. Writable InsecureSecrets MQTT Secrets clientkey blank* Indicates the value (contents) for the Client Key when connecting to the MQTT broker using clientcert authentication mode. Must be configured to the value the MQTT broker is expecting. Not Writable The following are additional configuration which are applicable to Application Services that require the service to be restarted after value(s) are changed. Database This optional section contains the connection information. It is only required when the Store and Forward capability is enabled. Note that it has a slightly different format that the database section used in the core services configuration. Configuration Default Value Description Database Type redisdb** Indicates the type of database used. redisdb and mongodb are the only valid types. Database Host localhost** Indicates the hostname for the database Database Port 6379** Indicates the port number for the database Database Timeout \"30s\"** Indicates the connection timeout for the database SecretStoreExclusive This optional section defines the configuration for the Exclusive Secret Store (i.e. Vault) used to Put and Get secrets that are exclusive to the instance of the Application Service. Please refer to the Secrets section for more details. Configuration Default Value Description SecretStoreExclusive Host localhost** Indicates the hostname for the Secret Store SecretStoreExclusive Port 8200** Indicates the port number for the Secret Store SecretStoreExclusive Path Depends on profile used Indicates the base path for the secrets with in the SecretStoreExclusive Protocol https** Indicates the protocol used for the Secret Store SecretStoreExclusive RootCaCertPath /vault/config/pki/ EdgeXFoundryCA/ EdgeXFoundryCA.pem** Indicates the path to the root CA Certificate for Vault SecretStoreExclusive ServerName localhost** Indicates the server name for the Secret Store SecretStoreExclusive TokenFile /vault/config/ assets/ resp-init.json** Indicates the path to the exclusive token for the service to connect to the Secret Store SecretStoreExclusive AdditionalRetryAttempts 10** Indicates the maximum number of failed connection attempts allowed SecretStoreExclusive RetryWaitPeriod \"1s\"** Indicates the wait time between failed connection attempts SecretStoreExclusive Authentication --- The section defines the Secret Store Authentication SecretStoreExclusive Authentication AuthType X-Vault-Token** Indicates the authentication type used when connecting to the Secret Store Clients This section defines the clients connect information. Please refer to the Note about Clients section for more details. Binding This section defines the Trigger binding for incoming data. Configuration Default Value Description Binding Type messagebus** Indicates the Trigger binding type. valid values are messagebus and http Binding SubscribeTopic events** Only used for messagebus binding type Indicates the subscribe topic to use to receive data from the Message Bus Binding PublishTopic blank** Only used for messagebus binding type Indicates the publish topic to use when sending data to the Message Bus MessageBus This section defines the message bus connect information. Only used for messagebus binding type Configuration Default Value Description MessageBus Type zero** Indicates the type of message bus being used. Valid type are zero , mqtt or redisstreams MessageBus SubscribeHost ... This section defines the connection information for subscribing to the Message Bus MessageBus SubscribeHost Host localhost** Indicates the hostname for subscribing to the Message Bus MessageBus SubscribeHost Port 5563** Indicates the port number for subscribing to the Message Bus MessageBus SubscribeHost Protocol tcp** Indicates the protocol number for subscribing to the Message Bus MessageBus PublishHost ... This section defines the connection information for publishing to the Message Bus MessageBus PublishHost Host \" \" * Indicates the hostname for publishing to the Message Bus MessageBus SubscribeHost Port 5565** Indicates the port number for publishing to the Message Bus MessageBus SubscribeHost Protocol tcp** Indicates the protocol number for publishing to the Message Bus MessageBus Optional ... This section is used for optional configuration specific to the Message Bus type used. Please refer to go-mod-messaging for more details Application Settings [ApplicationSettings] - Is used for custom application settings and is accessed via the ApplicationSettings() API. The ApplicationSettings API returns a map[string] string containing the contents on the ApplicationSetting section of the configuration.toml file. [ApplicationSettings] ApplicationName = \"My Application Service\"","title":"General App Service Configuration"},{"location":"microservices/application/GeneralAppServiceConfig/#general-app-service-configuration","text":"Similar to other EdgeX services, configuration is first determined by the configuration.toml file in the /res folder. If -cp is passed to the application on startup, the SDK will leverage the specific configuration provider (i.e Consul) to push configuration from the file into the registry and monitor configuration from there. You will find the configuration under the edgex/appservices/1.0/ key. This section describes the configuration elements that are unique to Application Services Please refer to the general Configuration documentation for configuration properties common across all services. Note * indicates the configuration value can be changed on the fly if using a configuration provider (like Consul). ** indicates the configuration value can be changed but the service must be restarted.","title":"General App Service Configuration"},{"location":"microservices/application/GeneralAppServiceConfig/#writable","text":"The following are additional entries in the Writable section which are applicable to Application Services.","title":"Writable"},{"location":"microservices/application/GeneralAppServiceConfig/#writable-storeandforward","text":"The section configures the Store and Forward capability. Please refer to Store and Forward section for more details. Configuration Default Value Description Writable StoreAndForward Enabled false* Indicates whether the Store and Forward capability enabled or disabled Writable StoreAndForward RetryInterval \"5m\"* Indicates the duration of time to wait before retries, aka Forward Writable StoreAndForward MaxRetryCount 10* Indicates whether maximum number of retries of failed data. The failed data is removed after the maximum retries has been exceeded. A value of 0 indicates endless retries.","title":"Writable StoreAndForward"},{"location":"microservices/application/GeneralAppServiceConfig/#writable-pipeline","text":"The section configures the Configurable Function Pipeline which is used only by App Service Configurable. Please refer to App Service Configurable - Getting Started section for more details","title":"Writable Pipeline"},{"location":"microservices/application/GeneralAppServiceConfig/#writable-insecuresecrets","text":"This section defines Insecure Secrets that are used when running is non-secure mode, i.e. when Vault isn't available. This is a dynamic map of configuration, so can empty if no secrets are used or can have as many or few user define secrets. Below are a few that are need if using the indicated capabilities. Configuration Default Value Description Writable InsecureSecrets DB --- This section defines a block of insecure secrets for database connection when Store and Forward is enabled and running is non-secure mode. This section is not required if Store and Forward is not enabled. Writable InsecureSecrets DB path redisdb* Indicates the type of database the insecure secrets are for. redisdb id the DB type name used internally and used to look up the credentials. Writable InsecureSecrets DB Secrets --- This section contains the Secrets key value pair map of database credentials Writable InsecureSecrets DB Secrets username blank* Indicates the value for the username when connecting to the database. When running in non-secure mode it is blank . Writable InsecureSecrets DB Secrets password blank* Indicates the value for the password when connecting to the database. When running in non-secure mode it is blank . Writable InsecureSecrets http --- This section defines a block of insecure secrets for HTTP Export, i.e HTTPPost function Writable InsecureSecrets http path http* Indicates the secrets path for HTTP Export. Must match the secretpath name configured for the HTTPPost function. Writable InsecureSecrets http Secrets --- This section contains the Secrets key value pair map for the HTTPPost function Writable InsecureSecrets http Secrets [headername] undefined* This indicates the HTTP header name and the value to set it to. I.e. the key name you choose is the actual HTTP Header name. The key name must match the secretheadername configured for HTTPPost . The value is what you need the header set to. Writable InsecureSecrets MQTT --- This section defines a block of insecure secrets for MQTT export, i.e. MQTTSecretSend function. Writable InsecureSecrets MQTT path mqtt* Indicates the secrets path for MQTT Export. Must match the secretpath name configured for the MQTTSecretSend function. Writable InsecureSecrets MQTT Secrets --- This section contains the Secrets key value pair map for the MQTTSecretSend function Writable InsecureSecrets MQTT Secrets username blank* Indicates the value for the username when connecting to the MQTT broker using usernamepassword authentication mode. Must be configured to the value the MQTT broker is expecting. Writable InsecureSecrets MQTT Secrets password blank* Indicates the value for the password when connecting to the MQTT broker using usernamepassword authentication mode. Must be configured to the value the MQTT broker is expecting. Writable InsecureSecrets MQTT Secrets cacert blank* Indicates the value (contents) for the CA Certificate when connecting to the MQTT broker using cacert authentication mode. Must be configured to the value the MQTT broker is expecting. Writable InsecureSecrets MQTT Secrets clientcert blank* Indicates the value (contents) for the Client Certificate when connecting to the MQTT broker using clientcert authentication mode. Must be configured to the value the MQTT broker is expecting. Writable InsecureSecrets MQTT Secrets clientkey blank* Indicates the value (contents) for the Client Key when connecting to the MQTT broker using clientcert authentication mode. Must be configured to the value the MQTT broker is expecting.","title":"Writable InsecureSecrets"},{"location":"microservices/application/GeneralAppServiceConfig/#not-writable","text":"The following are additional configuration which are applicable to Application Services that require the service to be restarted after value(s) are changed.","title":"Not Writable"},{"location":"microservices/application/GeneralAppServiceConfig/#database","text":"This optional section contains the connection information. It is only required when the Store and Forward capability is enabled. Note that it has a slightly different format that the database section used in the core services configuration. Configuration Default Value Description Database Type redisdb** Indicates the type of database used. redisdb and mongodb are the only valid types. Database Host localhost** Indicates the hostname for the database Database Port 6379** Indicates the port number for the database Database Timeout \"30s\"** Indicates the connection timeout for the database","title":"Database"},{"location":"microservices/application/GeneralAppServiceConfig/#secretstoreexclusive","text":"This optional section defines the configuration for the Exclusive Secret Store (i.e. Vault) used to Put and Get secrets that are exclusive to the instance of the Application Service. Please refer to the Secrets section for more details. Configuration Default Value Description SecretStoreExclusive Host localhost** Indicates the hostname for the Secret Store SecretStoreExclusive Port 8200** Indicates the port number for the Secret Store SecretStoreExclusive Path Depends on profile used Indicates the base path for the secrets with in the SecretStoreExclusive Protocol https** Indicates the protocol used for the Secret Store SecretStoreExclusive RootCaCertPath /vault/config/pki/ EdgeXFoundryCA/ EdgeXFoundryCA.pem** Indicates the path to the root CA Certificate for Vault SecretStoreExclusive ServerName localhost** Indicates the server name for the Secret Store SecretStoreExclusive TokenFile /vault/config/ assets/ resp-init.json** Indicates the path to the exclusive token for the service to connect to the Secret Store SecretStoreExclusive AdditionalRetryAttempts 10** Indicates the maximum number of failed connection attempts allowed SecretStoreExclusive RetryWaitPeriod \"1s\"** Indicates the wait time between failed connection attempts SecretStoreExclusive Authentication --- The section defines the Secret Store Authentication SecretStoreExclusive Authentication AuthType X-Vault-Token** Indicates the authentication type used when connecting to the Secret Store","title":"SecretStoreExclusive"},{"location":"microservices/application/GeneralAppServiceConfig/#clients","text":"This section defines the clients connect information. Please refer to the Note about Clients section for more details.","title":"Clients"},{"location":"microservices/application/GeneralAppServiceConfig/#binding","text":"This section defines the Trigger binding for incoming data. Configuration Default Value Description Binding Type messagebus** Indicates the Trigger binding type. valid values are messagebus and http Binding SubscribeTopic events** Only used for messagebus binding type Indicates the subscribe topic to use to receive data from the Message Bus Binding PublishTopic blank** Only used for messagebus binding type Indicates the publish topic to use when sending data to the Message Bus","title":"Binding"},{"location":"microservices/application/GeneralAppServiceConfig/#messagebus","text":"This section defines the message bus connect information. Only used for messagebus binding type Configuration Default Value Description MessageBus Type zero** Indicates the type of message bus being used. Valid type are zero , mqtt or redisstreams MessageBus SubscribeHost ... This section defines the connection information for subscribing to the Message Bus MessageBus SubscribeHost Host localhost** Indicates the hostname for subscribing to the Message Bus MessageBus SubscribeHost Port 5563** Indicates the port number for subscribing to the Message Bus MessageBus SubscribeHost Protocol tcp** Indicates the protocol number for subscribing to the Message Bus MessageBus PublishHost ... This section defines the connection information for publishing to the Message Bus MessageBus PublishHost Host \" \" * Indicates the hostname for publishing to the Message Bus MessageBus SubscribeHost Port 5565** Indicates the port number for publishing to the Message Bus MessageBus SubscribeHost Protocol tcp** Indicates the protocol number for publishing to the Message Bus MessageBus Optional ... This section is used for optional configuration specific to the Message Bus type used. Please refer to go-mod-messaging for more details","title":"MessageBus"},{"location":"microservices/application/GeneralAppServiceConfig/#application-settings","text":"[ApplicationSettings] - Is used for custom application settings and is accessed via the ApplicationSettings() API. The ApplicationSettings API returns a map[string] string containing the contents on the ApplicationSetting section of the configuration.toml file. [ApplicationSettings] ApplicationName = \"My Application Service\"","title":"Application Settings"},{"location":"microservices/application/Triggers/","text":"Triggers determine how the app functions pipeline begins execution. The trigger is determined by the configuration.toml file located in the /res directory under a section called [Binding] . Check out the Configuration Section for more information about the toml file. Message Bus Trigger A message bus trigger will execute the pipeline every time data is received off of the configured topic. Type and Topic configuration Here's an example: Type = \"messagebus\" SubscribeTopic = \"events\" PublishTopic = \"\" The Type= is set to \"messagebus\". EdgeX Core Data is publishing data to the events topic. So to receive data from core data, you can set your SubscribeTopic= either to \"\" or \"events\" . You may also designate a PublishTopic= if you wish to publish data back to the message bus. edgexcontext.Complete([]byte outputData) - Will send data back to back to the message bus with the topic specified in the PublishTopic= property Message bus connection configuration The other piece of configuration required are the connection settings: [MessageBus] Type = 'zero' #specifies of message bus (i.e zero for ZMQ) [MessageBus.PublishHost] Host = '*' Port = 5564 Protocol = 'tcp' [MessageBus.SubscribeHost] Host = 'localhost' Port = 5563 Protocol = 'tcp' By default, EdgeX Core Data publishes data to the events topic using ZMQ on port 5563. The publish host is used if publishing data back to the message bus. Important When using ZMQ for the message bus, the Publish Host MUST be different for every topic you wish to publish to since the SDK will bind to the specific port. 5563 for example cannot be used to publish since EdgeX Core Data has bound to that port. Similarly, you cannot have two separate instances of the app functions SDK running publishing to the same port. HTTP Trigger Designating an HTTP trigger will allow the pipeline to be triggered by a RESTful POST call to http://[host]:[port]/api/v1/trigger/ . The body of the POST must be an EdgeX event. edgexcontext.Complete([]byte outputData) - Will send the specified data as the response to the request that originally triggered the HTTP Request.","title":"Triggers"},{"location":"microservices/application/Triggers/#message-bus-trigger","text":"A message bus trigger will execute the pipeline every time data is received off of the configured topic.","title":"Message Bus Trigger"},{"location":"microservices/application/Triggers/#type-and-topic-configuration","text":"Here's an example: Type = \"messagebus\" SubscribeTopic = \"events\" PublishTopic = \"\" The Type= is set to \"messagebus\". EdgeX Core Data is publishing data to the events topic. So to receive data from core data, you can set your SubscribeTopic= either to \"\" or \"events\" . You may also designate a PublishTopic= if you wish to publish data back to the message bus. edgexcontext.Complete([]byte outputData) - Will send data back to back to the message bus with the topic specified in the PublishTopic= property","title":"Type and Topic configuration"},{"location":"microservices/application/Triggers/#message-bus-connection-configuration","text":"The other piece of configuration required are the connection settings: [MessageBus] Type = 'zero' #specifies of message bus (i.e zero for ZMQ) [MessageBus.PublishHost] Host = '*' Port = 5564 Protocol = 'tcp' [MessageBus.SubscribeHost] Host = 'localhost' Port = 5563 Protocol = 'tcp' By default, EdgeX Core Data publishes data to the events topic using ZMQ on port 5563. The publish host is used if publishing data back to the message bus. Important When using ZMQ for the message bus, the Publish Host MUST be different for every topic you wish to publish to since the SDK will bind to the specific port. 5563 for example cannot be used to publish since EdgeX Core Data has bound to that port. Similarly, you cannot have two separate instances of the app functions SDK running publishing to the same port.","title":"Message bus connection configuration"},{"location":"microservices/application/Triggers/#http-trigger","text":"Designating an HTTP trigger will allow the pipeline to be triggered by a RESTful POST call to http://[host]:[port]/api/v1/trigger/ . The body of the POST must be an EdgeX event. edgexcontext.Complete([]byte outputData) - Will send the specified data as the response to the request that originally triggered the HTTP Request.","title":"HTTP Trigger"},{"location":"microservices/configuration/Ch-Configuration/","text":"Configuration and Registry Introduction The purpose of this section is to describe the configuration and service registration capabilities of the EdgeX Foundry platform. In all cases unless otherwise specified, the examples provided are based on the reference architecture built using the Go programming language . Configuration An overview of the architectural decisions made with regard to how configuration works in EdgeX Foundry can be found here . Local Configuration Because EdgeX Foundry may be deployed and run in several different ways, it is important to understand how configuration is loaded and from where it is sourced. Referring to the cmd directory within the edgex-go repository , each service has its own folder. Inside each service folder there is a res directory (short for \"resource\"). There you will find the configuration files in TOML format that defines each service's configuration. A service may support several different configuration profiles, such as a \"docker\" profile. In this case, the configuration file located directly in the res directory should be considered the default configuration profile. Sub-directories will contain configurations appropriate to the respective profile. As of the Geneva release, it is recommended to utilize environment variable overrides rather than creating profiles to override some subset of config values. You can see examples of this in the related docker-compose files . If you choose to utilize profiles as described above, the config profile can be indicated using one of the following command line flags: --profile / -p Taking the core-data service as an example: ./core-data starts the service using the default profile found locally ./core-data --profile=<your profile> starts the service using the docker profile found locally Again, utilizing environment variables for configuration overrides is the recommended path. Config profiles have been deprecated and removed as of the Geneva release. * Seeding Configuration When utilizing the registry to provide centralized configuration management for the EdgeX Foundry microservices, it is necessary to seed the required configuration before starting the services. Each service has the built-in capability to perform this seeding operation. A service will use its local configuration file to initialize the structure and relevant values, and then overlay any environment variable override values as specified. The end result will be seeded into the configuration provider if such is being used. In order for a service to now load the configuration from the configuration provider, we must use one of the following flags: --configProvider / -cp Again, taking the core-data service as an example: ./core-data -cp=http://localhost:8500 will start the service using configuration values found in the provider Configuration Structure Configuration information is organized into a hierarchical structure allowing for a logical grouping of services, as well as versioning, beneath an \"edgex\" namespace at root level of the configuration tree. The root namespace separates EdgeX Foundry-related configuration information from other applications that may be using the same registry. Below the root, sub-nodes facilitate grouping of device services, EdgeX core services, security services, etc. As an example, the top-level nodes shown when one views the configuration registry might be as follows: edgex (root namespace) core (edgex core services) devices (device services) Versioning Incorporating versioning into the configuration hierarchy looks like this. edgex (root namespace) core (edgex core services) 1.0 edgex-core-command edgex-core-data edgex-core-metadata 2.0 devices (device services) 1.0 mqtt-c mqtt-go modbus-go 2.0 appservices (application services) 1.0 AppService-rules-engine 2.0 The versions shown correspond to major versions of the given services. For all minor/patch versions associated with a major version, the respective service keys live under the major version in configuration (such as 1.0). Changes to the configuration structure that may be required during the associated minor version development cycles can only be additive. That is, key names will not be removed or changed once set in a major version, nor will sections of the configuration tree be moved from one place to another. In this way backward compatibility for the lifetime of the major version is maintained. An advantage of grouping all minor/patch versions under a major version involves end-user configuration changes that need to be persisted during an upgrade. A service on startup will not overwrite existing configuration when it runs unless explicitly told to do so via the --overwrite / -o command line flag. Therefore if a user leaves their configuration provider running during an EdgeX Foundry upgrade any customizations will be left in place. Environment variable overrides such as those supplied in the docker-compose for a given release will always override existing content in Consul. Configuration Properties The following table documents configuration properties that are common to all services in the EdgeX Foundry platform. Service-specific properties can be found on the respective documentation page for each service. Configuration Default Value Dependencies Entries in the Writable section of the configuration can be changed on the fly while the service is running if the service is running with the -cp/--configProvider=<url> flag Writable LogLevel INFO * Logs messages set to a level of \"INFO\" or higher The following keys represent the core service-level configuration settings Service MaxResultCount 50000 ** Read data limit per invocation Service BootTimeout 300000 ** Heart beat time in milliseconds Service StartupMsg Logging Service heart beat ** Heart beat message Service Port 48061 ** Micro service port number Service Host localhost ** Micro service host name Service Protocol http ** Micro service host protocol Service ClientMonitor 15000 ** The interval in milliseconds at which any service clients will refresh their endpoint information from the service registry (Consul) Service CheckInterval 10s ** The interval in seconds at which the service registry(Consul) will conduct a health check of this service. Service Timeout 5000 ** Specifies a timeout (in milliseconds) for handling requests The following keys govern logging behavior. With the default values below, all logging will simply be written to StdOut. Logging EnableRemote false ** Facilitates delegation of logging via REST to the support-logging service Logging File [empty string] ** File path to save logging entries. Empty by default. The following keys govern database connectivity and the type of database to use. While not all services require DB connectivity, most do and so this has been included in the common configuration docs. Databases Primary Username [empty string] ** DB user name Databases Primary Password [empty string] ** DB password Databases Primary Host localhost ** DB host name Databases Primary Port 6379 ** DB port number Databases Primary Name coredata ** Database or document store name Databases Primary Timeout 5000 ** DB connection timeout Databases Primary Type redisdb ** DB type Following config only take effect when connecting to the registry for configuration info Registry Host localhost ** Registry host name Registry Port 8500 ** Registry port number Registry Type consul ** Registry implementation type Following config is an example of how service clients are configured. These will of necessity be different in each config because each service has a different set of dependencies, each of which requires a client. Clients Metadata Protocol http ** The protocol to use when building a URI to local the service endpoint Clients Metadata Host localhost ** The host name or IP address where the service is hosted Clients Metadata Port 48081 ** The port exposed by the target service Following config values govern the StartupTimer created at boot for ensuring the service starts in a timely fashion Startup Duration 30 ** The maximum amount of time (in seconds) the service is given to complete the bootstrap phase. Startup Interval 1 ** The amount of time (in seconds) to sleep between retries on a failed dependency such as DB connection Following config values are used when security is enabled and Vault access is required for obtaining secrets, such as database credentials SecretStore Host localhost ** The host name or IP address associated with Vault SecretStore Port 8200 ** The configured port on which Vault is listening SecretStore Path /v1/secret/edgex/coredata/ ** The service-specific path where the secrets are kept. This path will differ according to the given service SecretStore Protocol https ** The protocol to be used when communicating with Vault SecretStore RootCaCertPath /vault/config/pki/EdgeXFoundryCA/ EdgeXFoundryCA.pem ** The default location of the certificate used to communicate with Vault over a secure channel SecretStore ServerName localhost ** The name of the server where Vault is located. SecretStore TokenFile /vault/config/assets/resp-init.json ** Fully-qualified path to the location of the Vault root token. SecretStore AdditionalRetryAttempts 10 ** Number of attemtps to retry retrieving secrets before failing to start the service SecretStore RetryWaitPeriod 1s ** Amount of time to wait before attempting another connection to Vault SecretStore Authentication AuthType X-Vault-Token ** A header used to indicate how the given service will authenticate with Vault *means the configuration value can be changed on the fly if using a configuration provider (like Consul). **means the configuration value can be changed but the service must be restarted. Readable vs Writable Settings Within a given service's configuration, there are keys whose values can be edited and change the behavior of the service while it is running versus those that are effectively read-only. These writable settings are grouped under a given service key. For example, the top-level groupings for edgex-core-data are: /edgex/core/1.0/edgex-core-data/Clients /edgex/core/1.0/edgex-core-data/Databases /edgex/core/1.0/edgex-core-data/Logging /edgex/core/1.0/edgex-core-data/MessageQueue /edgex/core/1.0/edgex-core-data/Registry /edgex/core/1.0/edgex-core-data/SecretStore /edgex/core/1.0/edgex-core-data/Service /edgex/core/1.0/edgex-core-data/Writable Any configuration settings found in the Writable section shown above may be changed and affect a service's behavior without a restart. Any modifications to the other settings would require a restart. NOTE: As of the Geneva release, the support-logging service is deprecated. In the Readable section of each service's configuration there is currently a Logging section shown below. The recommendation is that you not change the default values in this section unless you have a specific reason for doing so. # Remote and file logging disabled so only stdout logging is used [Logging] EnableRemote = false File = '' Configuration Provider You can supply and manage configuration in a centralized manner by utilizing the -cp/--configProvider=<url> flag when starting a service. If the flag is provided and pointed to an application such as HashiCorp's Consul , the service will bootstrap its configuration into Consul if it doesn't exist. If configuration does already exist, it will load the content from the given location applying any environment variables overrides of which the service is aware. Integration with the configuration provider is handled through the go-mod-configuration module referenced by all services. Registry Provider The registry refers to any platform you may use for service discovery. For the EdgeX Foundry reference implementation, the default provider for this responsibility is Consul. Integration with the registry is handled through the go-mod-registry module referenced by all services. Introduction to Registry The objective of the registry is to enable microservices to find and to communicate with each other. When each microservice starts up, it registers itself with the registry, and the registry continues checking its availability periodically via a specified health check endpoint. When one microservice needs to connect to another one, it connects to the registry to retrieve the available host name and port number of the target microservice and then invokes the target microservice. The following figure shows the basic flow. Consul is the default registry implementation and provides native features for service registration, service discovery, and health checking. Please refer to the Consul official web site for more information: https://www.consul.io Physically, the \"registry\" and \"configuration\" management services are combined and running on the same Consul server node. Web User Interface A web user interface is also provided by Consul natively. Users can view the available service list and their health status through the web user interface. The web user interface is available at the /ui path on the same port as the HTTP API. By default this is http://localhost:8500/ui . For more detail, please see: https://www.consul.io/intro/getting-started/ui.html Running on Docker For ease of use to install and update, the microservices of EdgeX Foundry are also published as Docker images onto Docker Hub, including Registry: https://hub.docker.com/r/edgexfoundry/docker-core-consul/ After the Docker engine is ready, users can download the latest Consul image by the docker pull command: docker pull edgexfoundry/docker-core-consul Then, startup Consul using Docker container by the Docker run command: docker run -p 8400:8400 -p 8500:8500 -p 8600:8600 --name edgex-core-consul --hostname edgex-core-consul -d edgexfoundry/docker-core-consul These are the command steps to start up Consul and import the default configuration data: login to Docker Hub: \\$ docker login A Docker network is needed to enable one Docker container to communicate with another. This is preferred over use of --links that establishes a client-server relationship: \\$ docker network create edgex-network Create a Docker volume container for EdgeX Foundry: \\$ docker run -it --name edgex-files --net=edgex-network -v /data/db -v /edgex/logs -v /consul/config -v /consul/data -d edgexfoundry/docker-edgex-volume Create the Consul container: \\$ docker run -p 8400:8400 -p 8500:8500 -p 8600:8600 --name edgex-core-consul --hostname edgex-core-consul --net=edgex-network --volumes-from edgex-files -d edgexfoundry/docker-core-consul Verify the result: http://localhost:8500/ui Running on Local Machine To run Consul on the local machine, requires the following steps: Download the binary from Consul official website: https://www.consul.io/downloads.html . Please choose the correct binary file according to the operation system. Set up the environment variable. Please refer to https://www.consul.io/intro/getting-started/install.html . Execute the following command: \\$ consul agent -data-dir \\${DATA_FOLDER} -ui -advertise 127.0.0.1 -server -bootstrap-expect 1 \\${DATA_FOLDER} could be any folder to put the data files of Consul, and it needs the read/write permission. Verify the result: http://localhost:8500/ui","title":"Configuration and Registry"},{"location":"microservices/configuration/Ch-Configuration/#configuration-and-registry","text":"","title":"Configuration and Registry"},{"location":"microservices/configuration/Ch-Configuration/#introduction","text":"The purpose of this section is to describe the configuration and service registration capabilities of the EdgeX Foundry platform. In all cases unless otherwise specified, the examples provided are based on the reference architecture built using the Go programming language .","title":"Introduction"},{"location":"microservices/configuration/Ch-Configuration/#configuration","text":"An overview of the architectural decisions made with regard to how configuration works in EdgeX Foundry can be found here .","title":"Configuration"},{"location":"microservices/configuration/Ch-Configuration/#local-configuration","text":"Because EdgeX Foundry may be deployed and run in several different ways, it is important to understand how configuration is loaded and from where it is sourced. Referring to the cmd directory within the edgex-go repository , each service has its own folder. Inside each service folder there is a res directory (short for \"resource\"). There you will find the configuration files in TOML format that defines each service's configuration. A service may support several different configuration profiles, such as a \"docker\" profile. In this case, the configuration file located directly in the res directory should be considered the default configuration profile. Sub-directories will contain configurations appropriate to the respective profile. As of the Geneva release, it is recommended to utilize environment variable overrides rather than creating profiles to override some subset of config values. You can see examples of this in the related docker-compose files . If you choose to utilize profiles as described above, the config profile can be indicated using one of the following command line flags: --profile / -p Taking the core-data service as an example: ./core-data starts the service using the default profile found locally ./core-data --profile=<your profile> starts the service using the docker profile found locally Again, utilizing environment variables for configuration overrides is the recommended path. Config profiles have been deprecated and removed as of the Geneva release. *","title":"Local Configuration"},{"location":"microservices/configuration/Ch-Configuration/#seeding-configuration","text":"When utilizing the registry to provide centralized configuration management for the EdgeX Foundry microservices, it is necessary to seed the required configuration before starting the services. Each service has the built-in capability to perform this seeding operation. A service will use its local configuration file to initialize the structure and relevant values, and then overlay any environment variable override values as specified. The end result will be seeded into the configuration provider if such is being used. In order for a service to now load the configuration from the configuration provider, we must use one of the following flags: --configProvider / -cp Again, taking the core-data service as an example: ./core-data -cp=http://localhost:8500 will start the service using configuration values found in the provider","title":"Seeding Configuration"},{"location":"microservices/configuration/Ch-Configuration/#configuration-structure","text":"Configuration information is organized into a hierarchical structure allowing for a logical grouping of services, as well as versioning, beneath an \"edgex\" namespace at root level of the configuration tree. The root namespace separates EdgeX Foundry-related configuration information from other applications that may be using the same registry. Below the root, sub-nodes facilitate grouping of device services, EdgeX core services, security services, etc. As an example, the top-level nodes shown when one views the configuration registry might be as follows: edgex (root namespace) core (edgex core services) devices (device services)","title":"Configuration Structure"},{"location":"microservices/configuration/Ch-Configuration/#versioning","text":"Incorporating versioning into the configuration hierarchy looks like this. edgex (root namespace) core (edgex core services) 1.0 edgex-core-command edgex-core-data edgex-core-metadata 2.0 devices (device services) 1.0 mqtt-c mqtt-go modbus-go 2.0 appservices (application services) 1.0 AppService-rules-engine 2.0 The versions shown correspond to major versions of the given services. For all minor/patch versions associated with a major version, the respective service keys live under the major version in configuration (such as 1.0). Changes to the configuration structure that may be required during the associated minor version development cycles can only be additive. That is, key names will not be removed or changed once set in a major version, nor will sections of the configuration tree be moved from one place to another. In this way backward compatibility for the lifetime of the major version is maintained. An advantage of grouping all minor/patch versions under a major version involves end-user configuration changes that need to be persisted during an upgrade. A service on startup will not overwrite existing configuration when it runs unless explicitly told to do so via the --overwrite / -o command line flag. Therefore if a user leaves their configuration provider running during an EdgeX Foundry upgrade any customizations will be left in place. Environment variable overrides such as those supplied in the docker-compose for a given release will always override existing content in Consul.","title":"Versioning"},{"location":"microservices/configuration/Ch-Configuration/#configuration-properties","text":"The following table documents configuration properties that are common to all services in the EdgeX Foundry platform. Service-specific properties can be found on the respective documentation page for each service. Configuration Default Value Dependencies Entries in the Writable section of the configuration can be changed on the fly while the service is running if the service is running with the -cp/--configProvider=<url> flag Writable LogLevel INFO * Logs messages set to a level of \"INFO\" or higher The following keys represent the core service-level configuration settings Service MaxResultCount 50000 ** Read data limit per invocation Service BootTimeout 300000 ** Heart beat time in milliseconds Service StartupMsg Logging Service heart beat ** Heart beat message Service Port 48061 ** Micro service port number Service Host localhost ** Micro service host name Service Protocol http ** Micro service host protocol Service ClientMonitor 15000 ** The interval in milliseconds at which any service clients will refresh their endpoint information from the service registry (Consul) Service CheckInterval 10s ** The interval in seconds at which the service registry(Consul) will conduct a health check of this service. Service Timeout 5000 ** Specifies a timeout (in milliseconds) for handling requests The following keys govern logging behavior. With the default values below, all logging will simply be written to StdOut. Logging EnableRemote false ** Facilitates delegation of logging via REST to the support-logging service Logging File [empty string] ** File path to save logging entries. Empty by default. The following keys govern database connectivity and the type of database to use. While not all services require DB connectivity, most do and so this has been included in the common configuration docs. Databases Primary Username [empty string] ** DB user name Databases Primary Password [empty string] ** DB password Databases Primary Host localhost ** DB host name Databases Primary Port 6379 ** DB port number Databases Primary Name coredata ** Database or document store name Databases Primary Timeout 5000 ** DB connection timeout Databases Primary Type redisdb ** DB type Following config only take effect when connecting to the registry for configuration info Registry Host localhost ** Registry host name Registry Port 8500 ** Registry port number Registry Type consul ** Registry implementation type Following config is an example of how service clients are configured. These will of necessity be different in each config because each service has a different set of dependencies, each of which requires a client. Clients Metadata Protocol http ** The protocol to use when building a URI to local the service endpoint Clients Metadata Host localhost ** The host name or IP address where the service is hosted Clients Metadata Port 48081 ** The port exposed by the target service Following config values govern the StartupTimer created at boot for ensuring the service starts in a timely fashion Startup Duration 30 ** The maximum amount of time (in seconds) the service is given to complete the bootstrap phase. Startup Interval 1 ** The amount of time (in seconds) to sleep between retries on a failed dependency such as DB connection Following config values are used when security is enabled and Vault access is required for obtaining secrets, such as database credentials SecretStore Host localhost ** The host name or IP address associated with Vault SecretStore Port 8200 ** The configured port on which Vault is listening SecretStore Path /v1/secret/edgex/coredata/ ** The service-specific path where the secrets are kept. This path will differ according to the given service SecretStore Protocol https ** The protocol to be used when communicating with Vault SecretStore RootCaCertPath /vault/config/pki/EdgeXFoundryCA/ EdgeXFoundryCA.pem ** The default location of the certificate used to communicate with Vault over a secure channel SecretStore ServerName localhost ** The name of the server where Vault is located. SecretStore TokenFile /vault/config/assets/resp-init.json ** Fully-qualified path to the location of the Vault root token. SecretStore AdditionalRetryAttempts 10 ** Number of attemtps to retry retrieving secrets before failing to start the service SecretStore RetryWaitPeriod 1s ** Amount of time to wait before attempting another connection to Vault SecretStore Authentication AuthType X-Vault-Token ** A header used to indicate how the given service will authenticate with Vault *means the configuration value can be changed on the fly if using a configuration provider (like Consul). **means the configuration value can be changed but the service must be restarted.","title":"Configuration Properties"},{"location":"microservices/configuration/Ch-Configuration/#readable-vs-writable-settings","text":"Within a given service's configuration, there are keys whose values can be edited and change the behavior of the service while it is running versus those that are effectively read-only. These writable settings are grouped under a given service key. For example, the top-level groupings for edgex-core-data are: /edgex/core/1.0/edgex-core-data/Clients /edgex/core/1.0/edgex-core-data/Databases /edgex/core/1.0/edgex-core-data/Logging /edgex/core/1.0/edgex-core-data/MessageQueue /edgex/core/1.0/edgex-core-data/Registry /edgex/core/1.0/edgex-core-data/SecretStore /edgex/core/1.0/edgex-core-data/Service /edgex/core/1.0/edgex-core-data/Writable Any configuration settings found in the Writable section shown above may be changed and affect a service's behavior without a restart. Any modifications to the other settings would require a restart. NOTE: As of the Geneva release, the support-logging service is deprecated. In the Readable section of each service's configuration there is currently a Logging section shown below. The recommendation is that you not change the default values in this section unless you have a specific reason for doing so. # Remote and file logging disabled so only stdout logging is used [Logging] EnableRemote = false File = ''","title":"Readable vs Writable Settings"},{"location":"microservices/configuration/Ch-Configuration/#configuration-provider","text":"You can supply and manage configuration in a centralized manner by utilizing the -cp/--configProvider=<url> flag when starting a service. If the flag is provided and pointed to an application such as HashiCorp's Consul , the service will bootstrap its configuration into Consul if it doesn't exist. If configuration does already exist, it will load the content from the given location applying any environment variables overrides of which the service is aware. Integration with the configuration provider is handled through the go-mod-configuration module referenced by all services.","title":"Configuration Provider"},{"location":"microservices/configuration/Ch-Configuration/#registry-provider","text":"The registry refers to any platform you may use for service discovery. For the EdgeX Foundry reference implementation, the default provider for this responsibility is Consul. Integration with the registry is handled through the go-mod-registry module referenced by all services.","title":"Registry Provider"},{"location":"microservices/configuration/Ch-Configuration/#introduction-to-registry","text":"The objective of the registry is to enable microservices to find and to communicate with each other. When each microservice starts up, it registers itself with the registry, and the registry continues checking its availability periodically via a specified health check endpoint. When one microservice needs to connect to another one, it connects to the registry to retrieve the available host name and port number of the target microservice and then invokes the target microservice. The following figure shows the basic flow. Consul is the default registry implementation and provides native features for service registration, service discovery, and health checking. Please refer to the Consul official web site for more information: https://www.consul.io Physically, the \"registry\" and \"configuration\" management services are combined and running on the same Consul server node.","title":"Introduction to Registry"},{"location":"microservices/configuration/Ch-Configuration/#web-user-interface","text":"A web user interface is also provided by Consul natively. Users can view the available service list and their health status through the web user interface. The web user interface is available at the /ui path on the same port as the HTTP API. By default this is http://localhost:8500/ui . For more detail, please see: https://www.consul.io/intro/getting-started/ui.html","title":"Web User Interface"},{"location":"microservices/configuration/Ch-Configuration/#running-on-docker","text":"For ease of use to install and update, the microservices of EdgeX Foundry are also published as Docker images onto Docker Hub, including Registry: https://hub.docker.com/r/edgexfoundry/docker-core-consul/ After the Docker engine is ready, users can download the latest Consul image by the docker pull command: docker pull edgexfoundry/docker-core-consul Then, startup Consul using Docker container by the Docker run command: docker run -p 8400:8400 -p 8500:8500 -p 8600:8600 --name edgex-core-consul --hostname edgex-core-consul -d edgexfoundry/docker-core-consul These are the command steps to start up Consul and import the default configuration data: login to Docker Hub: \\$ docker login A Docker network is needed to enable one Docker container to communicate with another. This is preferred over use of --links that establishes a client-server relationship: \\$ docker network create edgex-network Create a Docker volume container for EdgeX Foundry: \\$ docker run -it --name edgex-files --net=edgex-network -v /data/db -v /edgex/logs -v /consul/config -v /consul/data -d edgexfoundry/docker-edgex-volume Create the Consul container: \\$ docker run -p 8400:8400 -p 8500:8500 -p 8600:8600 --name edgex-core-consul --hostname edgex-core-consul --net=edgex-network --volumes-from edgex-files -d edgexfoundry/docker-core-consul Verify the result: http://localhost:8500/ui","title":"Running on Docker"},{"location":"microservices/configuration/Ch-Configuration/#running-on-local-machine","text":"To run Consul on the local machine, requires the following steps: Download the binary from Consul official website: https://www.consul.io/downloads.html . Please choose the correct binary file according to the operation system. Set up the environment variable. Please refer to https://www.consul.io/intro/getting-started/install.html . Execute the following command: \\$ consul agent -data-dir \\${DATA_FOLDER} -ui -advertise 127.0.0.1 -server -bootstrap-expect 1 \\${DATA_FOLDER} could be any folder to put the data files of Consul, and it needs the read/write permission. Verify the result: http://localhost:8500/ui","title":"Running on Local Machine"},{"location":"microservices/core/command/Ch-Command/","text":"Command Introduction The Core Services Layer microservice Command (often called the Command and Control microservice) enables the issuance of commands or actions to devices and sensors on behalf of: other microservices within EdgeX Foundry (for example, a local edge analytics or rules engine microservice) other applications that may exist on the same system with EdgeX Foundry (for example, a system management agent that needs to shutoff a sensor) To any external system that needs to command those devices (for example, a cloud-based application that had determined the need to modify the settings on a collection of devices) The Command microservice exposes the commands in a common, normalized way to simplify communications with the devices. Commands to devices are made through the command GET, a request for data from the device or sensor, and the command PUT, a request to take action or receive new settings or data from EdgeX Foundry. The Command microservice gets its knowledge about the devices and sensors from the Metadata service. The Command service always relays commands and actions to the devices and sensors through the Device Service and never communicates directly to a device or sensor. Therefore, the Command microservice is a translator of command or action requests from the north side of EdgeX Foundry, such as the rules engine and export facilities, to the protocol-specific device or sensor and associated Device Service side of EdgeX Foundry and the gateway. The Command service provides a layer of protection around devices and sensors by not allowing unwarranted interaction with the devices and sensors through the Device Service. Data Dictionary Class Name Description CommandResponse Contains the target and parameters and expected responses, that describe a REST call. High Level Interaction Diagrams The two following High Level Diagrams show: EdgeX Foundry Command PUT Request EdgeX Foundry Command Request for Devices and Their Available Commands EdgeX Foundry Command PUT Request EdgeX Foundry Command Request for Devices and Their Available Commands","title":"Command"},{"location":"microservices/core/command/Ch-Command/#command","text":"","title":"Command"},{"location":"microservices/core/command/Ch-Command/#introduction","text":"The Core Services Layer microservice Command (often called the Command and Control microservice) enables the issuance of commands or actions to devices and sensors on behalf of: other microservices within EdgeX Foundry (for example, a local edge analytics or rules engine microservice) other applications that may exist on the same system with EdgeX Foundry (for example, a system management agent that needs to shutoff a sensor) To any external system that needs to command those devices (for example, a cloud-based application that had determined the need to modify the settings on a collection of devices) The Command microservice exposes the commands in a common, normalized way to simplify communications with the devices. Commands to devices are made through the command GET, a request for data from the device or sensor, and the command PUT, a request to take action or receive new settings or data from EdgeX Foundry. The Command microservice gets its knowledge about the devices and sensors from the Metadata service. The Command service always relays commands and actions to the devices and sensors through the Device Service and never communicates directly to a device or sensor. Therefore, the Command microservice is a translator of command or action requests from the north side of EdgeX Foundry, such as the rules engine and export facilities, to the protocol-specific device or sensor and associated Device Service side of EdgeX Foundry and the gateway. The Command service provides a layer of protection around devices and sensors by not allowing unwarranted interaction with the devices and sensors through the Device Service.","title":"Introduction"},{"location":"microservices/core/command/Ch-Command/#data-dictionary","text":"Class Name Description CommandResponse Contains the target and parameters and expected responses, that describe a REST call.","title":"Data Dictionary"},{"location":"microservices/core/command/Ch-Command/#high-level-interaction-diagrams","text":"The two following High Level Diagrams show: EdgeX Foundry Command PUT Request EdgeX Foundry Command Request for Devices and Their Available Commands EdgeX Foundry Command PUT Request EdgeX Foundry Command Request for Devices and Their Available Commands","title":"High Level Interaction Diagrams"},{"location":"microservices/core/data/Ch-CoreData/","text":"Core Data Introduction The Core Data microservice provides a centralized persistence facility for data readings collected by devices and sensors. Device services for devices and sensors that collect data, call on the Core Data service to store the device and sensor data on the edge system (such as in a gateway) until the data can be moved \"north\" and then exported to Enterprise and cloud systems. Other services, such as a Scheduling services, within EdgeX Foundry and potentially outside of EdgeX Foundry, access the device and sensor data stored on the gateway only through the Core Data service. Core Data provides a degree of security and protection of the data collected by devices and sensors while the data is at the edge. Core Data uses a REST API for moving data into and out of the local storage. In the future, the microservice could be expandable to allow data to be accessed via other protocols such as MQTT, AMQP, and so forth. Core Data moves data to the Export Service layer via ZeroMQ by default. An alternate configuration of the Core Data microservice allows the data to be distributed to the Export Services via MQTT, but would also require the installation of a broker such as ActiveMQ. The Rules Engine microservice receives its data from the Export Distribution microservice by default. Where latency or volume are of concern, an alternate configuration of the Rules Engine microservices allows it to also get its data directly from Core Data via ZeroMQ (it becomes a second subscriber to the same Export Services ZeroMQ distribution channel). Core Data \"Streaming\" By default, Core Data does persist all data collected by devices and sensors sent to it. However, when the data is too sensitive to be stored at the edge, or the need is not present for data at the edge to be used by other services locally (e.g. by an analytics microservice), the data can be \"streamed\" through Core Data without persisting it. A configuration change to Core Data (persist.data=false) has Core Data send data to the Export Service, through message queue, without persisting the data locally. This option has the advantage of reducing latency through this layer and storage needs at the network edge, but the cost is having no historical data to use for operations based on changes over time, and only minimal device actuation decisions, based on single event data, at the edge. Data Model The following diagram shows the Data Model for Core Data. Data Dictionary Class Description Event ID Device Identifier Collection of Readings Event has a one-to-many relationship with Reading. Reading name-value pair Examples: \"temp 62\" \"rpm 3000\" The value is an Integer, Decimal, String, or Boolean. The name a value descriptor reference. The value descriptor defines information about the information the Reading should convey. Value Descriptor This specifies a folder to put the log files. High Level Interaction Diagrams The two following High Level Interaction Diagrams show: EdgeX Foundry Core Data add device or sensor readings EdgeX Foundry Core Data request event reading or data for a device Core Data Add Device or Sensor Readings Core Data Request Event Reading or Data for a Device Configuration Properties Please refer to the general Configuration documentation for configuration properties common across all services. In order to support publishing events via message bus, Core-Data has the following additional configuration section. Changes made to any of these properties while the service is running will not be reflected until the service is restarted. Configuration Default Value Dependencies Entries in the MessageQueue section of the configuration allow for publication of events to a message bus MessageQueue Protocol tcp Indicates the connectivity protocol to use to use the bus. MessageQueue Host * Indicates the host of the messaging broker, if applicable. MessageQueue Port 5563 Indicates the port to use when publishing a message. MessageQueue Type zero Indicates the type of messaging library to use. Currently this is ZeroMQ by default. Refer to the go-mod-messaging module for more information. MessageQueue Topic events Indicates the topic to which messages should be published. The following are additional entries in Writable section applicable to the Core-Data service. Writable DeviceUpdateLastConnected false Indicates whether the \"Device Last Connected\" timestamp should be updated with each event received from a given device. Writable MetaDataCheck false Indicates whether a call to Core-Metadata should be made to check the validity of the device associated with the incoming event. Writable PersistData true Indicates whether sensor event data should be persisted to the database. If set to \"false\", then Core-Data is nothing but a pass-through. Writable ServiceUpdateLastConnected false Indicates whether the \"Device Service Last Connected\" timestamp should be updated with each event received from a given device. Writable ValidateCheck false Indicates whether a call to Core-Metadata should be made for validation of value descriptors assigned to each reading in an event. Writable ChecksumAlgo xxHash Identifies the algorithm to use when calculating an event's checksum.","title":"Core Data"},{"location":"microservices/core/data/Ch-CoreData/#core-data","text":"","title":"Core Data"},{"location":"microservices/core/data/Ch-CoreData/#introduction","text":"The Core Data microservice provides a centralized persistence facility for data readings collected by devices and sensors. Device services for devices and sensors that collect data, call on the Core Data service to store the device and sensor data on the edge system (such as in a gateway) until the data can be moved \"north\" and then exported to Enterprise and cloud systems. Other services, such as a Scheduling services, within EdgeX Foundry and potentially outside of EdgeX Foundry, access the device and sensor data stored on the gateway only through the Core Data service. Core Data provides a degree of security and protection of the data collected by devices and sensors while the data is at the edge. Core Data uses a REST API for moving data into and out of the local storage. In the future, the microservice could be expandable to allow data to be accessed via other protocols such as MQTT, AMQP, and so forth. Core Data moves data to the Export Service layer via ZeroMQ by default. An alternate configuration of the Core Data microservice allows the data to be distributed to the Export Services via MQTT, but would also require the installation of a broker such as ActiveMQ. The Rules Engine microservice receives its data from the Export Distribution microservice by default. Where latency or volume are of concern, an alternate configuration of the Rules Engine microservices allows it to also get its data directly from Core Data via ZeroMQ (it becomes a second subscriber to the same Export Services ZeroMQ distribution channel).","title":"Introduction"},{"location":"microservices/core/data/Ch-CoreData/#core-data-streaming","text":"By default, Core Data does persist all data collected by devices and sensors sent to it. However, when the data is too sensitive to be stored at the edge, or the need is not present for data at the edge to be used by other services locally (e.g. by an analytics microservice), the data can be \"streamed\" through Core Data without persisting it. A configuration change to Core Data (persist.data=false) has Core Data send data to the Export Service, through message queue, without persisting the data locally. This option has the advantage of reducing latency through this layer and storage needs at the network edge, but the cost is having no historical data to use for operations based on changes over time, and only minimal device actuation decisions, based on single event data, at the edge.","title":"Core Data \"Streaming\""},{"location":"microservices/core/data/Ch-CoreData/#data-model","text":"The following diagram shows the Data Model for Core Data.","title":"Data Model"},{"location":"microservices/core/data/Ch-CoreData/#data-dictionary","text":"Class Description Event ID Device Identifier Collection of Readings Event has a one-to-many relationship with Reading. Reading name-value pair Examples: \"temp 62\" \"rpm 3000\" The value is an Integer, Decimal, String, or Boolean. The name a value descriptor reference. The value descriptor defines information about the information the Reading should convey. Value Descriptor This specifies a folder to put the log files.","title":"Data Dictionary"},{"location":"microservices/core/data/Ch-CoreData/#high-level-interaction-diagrams","text":"The two following High Level Interaction Diagrams show: EdgeX Foundry Core Data add device or sensor readings EdgeX Foundry Core Data request event reading or data for a device Core Data Add Device or Sensor Readings Core Data Request Event Reading or Data for a Device","title":"High Level Interaction Diagrams"},{"location":"microservices/core/data/Ch-CoreData/#configuration-properties","text":"Please refer to the general Configuration documentation for configuration properties common across all services. In order to support publishing events via message bus, Core-Data has the following additional configuration section. Changes made to any of these properties while the service is running will not be reflected until the service is restarted. Configuration Default Value Dependencies Entries in the MessageQueue section of the configuration allow for publication of events to a message bus MessageQueue Protocol tcp Indicates the connectivity protocol to use to use the bus. MessageQueue Host * Indicates the host of the messaging broker, if applicable. MessageQueue Port 5563 Indicates the port to use when publishing a message. MessageQueue Type zero Indicates the type of messaging library to use. Currently this is ZeroMQ by default. Refer to the go-mod-messaging module for more information. MessageQueue Topic events Indicates the topic to which messages should be published. The following are additional entries in Writable section applicable to the Core-Data service. Writable DeviceUpdateLastConnected false Indicates whether the \"Device Last Connected\" timestamp should be updated with each event received from a given device. Writable MetaDataCheck false Indicates whether a call to Core-Metadata should be made to check the validity of the device associated with the incoming event. Writable PersistData true Indicates whether sensor event data should be persisted to the database. If set to \"false\", then Core-Data is nothing but a pass-through. Writable ServiceUpdateLastConnected false Indicates whether the \"Device Service Last Connected\" timestamp should be updated with each event received from a given device. Writable ValidateCheck false Indicates whether a call to Core-Metadata should be made for validation of value descriptors assigned to each reading in an event. Writable ChecksumAlgo xxHash Identifies the algorithm to use when calculating an event's checksum.","title":"Configuration Properties"},{"location":"microservices/core/metadata/Ch-Metadata/","text":"Metadata Introduction The Metadata microservice has the knowledge about the devices and sensors and how to communicate with them that is used by the other services, such as Core Data, Command, and so forth. Specifically, Metadata has the following abilities: Manages information about the devices and sensors connected to, and operated by, EdgeX Foundry Knows the type, and organization of data reported by the devices and sensors Knows how to command the devices and sensors The Metadata does not do the following activities: Does not do, and is not responsible for actual data collection from devices and sensors, which is performed by Device Services and Core Data Does not do, and is not responsible for issuing commands to the devices and sensors, which is performed by Command and Device Service General characteristics about Devices, the data they provide, and how to command them is shown in Device Profiles in EdgeX Foundry. A Device Profile can be thought of as a template of a type or classification of Device. For example, a device profile for BACnet thermostats provides general characteristics for the types of data a BACnet thermostat sends, such as current temperature, and which types of commands or actions can be sent to the BACnet thermostat, such as cooling set point, or heating set point. Therefore, Device Profiles are the first item that the Metadata service must be able to store or manage in local persistence, and provide to the other services of EdgeX Foundry. Data about actual devices and sensors is another type of information that the Metadata microservice stores and manages. Each specific device and sensor that is managed by EdgeX Foundry must be registered with Metadata and have a unique ID associated to it. Information, such as the device's or sensor's address is stored with that identifier. Each device and sensor is also associated to a device profile. This association enables Metadata to apply generic knowledge provided by the device profile to each device and sensor. For example, a specific device such as the BACNet thermostat located in the CTO Solutions lab in Dell's building, is associated to the BACnet thermostat device profile described above and this connection would imply that this specific BACnet thermostat provides current temperature data and responds to commands to set the cooling and heating points. {.align-center} Metadata stores and manages information about the device services that serve as EdgeX Foundry's interfaces to the actual devices and sensors. Device services are other microservices that communicate directly with the device or sensor in the device or sensor protocol of choice, and normalize information and communications with the device or sensor for the rest of EdgeX Foundry. A single Device Service facilitates the communications between EdgeX Foundry and one or more actual devices or sensors. Typically, a Device Service is built to communicate through a particular protocol with devices and sensors that use that protocol. For example, a Modbus Device Service that facilitates the communications among all types of Modbus devices such as motor controllers, proximity sensors, thermostats, power meters, and so forth. {.align-center} Data Models Metadata Command Model Metadata Device and Device Profile Model Metadata Device Profile Model Metadata Provision Watcher Model Data Dictionary Class Name Description Dependencies Action Contains the target and parameters and expected responses, that describe a REST call. Addressable The metadata required to make a request to an EdgeX Foundry target. For example, the Addressable could be HTTP and URL address details to reach a device service by REST and might include attributes such as HTTP Protocol, URL host of edgex-modbus-device-service, port of 49090. AdminState An object\u2019s current administrative state of \u201cLocked\u201d or\u201dUnlocked.\u201d CallbackAlert The object used by the system to alert regarding a change to a system object. Command The REST description of an interface. Device The object that contains information about the state, position, reachability, and methods of interfacing with a Device Top Level object DeviceManager An object that groups other Devices and groups of Devices. DeviceResource The atomic description of a particular protocol level interface for a class of Devices. DeviceProfile The description of both the protocol level interface, device service interface, and mapping and interpretation logic that describe communication to a class of devices. Top Level object DeviceReport DeviceService The current state and reachability information for a registered Device Services OperatingState An object\u2019s current operating state of \u201cEnabled\u201d or \u201cDisabled.\u201d ProfileProperty The transformation, constraint, and unit properties for a class of Device data. ProfileResource The set of operations that is executed by a Service for a particular Command. PropertyValue The transformation and constraint properties for a class of data. ProvisionWatcher The metadata used by a Service for automatically provisioning matching Devices. ResourceOperation An Operation or set of Operations executed by the Device Service on a Device. Response A description of a possible REST response for a Command. Schedule An object defining a timer or alarm. Top Level object ScheduleEvent The action taken by a Service when the schedule triggers. Top Level object Service The current state and reachability information registered for a Service. Units The unit metadata about a class of Device data. High Level Interaction Diagrams Sequence diagrams for some of the more critical or complex events regarding Metadata. The three following High Level Interaction Diagrams show: EdgeX Foundry Metadata Add a New Device Profile (Step 1 to provisioning a new device) EdgeX Foundry Metadata Add a New Device Profile (Step 2 to provisioning a new device) EdgeX Foundry Metadata Device Service Startup Metadata Add a New Device Profile (Step 1 to provisioning a new device) Configuration Properties The following are extra configuration parameters specific to the Core-Metadata service. Please refer to the general Configuration documentation for configuration properties common across all services. Configuration Default Value Dependencies The following are additional entries in Writable section applicable to the Core-Metadata service. Writable EnableValueDescriptorManagement false A flag that indicates whether value descriptors should be atomically managed when device profiles are added or updated. The following keys define default behavior and content for interaction with the support-notifications service. Notifications PostDeviceChanges true Indicates whether a notification should be sent when a device's definition is changed. Notifications Slug device-change- The stem of the \"topic\" assigned to the notification Notifications Content Device update: The stem of the message sent to the support-notifications service. Notifications Sender core-metadata Identifies the sender of the notification Notifications Description Metadata device notice A short description of the notification being sent. Notifications Label metadata A descriptive label for the notification.","title":"Metadata"},{"location":"microservices/core/metadata/Ch-Metadata/#metadata","text":"","title":"Metadata"},{"location":"microservices/core/metadata/Ch-Metadata/#introduction","text":"The Metadata microservice has the knowledge about the devices and sensors and how to communicate with them that is used by the other services, such as Core Data, Command, and so forth. Specifically, Metadata has the following abilities: Manages information about the devices and sensors connected to, and operated by, EdgeX Foundry Knows the type, and organization of data reported by the devices and sensors Knows how to command the devices and sensors The Metadata does not do the following activities: Does not do, and is not responsible for actual data collection from devices and sensors, which is performed by Device Services and Core Data Does not do, and is not responsible for issuing commands to the devices and sensors, which is performed by Command and Device Service General characteristics about Devices, the data they provide, and how to command them is shown in Device Profiles in EdgeX Foundry. A Device Profile can be thought of as a template of a type or classification of Device. For example, a device profile for BACnet thermostats provides general characteristics for the types of data a BACnet thermostat sends, such as current temperature, and which types of commands or actions can be sent to the BACnet thermostat, such as cooling set point, or heating set point. Therefore, Device Profiles are the first item that the Metadata service must be able to store or manage in local persistence, and provide to the other services of EdgeX Foundry. Data about actual devices and sensors is another type of information that the Metadata microservice stores and manages. Each specific device and sensor that is managed by EdgeX Foundry must be registered with Metadata and have a unique ID associated to it. Information, such as the device's or sensor's address is stored with that identifier. Each device and sensor is also associated to a device profile. This association enables Metadata to apply generic knowledge provided by the device profile to each device and sensor. For example, a specific device such as the BACNet thermostat located in the CTO Solutions lab in Dell's building, is associated to the BACnet thermostat device profile described above and this connection would imply that this specific BACnet thermostat provides current temperature data and responds to commands to set the cooling and heating points. {.align-center} Metadata stores and manages information about the device services that serve as EdgeX Foundry's interfaces to the actual devices and sensors. Device services are other microservices that communicate directly with the device or sensor in the device or sensor protocol of choice, and normalize information and communications with the device or sensor for the rest of EdgeX Foundry. A single Device Service facilitates the communications between EdgeX Foundry and one or more actual devices or sensors. Typically, a Device Service is built to communicate through a particular protocol with devices and sensors that use that protocol. For example, a Modbus Device Service that facilitates the communications among all types of Modbus devices such as motor controllers, proximity sensors, thermostats, power meters, and so forth. {.align-center}","title":"Introduction"},{"location":"microservices/core/metadata/Ch-Metadata/#data-models","text":"Metadata Command Model Metadata Device and Device Profile Model Metadata Device Profile Model Metadata Provision Watcher Model","title":"Data Models"},{"location":"microservices/core/metadata/Ch-Metadata/#data-dictionary","text":"Class Name Description Dependencies Action Contains the target and parameters and expected responses, that describe a REST call. Addressable The metadata required to make a request to an EdgeX Foundry target. For example, the Addressable could be HTTP and URL address details to reach a device service by REST and might include attributes such as HTTP Protocol, URL host of edgex-modbus-device-service, port of 49090. AdminState An object\u2019s current administrative state of \u201cLocked\u201d or\u201dUnlocked.\u201d CallbackAlert The object used by the system to alert regarding a change to a system object. Command The REST description of an interface. Device The object that contains information about the state, position, reachability, and methods of interfacing with a Device Top Level object DeviceManager An object that groups other Devices and groups of Devices. DeviceResource The atomic description of a particular protocol level interface for a class of Devices. DeviceProfile The description of both the protocol level interface, device service interface, and mapping and interpretation logic that describe communication to a class of devices. Top Level object DeviceReport DeviceService The current state and reachability information for a registered Device Services OperatingState An object\u2019s current operating state of \u201cEnabled\u201d or \u201cDisabled.\u201d ProfileProperty The transformation, constraint, and unit properties for a class of Device data. ProfileResource The set of operations that is executed by a Service for a particular Command. PropertyValue The transformation and constraint properties for a class of data. ProvisionWatcher The metadata used by a Service for automatically provisioning matching Devices. ResourceOperation An Operation or set of Operations executed by the Device Service on a Device. Response A description of a possible REST response for a Command. Schedule An object defining a timer or alarm. Top Level object ScheduleEvent The action taken by a Service when the schedule triggers. Top Level object Service The current state and reachability information registered for a Service. Units The unit metadata about a class of Device data.","title":"Data Dictionary"},{"location":"microservices/core/metadata/Ch-Metadata/#high-level-interaction-diagrams","text":"Sequence diagrams for some of the more critical or complex events regarding Metadata. The three following High Level Interaction Diagrams show: EdgeX Foundry Metadata Add a New Device Profile (Step 1 to provisioning a new device) EdgeX Foundry Metadata Add a New Device Profile (Step 2 to provisioning a new device) EdgeX Foundry Metadata Device Service Startup Metadata Add a New Device Profile (Step 1 to provisioning a new device)","title":"High Level Interaction Diagrams"},{"location":"microservices/core/metadata/Ch-Metadata/#configuration-properties","text":"The following are extra configuration parameters specific to the Core-Metadata service. Please refer to the general Configuration documentation for configuration properties common across all services. Configuration Default Value Dependencies The following are additional entries in Writable section applicable to the Core-Metadata service. Writable EnableValueDescriptorManagement false A flag that indicates whether value descriptors should be atomically managed when device profiles are added or updated. The following keys define default behavior and content for interaction with the support-notifications service. Notifications PostDeviceChanges true Indicates whether a notification should be sent when a device's definition is changed. Notifications Slug device-change- The stem of the \"topic\" assigned to the notification Notifications Content Device update: The stem of the message sent to the support-notifications service. Notifications Sender core-metadata Identifies the sender of the notification Notifications Description Metadata device notice A short description of the notification being sent. Notifications Label metadata A descriptive label for the notification.","title":"Configuration Properties"},{"location":"microservices/device/Ch-DeviceServices/","text":"Device Services Microservices The Device Services Layer interacts with Device Services. Device Services (DS) are the edge connectors interacting with the devices or IoT objects that include, but are not limited to: appliances in your home, alarm systems, HVAC equipment, lighting, machines in any industry, irrigation systems, drones, traffic signals, automated transportation, and so forth. Device services may service one or a number of devices, including sensors, actuators, and so forth, at one time. A \"device\" that a DS manages, could be something other than a simple single physical device and could be another gateway and all of that gateway's devices, a device manager, or a device aggregator that acts as a device, or collection of devices, to EdgeX Foundry. The Device Services layer's microservices communicate with the devices, sensors, actuators, and other IoT objects through protocols native to the IoT object. The DS Layer converts the data produced and communicated by the IoT object, into a common EdgeX Foundry data structure, and sends that converted data into the Core Services layer, and to other microservices in other layers of EdgeX Foundry. The EdgeX Foundry Device Services layer at this time, includes the following microservice: APIs--Device Services--Virtual Device Service Device Service Functional Requirements Requirements for the device service are provided below. These requirements are being used to define what functionality needs to be offered via any Device Service SDK to produce the device service scaffolding code. They may also help the reader understand the duties and role of a device service. DS-SDK Functional Requirements","title":"Device Services Microservices"},{"location":"microservices/device/Ch-DeviceServices/#device-services-microservices","text":"The Device Services Layer interacts with Device Services. Device Services (DS) are the edge connectors interacting with the devices or IoT objects that include, but are not limited to: appliances in your home, alarm systems, HVAC equipment, lighting, machines in any industry, irrigation systems, drones, traffic signals, automated transportation, and so forth. Device services may service one or a number of devices, including sensors, actuators, and so forth, at one time. A \"device\" that a DS manages, could be something other than a simple single physical device and could be another gateway and all of that gateway's devices, a device manager, or a device aggregator that acts as a device, or collection of devices, to EdgeX Foundry. The Device Services layer's microservices communicate with the devices, sensors, actuators, and other IoT objects through protocols native to the IoT object. The DS Layer converts the data produced and communicated by the IoT object, into a common EdgeX Foundry data structure, and sends that converted data into the Core Services layer, and to other microservices in other layers of EdgeX Foundry. The EdgeX Foundry Device Services layer at this time, includes the following microservice: APIs--Device Services--Virtual Device Service Device Service Functional Requirements Requirements for the device service are provided below. These requirements are being used to define what functionality needs to be offered via any Device Service SDK to produce the device service scaffolding code. They may also help the reader understand the duties and role of a device service. DS-SDK Functional Requirements","title":"Device Services Microservices"},{"location":"microservices/device/profile/Ch-DeviceProfile/","text":"Device Profile The device profile describes a type of device within the EdgeX system. Each device managed by a device service has an association with a device profile, which defines that device type in terms of the operations which it supports. For a full list of device profile fields and their required values see the Device Profile Reference Identification The profile contains various identification fields. The Name field is required and must be unique in an EdgeX deployment. Other fields are optional - they are not used by device services but may be populated for informational purposes: Description Manufacturer Model Labels DeviceResources A deviceResource specifies a sensor value within a device that may be read from or written to either individually or as part of a deviceCommand. It has a name for identification and a description for informational purposes. The device service allows access to deviceResources via its device REST endpoint. The Attributes in a deviceResource are the device-service-specific parameters required to access the particular value. Each device service implementation will have its own set of named values that are required here, for example a BACnet device service may need an Object Identifier and a Property Identifier whereas a Bluetooth device service could use a UUID to identify a value. The Properties of a deviceResource describe the value and optionally request some simple processing to be performed on it. Each device resource is given two properties, value and units . The following fields are available in the value property: type - Required. The data type of the value. Supported types are bool , int8 - int64 , uint8 - uint64 , float32 , float64 , string , binary and arrays of the primitive types (ints, floats, bool). Arrays are specified as eg. float32array , boolarray etc. readWrite - R , RW , or W indicating whether the value is readable or writable. defaultValue - a value used for PUT requests which do not specify one. assertion - a string value to which a reading (after processing) is compared. If the reading is not the same as the assertion value, the device's operating state will be set to disbled. This can be useful for health checks. base - a value to be raised to the power of the raw reading before it is returned. scale - a factor by which to multiply a reading before it is returned. offset - a value to be added to a reading before it is returned. mask - a binary mask which will be applied to an integer reading. shift - a number of bits by which an integer reading will be shifted right. The processing defined by base, scale, offset, mask and shift is applied in that order. This is done within the SDK. A reverse transformation is applied by the SDK to incoming data on set operations (NB mask transforms on set are NYI) The units property is used to indicate the units of the value, eg Amperes, degrees C, etc. It should have a defaultValue that specifies the units. DeviceCommands DeviceCommands define access to reads and writes for multiple simultaneous device resources. Each named deviceCommand should contain a number of get and/or set resourceOperations , describing the read or write respectively. DeviceCommands may be useful when readings are logically related, for example with a 3-axis accelerometer it is helpful to read all axes together. A resourceOperation consists of the following properties: index - a number, used to define an order in which the resource is processed. and set operations is not supported. deviceResource - the name of the deviceResource to access. parameter - optional, a value that will be used if a PUT request does not specify one. mappings - optional, allows readings of String type to be re-mapped. The device service allows access to deviceCommands via the same device REST endpoint as is used to access deviceResources. If a deviceCommand and deviceResource have the same name, it will be the deviceCommand which is available. CoreCommands CoreCommands specify the commands which are available via the core-command microservice, for reading and writing to the device. Both deviceResources and deviceCommands may be represented by coreCommands (the name of the coreCommand refers to the name of the deviceCommand or deviceResource). Commands may allow get or put methods (or both). For a get type, the returned values are specified in the expectedValues field, for a put type, the parameters to be given are specified in parameterNames . In either case, the different http response codes that the service may generate are indicated. Core Commands may be thought of as defining the outward-facing API for the device. A typical setup would prevent external access to the device service itself, so use of the full range of device resources and device commands would only be available to other components within the EdgeX deployment. Only those that has corresponding coreCommands would be available externally (via core-command).","title":"Device Profile"},{"location":"microservices/device/profile/Ch-DeviceProfile/#device-profile","text":"The device profile describes a type of device within the EdgeX system. Each device managed by a device service has an association with a device profile, which defines that device type in terms of the operations which it supports. For a full list of device profile fields and their required values see the Device Profile Reference","title":"Device Profile"},{"location":"microservices/device/profile/Ch-DeviceProfile/#identification","text":"The profile contains various identification fields. The Name field is required and must be unique in an EdgeX deployment. Other fields are optional - they are not used by device services but may be populated for informational purposes: Description Manufacturer Model Labels","title":"Identification"},{"location":"microservices/device/profile/Ch-DeviceProfile/#deviceresources","text":"A deviceResource specifies a sensor value within a device that may be read from or written to either individually or as part of a deviceCommand. It has a name for identification and a description for informational purposes. The device service allows access to deviceResources via its device REST endpoint. The Attributes in a deviceResource are the device-service-specific parameters required to access the particular value. Each device service implementation will have its own set of named values that are required here, for example a BACnet device service may need an Object Identifier and a Property Identifier whereas a Bluetooth device service could use a UUID to identify a value. The Properties of a deviceResource describe the value and optionally request some simple processing to be performed on it. Each device resource is given two properties, value and units . The following fields are available in the value property: type - Required. The data type of the value. Supported types are bool , int8 - int64 , uint8 - uint64 , float32 , float64 , string , binary and arrays of the primitive types (ints, floats, bool). Arrays are specified as eg. float32array , boolarray etc. readWrite - R , RW , or W indicating whether the value is readable or writable. defaultValue - a value used for PUT requests which do not specify one. assertion - a string value to which a reading (after processing) is compared. If the reading is not the same as the assertion value, the device's operating state will be set to disbled. This can be useful for health checks. base - a value to be raised to the power of the raw reading before it is returned. scale - a factor by which to multiply a reading before it is returned. offset - a value to be added to a reading before it is returned. mask - a binary mask which will be applied to an integer reading. shift - a number of bits by which an integer reading will be shifted right. The processing defined by base, scale, offset, mask and shift is applied in that order. This is done within the SDK. A reverse transformation is applied by the SDK to incoming data on set operations (NB mask transforms on set are NYI) The units property is used to indicate the units of the value, eg Amperes, degrees C, etc. It should have a defaultValue that specifies the units.","title":"DeviceResources"},{"location":"microservices/device/profile/Ch-DeviceProfile/#devicecommands","text":"DeviceCommands define access to reads and writes for multiple simultaneous device resources. Each named deviceCommand should contain a number of get and/or set resourceOperations , describing the read or write respectively. DeviceCommands may be useful when readings are logically related, for example with a 3-axis accelerometer it is helpful to read all axes together. A resourceOperation consists of the following properties: index - a number, used to define an order in which the resource is processed. and set operations is not supported. deviceResource - the name of the deviceResource to access. parameter - optional, a value that will be used if a PUT request does not specify one. mappings - optional, allows readings of String type to be re-mapped. The device service allows access to deviceCommands via the same device REST endpoint as is used to access deviceResources. If a deviceCommand and deviceResource have the same name, it will be the deviceCommand which is available.","title":"DeviceCommands"},{"location":"microservices/device/profile/Ch-DeviceProfile/#corecommands","text":"CoreCommands specify the commands which are available via the core-command microservice, for reading and writing to the device. Both deviceResources and deviceCommands may be represented by coreCommands (the name of the coreCommand refers to the name of the deviceCommand or deviceResource). Commands may allow get or put methods (or both). For a get type, the returned values are specified in the expectedValues field, for a put type, the parameters to be given are specified in parameterNames . In either case, the different http response codes that the service may generate are indicated. Core Commands may be thought of as defining the outward-facing API for the device. A typical setup would prevent external access to the device service itself, so use of the full range of device resources and device commands would only be available to other components within the EdgeX deployment. Only those that has corresponding coreCommands would be available externally (via core-command).","title":"CoreCommands"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/","text":"Device Profile Reference This chapter details the structure of a Device Profile and allowable values for its fields. Device Profile Field Name Type Required? Notes Name String Y Must be unique in the EdgeX deployment Manufacturer String N Model String N Labels Array of String N DeviceResources Array of DeviceResource Y DeviceCommands Array of DeviceCommand N CoreCommands Array of CoreCommand N DeviceResource Field Name Type Required? Notes Name String Y Must be unique in the EdgeX deployment Description String N Tag String N Attributes String-String Map Y Each Device Service should define required and optional keys Properties ProfileProperty Y ProfileProperty Field Name Type Required? Notes Value PropertyValue Y Units Units N PropertyValue Field Name Type Required? Notes Type Enum Y uint8 , uint16 , uint32 , uint64 , int8 , int16 , int32 , int64 , float32 , float64 , bool , string , binary , uint8array , uint16array , uint32array , uint64array , int8array , int16array , int32array , int64array , float32array , float64array , boolarray ReadWrite Enum Y R , W , RW DefaultValue String N If present, should be compatible with the Type field Mask Unsigned Int N Only valid where Type is one of the integer types Shift Unsigned Int N Only valid where Type is one of the integer types Scale Int or Float N Only valid where Type is one of the integer or float types Offset Int or Float N Only valid where Type is one of the integer or float types Base Int or Float N Only valid where Type is one of the integer or float types Assertion String N FloatEncoding Enum N base64 , eNotation - Only valid where Type is one of the float types MediaType String N Only valid where Type is Binary Units Field Name Type Required? Notes DefaultValue String Y DeviceCommand (Note: represented in Go by ProfileResource ) Field Name Type Required? Notes Name String Y Must be unique in this profile. May have the same name as a DeviceResource but this will make the DeviceResource not accessible individually. Such a configuration is only recommended if Mappings are used: see below. Get Array of ResourceOperation N At least one of Get and Set must be present Set Array of ResourceOperation N At least one of Get and Set must be present ResourceOperation Field Name Type Required? Notes DeviceResource String Y Must name a DeviceResource in this profile Parameter String N If present, should be compatible with the Type field of the named DeviceResource Mappings String-String Map N Only valid where the Type of the named DeviceResource is String CoreCommand (Note: represented in Go by Command ) Field Name Type Required? Notes Name String Y Must name a DeviceCommand or a DeviceResource in this profile Get GetCommand See note At least one of Get and Put must be present Put PutCommand See note At least one of Get and Put must be present GetCommand Field Name Type Required? Notes Path String Y Must be /api/v1/device/{deviceId}/XXX where XXX is the name of the command Responses Array of Response Y PutCommand Field Name Type Required? Notes Path String Y Must be /api/v1/device/{deviceId}/XXX where XXX is the name of the command Responses Array of Response Y ParameterNames Array of String Y Should correspond to the DeviceResource names associated with this Command Response Field Name Type Required? Notes Code Unsigned Int Y Should be a valid HTTP response code Description String N ExpectedValues Array of String Y For Get commands with success (2xx) Code, should correspond to the DeviceResource names associated with this command. For failing Get commands and Put commands, should be an empty array.","title":"Device Profile Reference"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/#device-profile-reference","text":"This chapter details the structure of a Device Profile and allowable values for its fields.","title":"Device Profile Reference"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/#device-profile","text":"Field Name Type Required? Notes Name String Y Must be unique in the EdgeX deployment Manufacturer String N Model String N Labels Array of String N DeviceResources Array of DeviceResource Y DeviceCommands Array of DeviceCommand N CoreCommands Array of CoreCommand N","title":"Device Profile"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/#deviceresource","text":"Field Name Type Required? Notes Name String Y Must be unique in the EdgeX deployment Description String N Tag String N Attributes String-String Map Y Each Device Service should define required and optional keys Properties ProfileProperty Y","title":"DeviceResource"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/#profileproperty","text":"Field Name Type Required? Notes Value PropertyValue Y Units Units N","title":"ProfileProperty"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/#propertyvalue","text":"Field Name Type Required? Notes Type Enum Y uint8 , uint16 , uint32 , uint64 , int8 , int16 , int32 , int64 , float32 , float64 , bool , string , binary , uint8array , uint16array , uint32array , uint64array , int8array , int16array , int32array , int64array , float32array , float64array , boolarray ReadWrite Enum Y R , W , RW DefaultValue String N If present, should be compatible with the Type field Mask Unsigned Int N Only valid where Type is one of the integer types Shift Unsigned Int N Only valid where Type is one of the integer types Scale Int or Float N Only valid where Type is one of the integer or float types Offset Int or Float N Only valid where Type is one of the integer or float types Base Int or Float N Only valid where Type is one of the integer or float types Assertion String N FloatEncoding Enum N base64 , eNotation - Only valid where Type is one of the float types MediaType String N Only valid where Type is Binary","title":"PropertyValue"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/#units","text":"Field Name Type Required? Notes DefaultValue String Y","title":"Units"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/#devicecommand","text":"(Note: represented in Go by ProfileResource ) Field Name Type Required? Notes Name String Y Must be unique in this profile. May have the same name as a DeviceResource but this will make the DeviceResource not accessible individually. Such a configuration is only recommended if Mappings are used: see below. Get Array of ResourceOperation N At least one of Get and Set must be present Set Array of ResourceOperation N At least one of Get and Set must be present","title":"DeviceCommand"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/#resourceoperation","text":"Field Name Type Required? Notes DeviceResource String Y Must name a DeviceResource in this profile Parameter String N If present, should be compatible with the Type field of the named DeviceResource Mappings String-String Map N Only valid where the Type of the named DeviceResource is String","title":"ResourceOperation"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/#corecommand","text":"(Note: represented in Go by Command ) Field Name Type Required? Notes Name String Y Must name a DeviceCommand or a DeviceResource in this profile Get GetCommand See note At least one of Get and Put must be present Put PutCommand See note At least one of Get and Put must be present","title":"CoreCommand"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/#getcommand","text":"Field Name Type Required? Notes Path String Y Must be /api/v1/device/{deviceId}/XXX where XXX is the name of the command Responses Array of Response Y","title":"GetCommand"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/#putcommand","text":"Field Name Type Required? Notes Path String Y Must be /api/v1/device/{deviceId}/XXX where XXX is the name of the command Responses Array of Response Y ParameterNames Array of String Y Should correspond to the DeviceResource names associated with this Command","title":"PutCommand"},{"location":"microservices/device/profile/Ch-DeviceProfileRef/#response","text":"Field Name Type Required? Notes Code Unsigned Int Y Should be a valid HTTP response code Description String N ExpectedValues Array of String Y For Get commands with success (2xx) Code, should correspond to the DeviceResource names associated with this command. For failing Get commands and Put commands, should be an empty array.","title":"Response"},{"location":"microservices/device/sdk/Ch-DeviceSDK/","text":"Device Services SDK Introduction to the SDK The EdgeX Foundry Device Service Software Development Kit (SDK) takes the Developer through the step-by-step process to create an EdgeX Foundry Device Service microservice. Then setup the SDK and execute the code to generate the Device Service scaffolding to get you started using EdgeX. The Device Service SDK supports: Synchronous read and write operations Asynchronous Device data Initialization and deconstruction of Driver Interface Initialization and destruction of Device Connection Framework for automated Provisioning Mechanism Support for multiple classes of Devices with Profiles Support for sets of actions triggered by a command Cached responses to queries Device Service Workflow Key to diagram Colour of Box Description Orange Everything is part of a Base Service. Light Green Initialization. Gets its own configuration and registers itself. Yellow Update Controller. receives, processes, and publishes the update. Dark Blue UInitializing and setting up of schedules. Gray Scaffolding code to be receivers into Device Service. Processes commands. Purple Initializes itself. Set up in metadata. Registers its Device Service discovery process and registration, and sets up Device Services. Gets Device Watchers . When a Device Service first comes up it has its initial set of devices. The Device Watcher waits to receive information that a new device has occurred.!Then a Device Watcher sends metadata messages out about the new device. Dark Green Send data to Core Data. How to communicate with the devices, and on what schedule, and to receive information back from the devices. Writing a Device Service Writing a new Device Service in Go \u3009","title":"Device Services SDK"},{"location":"microservices/device/sdk/Ch-DeviceSDK/#device-services-sdk","text":"","title":"Device Services SDK"},{"location":"microservices/device/sdk/Ch-DeviceSDK/#introduction-to-the-sdk","text":"The EdgeX Foundry Device Service Software Development Kit (SDK) takes the Developer through the step-by-step process to create an EdgeX Foundry Device Service microservice. Then setup the SDK and execute the code to generate the Device Service scaffolding to get you started using EdgeX. The Device Service SDK supports: Synchronous read and write operations Asynchronous Device data Initialization and deconstruction of Driver Interface Initialization and destruction of Device Connection Framework for automated Provisioning Mechanism Support for multiple classes of Devices with Profiles Support for sets of actions triggered by a command Cached responses to queries","title":"Introduction to the SDK"},{"location":"microservices/device/sdk/Ch-DeviceSDK/#device-service-workflow","text":"Key to diagram Colour of Box Description Orange Everything is part of a Base Service. Light Green Initialization. Gets its own configuration and registers itself. Yellow Update Controller. receives, processes, and publishes the update. Dark Blue UInitializing and setting up of schedules. Gray Scaffolding code to be receivers into Device Service. Processes commands. Purple Initializes itself. Set up in metadata. Registers its Device Service discovery process and registration, and sets up Device Services. Gets Device Watchers . When a Device Service first comes up it has its initial set of devices. The Device Watcher waits to receive information that a new device has occurred.!Then a Device Watcher sends metadata messages out about the new device. Dark Green Send data to Core Data. How to communicate with the devices, and on what schedule, and to receive information back from the devices.","title":"Device Service Workflow"},{"location":"microservices/device/sdk/Ch-DeviceSDK/#writing-a-device-service","text":"Writing a new Device Service in Go \u3009","title":"Writing a Device Service"},{"location":"microservices/device/virtual/Ch-VirtualDevice/","text":"Virtual Device Introduction The Virtual Device Service simulates different kinds of devices to generate Events and Readings to the Core Data Microservice, and Users send commands and get responses through the Command and Control Microservice. These features of the Virtual Device Services are useful when executing functional or performance tests without having any real devices. Virtual Device Service Overall Flow Virtual Device Service has dependencies on Core Data and Meta Data Microservices, since the initialization process needs to check or register profiles, devices, and value descriptors. Therefore, Core Data and Metadata Microservices have to fully start up before Virtual Device Service initialization. At the beginning of the Virtual Device Service initialization process, the process sends a ping request to Core Data and Metadata Microservices to verify their status until they both fully start up. The fixed time out limit is 600 seconds. After 600 seconds, if Core Data and Metadata Microservices have not fully started up, the initialization process fails. When the Virtual Device Service starts up, the initialization process loads the definitions of ValueDescriptor, DeviceService, and DeviceProfile from YAML files and creates them in the Metadata Microservice. Some default YAML files exist, and Users can create their own. Please see Device Profile Definition section (on this page) for more detail. In addition, the Virtual Device Service provides callback APIs for the Metadata Microservice to manage the Device instances. According to the GET Commands in the DeviceProfile definitions, there is an H2 database (in-memory) storing resource in Virtual Devices, called \"Virtual Resources.\" Virtual Resources Example After starting up, the Virtual Device Service reads the H2 database and periodically sends the current value as an Event to the Core Data Microservice. The default frequency is 15 seconds, and that can be modified from the configuration file. The current value of virtual resources is re-generated at a random value before each collection cycle, so the Reading should be different in each Event. The Virtual Device Service provides APIs for Command and Control Microservice, reads the H2 database, and returns the current value for GET Commands. At this time, the Virtual Device Service supports two special PUT Commands: \"enableRandomization\" and \"collectionFrequency.\" These Commands modify the \"ENABLE_RANDOMIZATION\" and \"COLLECTION_FREQUENCY\" column of a specific virtual resource in the H2 database. For example , sending an HTTP PUT method to http : // localhost : 48082 / device / 56 b1acf1d66f9c9762581ea4 / command / 56 b1acedd66f9c9762581e9d / put / 0 with the sample payload : { \"enableRandomization\" : false , \"collectionFrequency\" : 6 } modifies a virtual resource record whose command ID is \"56b1acedd66f9c9762581e9d \" and device ID is \"56b1acf1d66f9c9762581ea4.\" By modifying the \"ENABLE_RANDOMIZATION\" column to FALSE, the value of the virtual resource will not re-generate a random value anymore. By modifying the \"COLLECTION_FREQUENCY\" column, the collecting frequency will be changed after next collection cycle. They both can be modified manually through H2 Console: http://localhost:49990/console Modify the \u201cJDBC URL\u201d to jdbc:h2:mem:testdb, and click on \u201cConnect.\u201d Special Configuration The virtual device micro service does contain some unique configuration properties and these often change between development (for example when run from Eclipse) and the containerized version (i.e. Docker container) of the same service. Here are the list of unique properties you should investigate before running the virtual device service to better understand how it works in your environment: The path used to locate the Device Profile YAML files used to define the virtual devices managed by the device service (see Device Profile Definition section below). application.device-profile-paths=./bacnet_sample_profiles,./modbus_sample_profiles Indicator to the virtual device service to provisioning devices from the YAML profiles in the above directory, creating one device for each profile automatically when set to true. application.auto-create-device=true When developing, it is often advantageous to have the virtual device service start afresh and with a clean Meta data database each time the service starts. The property below indicates whether the device service should clean out any existing virtual devices in the database when it shuts down so a clean environment is available when the service starts backup. Typically set to true for development and false for runtime/demonstration environments. application.auto-cleanup=true How often, in seconds, the virtual device service's scheduler should collect data from the virtual device. application.collection-frequency=15 Service Name and Host Name In EdgeX device services the service name (which is represented by service.name key in the application.properties file or Consul configuration) is the identity of the Device Service object. The name is used by EdgeX to attribute all the information about the service (in particular schedules, device ownership, etc.) to this name. However, the service.host parameter is used to describe how to interact with the service. Depending on your operating mode, the following guidelines for configuring the service host apply. Deployment Mode (running everything containerized in Docker): The Service host (which is represented by the service.host key in the application.properties file or Consul configuration) is the DNS or IP address networking entry of the entity that the service is bound to (container, machine, etc) and reachable from the other microservices. This allows a full location URL for the service to be determined. In Docker environments, the host name is the name of the Docker container running the microservice (such as edgex-device-virtual). Use service.host=\\${service.name} and the docker-compose file for all services (default). Important Note: be sure to use Docker Compose and docker-compose file (found in the developer-scripts repos) to bring up the containers for all services. Docker Compose establishes the networking and container naming for you, which can otherwise be difficult to do and prone to errors if bringing up containers manually. Developer Mode (running everything natively): When running a service natively, the service names will not resolve to a DNS entry as they will in a Docker environment. Use service.host=localhost for all services (default). Hybrid Mode (running some services natively with the rest deployed with Docker): Use service.host=\\<Host Machine IP Address> for the native services (manual configuration) and the docker-compose file to bring up the containerized services (default). Ensure that Addressable objects for the native services are not accidentally created by bringing them up with the docker-compose file, otherwise conflicts may arise. This issue is being addressed in future versions. System Architecture The Virtual Device Service adopts a normal MVC design pattern, separating logic into different layers. System Architecture Graphic Interface Layer --interacting with other microservices. Controllers provide the RESTful API. The implementation is located in org.edgexfoundry.device.virtual.controller package. Scheduled Collection Tasks is a set of async tasks which is executed periodically, and they are created for each virtual resource (GET Command). See org.edgexfoundry.device.virtual.scheduling package for the detailed implementation. Also, org.edgexfoundry.device.virtual.scheduling.Scheduler reads Schedule and ScheduleEvent from Meta Data Microservice and arranges the collection tasks. Tasks execution logic is located in org.edgexfoundry.device.virtual.service.impl.CollectionTaskExecutorImpl, and the tasks creation behavior is located in org.edgexfoundry.device.virtual.service.impl.VirtualResourceManagerImpl.createDefaultRecords(). Service Layer --processing business logic, such as, executing collection tasks and commands, managing profiles and devices, and so forth. See org.edgexfoundry.device.virtual.service.impl package for more details. DAO Layer --processing protocol access. For Virtual Device Services, a Spring Data JPA interface in org.edgexfoundry.device.virtual.dao package. Spring framework will process the communication effort to access H2 DB. Data Layer --an H2 DB to simulate device resources. Device Profile Definition Users can define any virtual device profile in YAML format, if the structure is in accordance with the \"Device and Device Profile Model\" (in the graphic 3 paragraphs below). By assigning the file path to application property \"application.device-profile-paths\" , Virtual Device Service loads all the YAML files under this folder, and this property accepts multiple values separated by comma (,). For instance, the following setting causes Virtual Device Service to load all YAML files under ./bacnet_sample_profiles and ./modbus_sample_profiles folders. In addition to Profile Definition, ValueDescriptors are defined in the \"deviceResources.properties\" part of the profile definition. The structure needs to conform with the ProfileProperty in \"Device Profile\" (in the graphic below the \"Device and Device Profile Model\"), and the ValueDescriptors will be created and send to the Core Data Microservice during the Device creation callback process. By assigning the application property \"application.auto-create-device\" = true (the default value is true), the Virtual Device Service creates Device instances for each profile automatically during starting up, and the Device instances start sending events and readings to Core Data Microservice. Data Model Virtual Device Service Data Model--Device and Device Profile Model Virtual Device Service Data Model--Command Model VirtualResource VirtualResource is the data object generated from Device instances and persisted in H2 database. Data Dictionary Class Name Description ScanList The object containing a protocol discovery method query. Transaction The asynchronous helper object used for gathering sets of device responses.","title":"Virtual Device"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#virtual-device","text":"","title":"Virtual Device"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#introduction","text":"The Virtual Device Service simulates different kinds of devices to generate Events and Readings to the Core Data Microservice, and Users send commands and get responses through the Command and Control Microservice. These features of the Virtual Device Services are useful when executing functional or performance tests without having any real devices. Virtual Device Service Overall Flow Virtual Device Service has dependencies on Core Data and Meta Data Microservices, since the initialization process needs to check or register profiles, devices, and value descriptors. Therefore, Core Data and Metadata Microservices have to fully start up before Virtual Device Service initialization. At the beginning of the Virtual Device Service initialization process, the process sends a ping request to Core Data and Metadata Microservices to verify their status until they both fully start up. The fixed time out limit is 600 seconds. After 600 seconds, if Core Data and Metadata Microservices have not fully started up, the initialization process fails. When the Virtual Device Service starts up, the initialization process loads the definitions of ValueDescriptor, DeviceService, and DeviceProfile from YAML files and creates them in the Metadata Microservice. Some default YAML files exist, and Users can create their own. Please see Device Profile Definition section (on this page) for more detail. In addition, the Virtual Device Service provides callback APIs for the Metadata Microservice to manage the Device instances. According to the GET Commands in the DeviceProfile definitions, there is an H2 database (in-memory) storing resource in Virtual Devices, called \"Virtual Resources.\"","title":"Introduction"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#virtual-resources-example","text":"After starting up, the Virtual Device Service reads the H2 database and periodically sends the current value as an Event to the Core Data Microservice. The default frequency is 15 seconds, and that can be modified from the configuration file. The current value of virtual resources is re-generated at a random value before each collection cycle, so the Reading should be different in each Event. The Virtual Device Service provides APIs for Command and Control Microservice, reads the H2 database, and returns the current value for GET Commands. At this time, the Virtual Device Service supports two special PUT Commands: \"enableRandomization\" and \"collectionFrequency.\" These Commands modify the \"ENABLE_RANDOMIZATION\" and \"COLLECTION_FREQUENCY\" column of a specific virtual resource in the H2 database. For example , sending an HTTP PUT method to http : // localhost : 48082 / device / 56 b1acf1d66f9c9762581ea4 / command / 56 b1acedd66f9c9762581e9d / put / 0 with the sample payload : { \"enableRandomization\" : false , \"collectionFrequency\" : 6 } modifies a virtual resource record whose command ID is \"56b1acedd66f9c9762581e9d \" and device ID is \"56b1acf1d66f9c9762581ea4.\" By modifying the \"ENABLE_RANDOMIZATION\" column to FALSE, the value of the virtual resource will not re-generate a random value anymore. By modifying the \"COLLECTION_FREQUENCY\" column, the collecting frequency will be changed after next collection cycle. They both can be modified manually through H2 Console: http://localhost:49990/console Modify the \u201cJDBC URL\u201d to jdbc:h2:mem:testdb, and click on \u201cConnect.\u201d","title":"Virtual Resources Example"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#special-configuration","text":"The virtual device micro service does contain some unique configuration properties and these often change between development (for example when run from Eclipse) and the containerized version (i.e. Docker container) of the same service. Here are the list of unique properties you should investigate before running the virtual device service to better understand how it works in your environment: The path used to locate the Device Profile YAML files used to define the virtual devices managed by the device service (see Device Profile Definition section below). application.device-profile-paths=./bacnet_sample_profiles,./modbus_sample_profiles Indicator to the virtual device service to provisioning devices from the YAML profiles in the above directory, creating one device for each profile automatically when set to true. application.auto-create-device=true When developing, it is often advantageous to have the virtual device service start afresh and with a clean Meta data database each time the service starts. The property below indicates whether the device service should clean out any existing virtual devices in the database when it shuts down so a clean environment is available when the service starts backup. Typically set to true for development and false for runtime/demonstration environments. application.auto-cleanup=true How often, in seconds, the virtual device service's scheduler should collect data from the virtual device. application.collection-frequency=15 Service Name and Host Name In EdgeX device services the service name (which is represented by service.name key in the application.properties file or Consul configuration) is the identity of the Device Service object. The name is used by EdgeX to attribute all the information about the service (in particular schedules, device ownership, etc.) to this name. However, the service.host parameter is used to describe how to interact with the service. Depending on your operating mode, the following guidelines for configuring the service host apply. Deployment Mode (running everything containerized in Docker): The Service host (which is represented by the service.host key in the application.properties file or Consul configuration) is the DNS or IP address networking entry of the entity that the service is bound to (container, machine, etc) and reachable from the other microservices. This allows a full location URL for the service to be determined. In Docker environments, the host name is the name of the Docker container running the microservice (such as edgex-device-virtual). Use service.host=\\${service.name} and the docker-compose file for all services (default). Important Note: be sure to use Docker Compose and docker-compose file (found in the developer-scripts repos) to bring up the containers for all services. Docker Compose establishes the networking and container naming for you, which can otherwise be difficult to do and prone to errors if bringing up containers manually. Developer Mode (running everything natively): When running a service natively, the service names will not resolve to a DNS entry as they will in a Docker environment. Use service.host=localhost for all services (default). Hybrid Mode (running some services natively with the rest deployed with Docker): Use service.host=\\<Host Machine IP Address> for the native services (manual configuration) and the docker-compose file to bring up the containerized services (default). Ensure that Addressable objects for the native services are not accidentally created by bringing them up with the docker-compose file, otherwise conflicts may arise. This issue is being addressed in future versions.","title":"Special Configuration"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#system-architecture","text":"The Virtual Device Service adopts a normal MVC design pattern, separating logic into different layers. System Architecture Graphic Interface Layer --interacting with other microservices. Controllers provide the RESTful API. The implementation is located in org.edgexfoundry.device.virtual.controller package. Scheduled Collection Tasks is a set of async tasks which is executed periodically, and they are created for each virtual resource (GET Command). See org.edgexfoundry.device.virtual.scheduling package for the detailed implementation. Also, org.edgexfoundry.device.virtual.scheduling.Scheduler reads Schedule and ScheduleEvent from Meta Data Microservice and arranges the collection tasks. Tasks execution logic is located in org.edgexfoundry.device.virtual.service.impl.CollectionTaskExecutorImpl, and the tasks creation behavior is located in org.edgexfoundry.device.virtual.service.impl.VirtualResourceManagerImpl.createDefaultRecords(). Service Layer --processing business logic, such as, executing collection tasks and commands, managing profiles and devices, and so forth. See org.edgexfoundry.device.virtual.service.impl package for more details. DAO Layer --processing protocol access. For Virtual Device Services, a Spring Data JPA interface in org.edgexfoundry.device.virtual.dao package. Spring framework will process the communication effort to access H2 DB. Data Layer --an H2 DB to simulate device resources.","title":"System Architecture"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#device-profile-definition","text":"Users can define any virtual device profile in YAML format, if the structure is in accordance with the \"Device and Device Profile Model\" (in the graphic 3 paragraphs below). By assigning the file path to application property \"application.device-profile-paths\" , Virtual Device Service loads all the YAML files under this folder, and this property accepts multiple values separated by comma (,). For instance, the following setting causes Virtual Device Service to load all YAML files under ./bacnet_sample_profiles and ./modbus_sample_profiles folders. In addition to Profile Definition, ValueDescriptors are defined in the \"deviceResources.properties\" part of the profile definition. The structure needs to conform with the ProfileProperty in \"Device Profile\" (in the graphic below the \"Device and Device Profile Model\"), and the ValueDescriptors will be created and send to the Core Data Microservice during the Device creation callback process. By assigning the application property \"application.auto-create-device\" = true (the default value is true), the Virtual Device Service creates Device instances for each profile automatically during starting up, and the Device instances start sending events and readings to Core Data Microservice.","title":"Device Profile Definition"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#data-model","text":"Virtual Device Service Data Model--Device and Device Profile Model Virtual Device Service Data Model--Command Model VirtualResource VirtualResource is the data object generated from Device instances and persisted in H2 database.","title":"Data Model"},{"location":"microservices/device/virtual/Ch-VirtualDevice/#data-dictionary","text":"Class Name Description ScanList The object containing a protocol discovery method query. Transaction The asynchronous helper object used for gathering sets of device responses.","title":"Data Dictionary"},{"location":"microservices/security/Ch-APIGateway/","text":"API Gateway The security API gateway is the single point of entry for all EdgeX REST traffic. It is the barrier between external clients and the EdgeX microservices preventing unauthorized access to EdgeX REST APIs. The API gateway accepts client requests, verifies the identity of the clients, redirects the requests to correspondent microservice and relays the results back to the client. The API Gateway provides an HTTP REST interface for administration management. The administrative management offers the means to configure API routing, as well as client authentication and access control. This configuration is store in an embedded database. KONG ( https://konghq.com/ ) is the product underlying the API gateway. The EdgeX community has added code to initialize the KONG environment, set up service routes for EdgeX microservices, and add various authentication/authorization mechanisms including JWT authentication, OAuth2 authentication and ACL. Start the API Gateway Start the API gateway with Docker Compose and a Docker Compose manifest file (the Docker Compose file named docker-compose-nexus-{redis,mongo}.yml (or -arm64 variabnts) found at https://github.com/edgexfoundry/developer-scripts/tree/master/releases/geneva/compose-files )). This Compose file starts all of EdgeX including the security services. The command to start EdgeX inclusive of API gateway related services is: : docker-compose up -d For debugging purpose, the API gateway services can be started individually with these commands used in sequence after secret store starts successfully. Lines starts with # are comments to explain the purpose of the command. : docker - compose up - d kong - db # start up backend database for API gateway docker - compose up - d kong - migrations # initialize the backend database for API gateway docker - compose up - d kong # start up KONG the major component of API gateway docker - compose up - d edgex - proxy # initialize KONG , configure proxy routes , apply certificates to routes , and enable various authentication / ACL features . If the last command returns an error message for any reason (such as incorrect configuration file), the API gateway may be in an unstable status. The following command can be used to stop and remove the containers. : docker-compose down # stop and remove the containers After stopping and removing the containers, you can attempt to recreate and start them again. Alternatively you can use the command to reset the API gateway as shown below: : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go --reset=true After issuing the reset command, attempt to start and reinitialize with the command below. : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go --init=true You can learn more about these commands, to include some additional options by running: : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go \u2013h On successfully starting EdgeX with the API Gateway services, the list of running containers should close follow the listing shown below. Note key security service containers like kong, kong-db, edgex-vault are listed. Configuring API Gateway The API gateway supports two different forms of authentication: JSON Web Token (JWT) or OAuth2 Authentication. Only one authentication method can be enabled at a time. The API Gateway also supports an Access Control List (ACL) which can be enabled with one of the authentication methods mentioned earlier for fine control among the groups. The authentication and ACL need to be specified in the API gateway's configuration file. Setup of authentication and access control occurs automatically as part of API gateway initialization. The configuration file can be found at https://github.com/edgexfoundry/security-api-gateway/blob/master/core/res/configuration-docker.toml Configuration of JWT Authentication for API Gateway When using JWT Authentication, the [kongauth] section needs to be specified in the configuration file as shown below. : [kongauth] name = \"jwt\" Configuration of OAuth2 Authentication for API Gateway When using OAuth2 Authentication, the [kongauth] section needs to specify oauth2 in the configuration file as shown below. Note, today EdgeX only supports \"client credential\" authentication (specified in \"granttype\") currently for OAuth. [kongauth] name = \"oauth2\" scopes = \"email,phone,address\" mandatoryscope = \"true\" enableclientcredentials = \"true\" clientid = \"test\" clientsecret = \"test\" redirecturi = \"http://edgex.com\" granttype = \"client_credentials\" scopegranted = \"email\" resource = \"coredata\" Configuration of ACL for API Gateway Access control is also specified in the configuration file as shown below. Note, users that belong to the whitelist will have access to the resources of EdgeX, and users not belonging to the group listed here will be denied when trying to access resources through the API Gateway. : [kongacl] name = \"acl\" whitelist = \"admin,user\" Configuration of Adding Microservices Routes for API Gateway For the current pre-existing Kong routes, they are configured and initialized statically through configuration TOML file specified in security-proxy-setup application. This is not sufficient for some other new additional microservices like application services, for example. Thus, adding new proxy Kong routes are now possibly achieved via the environment variable, ADD_PROXY_ROUTE , of service edgex-proxy in the docker-compose file. Here is an example: edgex-proxy : ... environment : ... ADD_PROXY_ROUTE : \"myApp.http://my-app:56789\" ... ... my-app : ... container_name : myApp hostname : myApp ... The value of ADD_PROXY_ROUTE takes a comma-separated list of one or more (at least one) paired additional service name and URL for which to create proxy Kong routes. The paired specification is given as the following: \\<RoutePrefix>.\\<TargetRouteURL> where RoutePrefix is the name of service which requests to create proxy Kong route and it is case insensitive; it is the docker network hostname of the service that want to add the proxy Kong route in the docker-compose file if running from docker-compose, for example, myApp in this case. TargetRouteURL is the full qualified URL for the target service, like http://my-app:56789 So as an example, for a single service, the value of ADD_PROXY_ROUTE would be: \" myApp.http://my-app:56789 \". Once ADD_PROXY_ROUTE is configured and composed-up successfully, the proxy route then can be accessed the app's REST API via Kong as http://localhost:8000/myApp/api/v1/... in the same way you would access the edgex service in which you will also need an access token and it is using default access role if not specified in the TOML configuration file as well. Using API Gateway Resource Mapping between EdgeX Microservice and API gateway If the EdgeX API gateway is not in use, a client can access and use any REST API provided by the EdgeX microservices by sending an HTTP request to the service endpoint. E.g., a client can consume the ping endpoint of the Core Data microservice with curl command like this: : curl http://<core-data-microservice-ip>:48080/api/v1/ping Once the API gateway is started and initialized successfully, and all the common ports for EdgeX microservices are blocked by disabling the exposed external ports of the EdgeX microservices through updating the docker compose file, the EdgeX microservice will be behind the gateway. At this time both the microservice host/IP Address (\\<core-data-microservice-ip> in the example) as well as the service port (48080 in the example) are not available to external access. EdgeX uses the gateway as a single entry point for all the REST APIs. With the API gateway in place, the curl command to ping the endpoint of the same Core Data service, as shown above, needs to change to : : curl https://<api-gateway-host-ip>:8443/coredata/api/v1/ping Comparing these two curl commands you may notice several differences. \"Http\" is switched to \"https\" as we enable the SSL/TLS for secure communication. This applies to any client side request. The EdgeX microservice IP address where the request is sent changed to the host/IP address of API gateway service (recall the API gateway becomes the single entry point for all the EdgeX micro services). The API gateway will eventually lateral the request to the Core Data service if the client is authorized. The port of the request is switched from 48080 to 8443, which is the default SSL/TLS port for API gateway (versus the micro service port). This applies to any client side request. The \"/coredata/\" path in the URL is used to identify which EdgeX micro service the request is routed to. As each EdgeX micro service has a dedicated service port open that accepts incoming requests, there is a mapping table kept by the API gateway that maps paths to micro service ports. A partial listing of the map between ports and URL paths is shown in the table below. EdgeX microservice Name Port number Partial URL coredata 48080 coredata metadata 48081 metadata command 48082 command notifications 48060 notifications supportlogging 48061 supportlogging Creating Access Token for API Gateway Authentication If the EdgeX API gateway is not in use, a client can access and use any REST API provided by the EdgeX microservices by sending an HTTP request to the service endpoint. E.g., a client can consume the ping endpoint of the Core Data microservice with curl command like this: : curl http://<core-data-microservice-ip>:48080/api/v1/ping Again, the request doesn't include client identity information. Once the API gateway is started and initialized successfully, the EdgeX microservice REST APIs will be behind the gateway, an access token must be attached with any client-side HTTP request for identity verification and authentication done by the API gateway. This access token is different from the access token of secret store even though they have the same name. The purpose of the access token for the API gateway is to identity clients that send the requests to consume the REST API of EdgeX. The secret store will then use the token to verify the identity of clients that send the request to access the secrets of EdgeX that are stored in the secret store. To obtain an access token for a client, a user that is associated with the client as well as a group that the user belongs to needs to be added into the API gateway. The command to add a user and the group is: : docker-compose -f docker-compose-nexus-mongo.yml run --rm --entrypoint /edgex/security-proxy-setup edgex-proxy --init=false --useradd=<user> --group=<groupname> The command above will return an access token that can then be used by the client to access the EdgeX REST API resources. Depending on the choice of authentication method, the format of the access token will be something like this if JWT is enabled: : eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiI5M3V3cmZBc0xzS2Qwd1JnckVFdlRzQloxSmtYOTRRciIsImFjY291bnQiOiJhZG1pbmlzdHJhdG9yIn0.em8ffitqrd59_DeYKfQkTZGtUA1T99NikETwtedOgHQ Alternatively, the access token may look like what is shown below if the OAuth2 is enabled: : MNsBh6jDDSxaECzUtimW1nDSvI2v0xsZ If a client needs to be disabled and the client's token invalidated, use the command here to remove/delete the user: : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go \u2013-userdel=<user> Using API Gateway to Proxy Existing EdgeX Microservices Once the resource mapping and access token to API gateway are in place, a client can use the access token to use the protected EdgeX REST API resources behind the API gateway. Again, without the API Gateway in place, here is the sample request to hit the ping endpoint of the EdgeX Core Data microservice using curl: : curl http://<core-data-microservice-ip>:48080/api/v1/ping With the security service and JWT authentication is enabled, the command changes to: : curl \u2013H \u201chost: edgex\u201d https://<api-gateway-service-ip>:8443/coredata/api/v1/ping? -H \"Authorization: Bearer <access-token>\u201d In summary the difference between the two commands are listed below: --H \"host: edgex\" is used to indicate that the request is for EdgeX domain as the API gateway could be used to take requests for different domains. Use the https versus http protocol identifier for SSL/TLS secure communication. The service port 8443 is the default TLS service port of API gateway Use the URL path \"coredata\" to indicate which EdgeX microservice the request is routed to Use header of -H \"Authorization: Bearer \\<access-token>\" to specify the access token associated with the client that was generated when the client was added. The format for OAuth2 authentication is similar. For OAuth2 use the bearer token from OAuth2 authentication instead of the JWT token. Here is an example of the curl command using OAuth2: : curl \u2013H \"host: edgex\" https://<api-gateway-service-ip>:8443/coredata/api/v1/ping -H \"Authorization:bearer <access-token>\"","title":"API Gateway"},{"location":"microservices/security/Ch-APIGateway/#api-gateway","text":"The security API gateway is the single point of entry for all EdgeX REST traffic. It is the barrier between external clients and the EdgeX microservices preventing unauthorized access to EdgeX REST APIs. The API gateway accepts client requests, verifies the identity of the clients, redirects the requests to correspondent microservice and relays the results back to the client. The API Gateway provides an HTTP REST interface for administration management. The administrative management offers the means to configure API routing, as well as client authentication and access control. This configuration is store in an embedded database. KONG ( https://konghq.com/ ) is the product underlying the API gateway. The EdgeX community has added code to initialize the KONG environment, set up service routes for EdgeX microservices, and add various authentication/authorization mechanisms including JWT authentication, OAuth2 authentication and ACL.","title":"API Gateway"},{"location":"microservices/security/Ch-APIGateway/#start-the-api-gateway","text":"Start the API gateway with Docker Compose and a Docker Compose manifest file (the Docker Compose file named docker-compose-nexus-{redis,mongo}.yml (or -arm64 variabnts) found at https://github.com/edgexfoundry/developer-scripts/tree/master/releases/geneva/compose-files )). This Compose file starts all of EdgeX including the security services. The command to start EdgeX inclusive of API gateway related services is: : docker-compose up -d For debugging purpose, the API gateway services can be started individually with these commands used in sequence after secret store starts successfully. Lines starts with # are comments to explain the purpose of the command. : docker - compose up - d kong - db # start up backend database for API gateway docker - compose up - d kong - migrations # initialize the backend database for API gateway docker - compose up - d kong # start up KONG the major component of API gateway docker - compose up - d edgex - proxy # initialize KONG , configure proxy routes , apply certificates to routes , and enable various authentication / ACL features . If the last command returns an error message for any reason (such as incorrect configuration file), the API gateway may be in an unstable status. The following command can be used to stop and remove the containers. : docker-compose down # stop and remove the containers After stopping and removing the containers, you can attempt to recreate and start them again. Alternatively you can use the command to reset the API gateway as shown below: : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go --reset=true After issuing the reset command, attempt to start and reinitialize with the command below. : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go --init=true You can learn more about these commands, to include some additional options by running: : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go \u2013h On successfully starting EdgeX with the API Gateway services, the list of running containers should close follow the listing shown below. Note key security service containers like kong, kong-db, edgex-vault are listed.","title":"Start the API Gateway"},{"location":"microservices/security/Ch-APIGateway/#configuring-api-gateway","text":"The API gateway supports two different forms of authentication: JSON Web Token (JWT) or OAuth2 Authentication. Only one authentication method can be enabled at a time. The API Gateway also supports an Access Control List (ACL) which can be enabled with one of the authentication methods mentioned earlier for fine control among the groups. The authentication and ACL need to be specified in the API gateway's configuration file. Setup of authentication and access control occurs automatically as part of API gateway initialization. The configuration file can be found at https://github.com/edgexfoundry/security-api-gateway/blob/master/core/res/configuration-docker.toml Configuration of JWT Authentication for API Gateway When using JWT Authentication, the [kongauth] section needs to be specified in the configuration file as shown below. : [kongauth] name = \"jwt\" Configuration of OAuth2 Authentication for API Gateway When using OAuth2 Authentication, the [kongauth] section needs to specify oauth2 in the configuration file as shown below. Note, today EdgeX only supports \"client credential\" authentication (specified in \"granttype\") currently for OAuth. [kongauth] name = \"oauth2\" scopes = \"email,phone,address\" mandatoryscope = \"true\" enableclientcredentials = \"true\" clientid = \"test\" clientsecret = \"test\" redirecturi = \"http://edgex.com\" granttype = \"client_credentials\" scopegranted = \"email\" resource = \"coredata\" Configuration of ACL for API Gateway Access control is also specified in the configuration file as shown below. Note, users that belong to the whitelist will have access to the resources of EdgeX, and users not belonging to the group listed here will be denied when trying to access resources through the API Gateway. : [kongacl] name = \"acl\" whitelist = \"admin,user\" Configuration of Adding Microservices Routes for API Gateway For the current pre-existing Kong routes, they are configured and initialized statically through configuration TOML file specified in security-proxy-setup application. This is not sufficient for some other new additional microservices like application services, for example. Thus, adding new proxy Kong routes are now possibly achieved via the environment variable, ADD_PROXY_ROUTE , of service edgex-proxy in the docker-compose file. Here is an example: edgex-proxy : ... environment : ... ADD_PROXY_ROUTE : \"myApp.http://my-app:56789\" ... ... my-app : ... container_name : myApp hostname : myApp ... The value of ADD_PROXY_ROUTE takes a comma-separated list of one or more (at least one) paired additional service name and URL for which to create proxy Kong routes. The paired specification is given as the following: \\<RoutePrefix>.\\<TargetRouteURL> where RoutePrefix is the name of service which requests to create proxy Kong route and it is case insensitive; it is the docker network hostname of the service that want to add the proxy Kong route in the docker-compose file if running from docker-compose, for example, myApp in this case. TargetRouteURL is the full qualified URL for the target service, like http://my-app:56789 So as an example, for a single service, the value of ADD_PROXY_ROUTE would be: \" myApp.http://my-app:56789 \". Once ADD_PROXY_ROUTE is configured and composed-up successfully, the proxy route then can be accessed the app's REST API via Kong as http://localhost:8000/myApp/api/v1/... in the same way you would access the edgex service in which you will also need an access token and it is using default access role if not specified in the TOML configuration file as well.","title":"Configuring API Gateway"},{"location":"microservices/security/Ch-APIGateway/#using-api-gateway","text":"Resource Mapping between EdgeX Microservice and API gateway If the EdgeX API gateway is not in use, a client can access and use any REST API provided by the EdgeX microservices by sending an HTTP request to the service endpoint. E.g., a client can consume the ping endpoint of the Core Data microservice with curl command like this: : curl http://<core-data-microservice-ip>:48080/api/v1/ping Once the API gateway is started and initialized successfully, and all the common ports for EdgeX microservices are blocked by disabling the exposed external ports of the EdgeX microservices through updating the docker compose file, the EdgeX microservice will be behind the gateway. At this time both the microservice host/IP Address (\\<core-data-microservice-ip> in the example) as well as the service port (48080 in the example) are not available to external access. EdgeX uses the gateway as a single entry point for all the REST APIs. With the API gateway in place, the curl command to ping the endpoint of the same Core Data service, as shown above, needs to change to : : curl https://<api-gateway-host-ip>:8443/coredata/api/v1/ping Comparing these two curl commands you may notice several differences. \"Http\" is switched to \"https\" as we enable the SSL/TLS for secure communication. This applies to any client side request. The EdgeX microservice IP address where the request is sent changed to the host/IP address of API gateway service (recall the API gateway becomes the single entry point for all the EdgeX micro services). The API gateway will eventually lateral the request to the Core Data service if the client is authorized. The port of the request is switched from 48080 to 8443, which is the default SSL/TLS port for API gateway (versus the micro service port). This applies to any client side request. The \"/coredata/\" path in the URL is used to identify which EdgeX micro service the request is routed to. As each EdgeX micro service has a dedicated service port open that accepts incoming requests, there is a mapping table kept by the API gateway that maps paths to micro service ports. A partial listing of the map between ports and URL paths is shown in the table below. EdgeX microservice Name Port number Partial URL coredata 48080 coredata metadata 48081 metadata command 48082 command notifications 48060 notifications supportlogging 48061 supportlogging Creating Access Token for API Gateway Authentication If the EdgeX API gateway is not in use, a client can access and use any REST API provided by the EdgeX microservices by sending an HTTP request to the service endpoint. E.g., a client can consume the ping endpoint of the Core Data microservice with curl command like this: : curl http://<core-data-microservice-ip>:48080/api/v1/ping Again, the request doesn't include client identity information. Once the API gateway is started and initialized successfully, the EdgeX microservice REST APIs will be behind the gateway, an access token must be attached with any client-side HTTP request for identity verification and authentication done by the API gateway. This access token is different from the access token of secret store even though they have the same name. The purpose of the access token for the API gateway is to identity clients that send the requests to consume the REST API of EdgeX. The secret store will then use the token to verify the identity of clients that send the request to access the secrets of EdgeX that are stored in the secret store. To obtain an access token for a client, a user that is associated with the client as well as a group that the user belongs to needs to be added into the API gateway. The command to add a user and the group is: : docker-compose -f docker-compose-nexus-mongo.yml run --rm --entrypoint /edgex/security-proxy-setup edgex-proxy --init=false --useradd=<user> --group=<groupname> The command above will return an access token that can then be used by the client to access the EdgeX REST API resources. Depending on the choice of authentication method, the format of the access token will be something like this if JWT is enabled: : eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiI5M3V3cmZBc0xzS2Qwd1JnckVFdlRzQloxSmtYOTRRciIsImFjY291bnQiOiJhZG1pbmlzdHJhdG9yIn0.em8ffitqrd59_DeYKfQkTZGtUA1T99NikETwtedOgHQ Alternatively, the access token may look like what is shown below if the OAuth2 is enabled: : MNsBh6jDDSxaECzUtimW1nDSvI2v0xsZ If a client needs to be disabled and the client's token invalidated, use the command here to remove/delete the user: : docker run \u2013network=edgex-network edgexfoundry/docker-edgex-proxy-go \u2013-userdel=<user> Using API Gateway to Proxy Existing EdgeX Microservices Once the resource mapping and access token to API gateway are in place, a client can use the access token to use the protected EdgeX REST API resources behind the API gateway. Again, without the API Gateway in place, here is the sample request to hit the ping endpoint of the EdgeX Core Data microservice using curl: : curl http://<core-data-microservice-ip>:48080/api/v1/ping With the security service and JWT authentication is enabled, the command changes to: : curl \u2013H \u201chost: edgex\u201d https://<api-gateway-service-ip>:8443/coredata/api/v1/ping? -H \"Authorization: Bearer <access-token>\u201d In summary the difference between the two commands are listed below: --H \"host: edgex\" is used to indicate that the request is for EdgeX domain as the API gateway could be used to take requests for different domains. Use the https versus http protocol identifier for SSL/TLS secure communication. The service port 8443 is the default TLS service port of API gateway Use the URL path \"coredata\" to indicate which EdgeX microservice the request is routed to Use header of -H \"Authorization: Bearer \\<access-token>\" to specify the access token associated with the client that was generated when the client was added. The format for OAuth2 authentication is similar. For OAuth2 use the bearer token from OAuth2 authentication instead of the JWT token. Here is an example of the curl command using OAuth2: : curl \u2013H \"host: edgex\" https://<api-gateway-service-ip>:8443/coredata/api/v1/ping -H \"Authorization:bearer <access-token>\"","title":"Using API Gateway"},{"location":"microservices/security/Ch-AccessEdgeXRESTResources/","text":"Access EdgeX REST resources When the EdgeX API Gateway is used, access to the micro service APIs must go through the reverse proxy. Requestors of an EdgeX REST endpoints must therefore change the URL they use to access the services. Example below explain how to map the non-secured micro service URLs with reverse proxy protected URLS. To access the ping endpoint of an EdgeX micro service (using the command service as an example), the URL is http://edgex-command-service:48082/api/v1/ping With API gateway serving as the single access point for the EdgeX services, the ping URL is https://api-gateway-server:8443/command/api/v1/ping?jwt= \\<JWT-Token> Please notice that there are 4 major differences when comparing the URLs above Switch from http to https as the API Gateway server enables https The host address and port are switched from original micro service host address and port to a common api gateway service address and 8443 port as the api gateway server will serve as the single point for all the EdgeX services Use the name of the service (in this case \"command\") within the URL to indicate that the request is to be routed to the appropriate EdgeX service (command in this example) Add a JWT as part of the URL as all the REST resources are protected by either OAuth2 or JWT authentication. The JWT can be obtained when a user account is created with the security API Gateway.","title":"Access EdgeX REST resources"},{"location":"microservices/security/Ch-AccessEdgeXRESTResources/#access-edgex-rest-resources","text":"When the EdgeX API Gateway is used, access to the micro service APIs must go through the reverse proxy. Requestors of an EdgeX REST endpoints must therefore change the URL they use to access the services. Example below explain how to map the non-secured micro service URLs with reverse proxy protected URLS. To access the ping endpoint of an EdgeX micro service (using the command service as an example), the URL is http://edgex-command-service:48082/api/v1/ping With API gateway serving as the single access point for the EdgeX services, the ping URL is https://api-gateway-server:8443/command/api/v1/ping?jwt= \\<JWT-Token> Please notice that there are 4 major differences when comparing the URLs above Switch from http to https as the API Gateway server enables https The host address and port are switched from original micro service host address and port to a common api gateway service address and 8443 port as the api gateway server will serve as the single point for all the EdgeX services Use the name of the service (in this case \"command\") within the URL to indicate that the request is to be routed to the appropriate EdgeX service (command in this example) Add a JWT as part of the URL as all the REST resources are protected by either OAuth2 or JWT authentication. The JWT can be obtained when a user account is created with the security API Gateway.","title":"Access EdgeX REST resources"},{"location":"microservices/security/Ch-SecretStore/","text":"Secret Store There are all kinds of secrets used within EdgeX Foundry micro services, such as tokens, passwords, certificates etc. The secret store serves as the central repository to keep these secrets. The developers of other EdgeX Foundry micro services utilize the secret store to create, store and retrieve secrets relevant to their corresponding micro service. The communications the between secret store and other micro services are secured by TLS. Currently the EdgeX Foundry secret store is implemented with Vault , a HashiCorp open source software product. Vault is a tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, database credentials, service credentials, or certificates. Vault provides a unified interface to any secret, while providing tight access control and multiple authentication mechanisms (token, LDAP, etc.). Vault adds on key rolling, revocation rules, time-to-live access tokens, secure storage, Shamir Secret Sharing based unlocking mechanism, high availability and detailed auditing. Vault can use several backend systems (filesystem, databases, Consul, Etcd, S3, Azure, etc.) to securely store every sensitive asset. The current EdgeX Foundry implementation of Vault is using Consul , another HashiCorp open source software product. Consul is a distributed service mesh to connect, secure, and configure services across any runtime platform and public or private cloud. Consul uses a consensus protocol to provide Consistency as defined by CAP . The consensus protocol is based on \"Raft: In search of an Understandable Consensus Algorithm\" . For a visual explanation of Raft, see The Secret Lives of Data . The seamless integration of Vault and Consul provides a strong yet simple infrastructure to setup a reliable high availability architecture (Vault failover nodes, Consul Clustering) for the EdgeX Foundry Security services in production. The key features of Vault are: Secure Secret Storage: Arbitrary key/value secrets can be stored in Vault. Vault encrypts these secrets prior to writing them to persistent storage, so gaining access to the raw storage isn't enough to access your secrets. Authentication mechanisms (internal and/or external) and authorizations based upon policies provide access management. Dynamic Secrets: Vault can generate secrets on-demand for some systems, automatically revoking them after the lease is up. Data Encryption: Vault can encrypt and decrypt data without storing it. Leasing and Renewal: All secrets in Vault have a lease associated with them (automatic revocation). However, clients can renew leases via built-in renew APIs. Revocation: Vault has built-in support for secret revocation. Single secrets, but also a tree of secrets. Revocation assists in key rolling as well as locking down systems in the case of an intrusion. Start the Secret Store Start the Secret Store with Docker Compose and a Docker Compose manifest file. The whole set of Docker Compose files for Geneva release can be found here: https://github.com/edgexfoundry/developer-scripts/blob/master/releases/geneva/compose-files The Compose file starts the entire EdgeX Foundry platform with Redis including the security services. The command to start EdgeX Foundry platform including the Secret Store and API gateway related services is: sh> docker-compose up -d For a pristine run, it is strongly recommended to thoroughly clean up any Docker artifacts remaining from the current run. sh> docker-compose down -v The \"down\" operation will remove containers, network and adding the -v option it will also remove persistent volumes: Stopping edgex-vault ... done Stopping edgex-core-consul ... done Stopping edgex-files ... done Removing edgex-vault-worker ... done Removing edgex-vault ... done Removing edgex-config-seed ... done Removing edgex-core-consul ... done Removing edgex-files ... done Removing network security-secret-store_edgex-network Removing volume security-secret-store_db-data Removing volume security-secret-store_log-data Removing volume security-secret-store_consul-config Removing volume security-secret-store_consul-data Removing volume security-secret-store_vault-config Removing volume security-secret-store_vault-file Removing volume security-secret-store_vault-logs Troubleshooting steps For debugging purpose, the Secret Store services can be started individually with these commands. A pre-requisite being to carefully follow the invocation sequence in order to avoid any dependency failure. Lines starting with # (hashtag) are contextual comments to explicit the purpose of the above corresponding command: Clean-up sh> cd <path-to-EdgeX-Foundry-Secret-Store> # <...>/security-secret-store/ sh> docker-compose down -v sh> docker-compose ps # Check no previous container is running sh> docker volume ls # Check and remove any previous persistent and/or unused volumes sh> docker volume prune sh> docker volume rm <volume-name> sh> docker network ls # Check and remove the previous EdgeX Foundry Docker network sh> docker network rm edgex-network Start the first service: volume (platform volume initializations) sh> docker-compose up -d volume Sample output: Creating network \"security-secret-store_edgex-network\" with driver \"bridge\" Creating volume \"security-secret-store_db-data\" with default driver Creating volume \"security-secret-store_log-data\" with default driver Creating volume \"security-secret-store_consul-config\" with default driver Creating volume \"security-secret-store_consul-data\" with default driver Creating volume \"security-secret-store_vault-config\" with default driver Creating volume \"security-secret-store_vault-file\" with default driver Creating volume \"security-secret-store_vault-logs\" with default driver Creating edgex-files ... done Start the second service: consul (Consul is Vault store backend) sh> docker-compose up -d consul Sample output: edgex-files is up-to-date Creating edgex-core-consul ... done Display and inspect consul service logs: important lines are highlighted sh> docker-compose logs consul Sample output: Attaching to edgex-core-consul edgex-core-consul | ==> Starting Consul agent... edgex-core-consul | ==> Consul agent running! edgex-core-consul | Version: 'v1.1.0' edgex-core-consul | Node ID: '371cbce6-02a8-65f6-ddea-6df5c40a4c50' edgex-core-consul | Node name: 'edgex-core-consul' edgex-core-consul | Datacenter: 'dc1' (Segment: '<all>') edgex-core-consul | Server: true (Bootstrap: false) edgex-core-consul | Client Addr: [0.0.0.0] (HTTP: 8500, HTTPS: -1, DNS: 8600) edgex-core-consul | Cluster Addr: 127.0.0.1 (LAN: 8301, WAN: 8302) edgex-core-consul | Encrypt: Gossip: false, TLS-Outgoing: false, TLS-Incoming: false edgex-core-consul | edgex-core-consul | ==> Log data will now stream in as it occurs: edgex-core-consul | edgex-core-consul | 2019/01/13 13:25:06 [DEBUG] agent: Using random ID \"371cbce6-02a8-65f6-ddea-6df5c40a4c50\" as node ID edgex-core-consul | 2019/01/13 13:25:06 [INFO] raft: Initial configuration (index=1): [{Suffrage:Voter ID:371cbce6-02a8-65f6-ddea-6df5c40a4c50 Address:127.0.0.1:8300}] edgex-core-consul | 2019/01/13 13:25:06 [INFO] raft: Node at 127.0.0.1:8300 [Follower] entering Follower state (Leader: \"\") edgex-core-consul | 2019/01/13 13:25:06 [INFO] serf: EventMemberJoin: edgex-core-consul.dc1 127.0.0.1 edgex-core-consul | 2019/01/13 13:25:06 [INFO] serf: EventMemberJoin: edgex-core-consul 127.0.0.1 edgex-core-consul | 2019/01/13 13:25:06 [INFO] consul: Adding LAN server edgex-core-consul (Addr: tcp/127.0.0.1:8300) (DC: dc1) edgex-core-consul | 2019/01/13 13:25:06 [INFO] consul: Handled member-join event for server \"edgex-core-consul.dc1\" in area \"wan\" edgex-core-consul | 2019/01/13 13:25:06 [INFO] agent: Started DNS server 0.0.0.0:8600 (tcp) edgex-core-consul | 2019/01/13 13:25:06 [INFO] agent: Started DNS server 0.0.0.0:8600 (udp) edgex-core-consul | 2019/01/13 13:25:06 [INFO] agent: Started HTTP server on [::]:8500 (tcp) edgex-core-consul | 2019/01/13 13:25:06 [INFO] agent: started state syncer edgex-core-consul | 2019/01/13 13:25:06 [WARN] raft: Heartbeat timeout from \"\" reached, starting election edgex-core-consul | 2019/01/13 13:25:06 [INFO] raft: Node at 127.0.0.1:8300 [Candidate] entering Candidate state in term 2 edgex-core-consul | 2019/01/13 13:25:06 [DEBUG] raft: Votes needed: 1 edgex-core-consul | 2019/01/13 13:25:06 [DEBUG] raft: Vote granted from 371cbce6-02a8-65f6-ddea-6df5c40a4c50 in term 2. Tally: 1 edgex-core-consul | 2019/01/13 13:25:06 [INFO] raft: Election won. Tally: 1 edgex-core-consul | 2019/01/13 13:25:06 [INFO] raft: Node at 127.0.0.1:8300 [Leader] entering Leader state edgex-core-consul | 2019/01/13 13:25:06 [INFO] consul: cluster leadership acquired Start the third service: config-seed (platform configuration initializations) sh> docker-compose up -d config-seed Sample output: edgex-files is up-to-date edgex-core-consul is up-to-date Creating edgex-config-seed ... done Display and inspect the created container states: important lines are highlighted sh> docker-compose ps Sample output: Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------------------edgex-config-seed /bin/sh -c /edgex/cmd/conf ... Exit 0 edgex-core-consul docker-entrypoint.sh agent ... Up 8300/tcp, 8301/tcp, 8301/udp, 8302/tcp, 8302/udp, 0.0.0.0:8400->8400/tcp, 0.0.0.0:8500->8500/tcp, 0.0.0.0:8600->8600/tcp, 8600/udp edgex-files /bin/sh -c /usr/bin/tail - ... Up Note Line 3: edgex-config-seed service has exited after successful processing (exit code 0) Start the fourth service: vault (Vault tool) Note Vault will be uninitialized and unsealed upon success. The vault-worker service will process the initialization and unsealing tasks. sh> docker-compose up -d vault Sample output: edgex-files is up-to-date edgex-core-consul is up-to-date Creating edgex-vault ... done Display and inspect \"vault\" service logs: important lines are highlighted sh> docker-compose logs vault Sample output: Attaching to edgex-vault edgex-vault | ==> Vault server configuration: edgex-vault | edgex-vault | Api Address: https://edgex-vault:8200 edgex-vault | Cgo: disabled edgex-vault | Cluster Address: https://edgex-vault:8201 edgex-vault | Listener 1: tcp (addr: \"edgex-vault:8200\", cluster address: \"edgex-vault:8201\", tls: \"enabled\") edgex-vault | Log Level: info edgex-vault | Mlock: supported: true, enabled: true edgex-vault | Storage: consul (HA available) edgex-vault | Version: Vault v0.10.2 edgex-vault | Version Sha: 3ee0802ed08cb7f4046c2151ec4671a076b76166 edgex-vault | edgex-vault | ==> Vault server started! Log data will stream in below: edgex-vault | Note Line 4 & 7: Vault API endpoint on port 8200 (lines 4 and 7). Line 7: Vault has TLS enabled. Line 10: Vault backend storage is Consul . Start the fifth service: vault-worker (Vault init/unseal process and setups) sh> docker-compose up -d vault-worker Sample output: edgex-files is up-to-date edgex-core-consul is up-to-date edgex-vault is up-to-date Creating edgex-vault-worker ... done Display and inspect \"vault-worker\" service logs: important lines are highlighted sh> docker-compose logs vault-worker Sample output: Attaching to edgex-vault-worker edgex-vault-worker | INFO: 2019/01/13 13:35:42 successful loading the rootCA cert. edgex-vault-worker | INFO: 2019/01/13 13:35:43 {\"keys\":[\"564b9444eebe28b393c21a4dca1e32835b7dc27f5da03b73d22b666cb20224a9\"],\"keys_base64\":[\"VkuURO6+KLOTwhpNyh4yg1t9wn9doDtz0itmbLICJKk=\"],\"recovery_keys\":null,\"recovery_keys_base64\":null,\"root_token\":\"01dbbae4-353a-8cdf-8189-4d50e5535a6f\"} edgex-vault-worker | INFO: 2019/01/13 13:35:43 Vault has been initialized successfully. edgex-vault-worker | INFO: 2019/01/13 13:35:43 Vault has been unsealed successfully. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Vault Health Check HTTP Status: 200 OK (StatusCode: 200) edgex-vault-worker | INFO: 2019/01/13 13:35:48 Verifying Admin policy file hash (SHA256). edgex-vault-worker | INFO: 2019/01/13 13:35:48 Vault policy file checksum (SHA256): 5ce8d58cf7d931735f6532742f677c109a91a263bcefe9aef73ab2a69f4b43d3 edgex-vault-worker | INFO: 2019/01/13 13:35:48 Reading Admin policy file. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Importing Vault Admin policy. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Import Policy Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Reading Kong policy file. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Importing Vault Kong policy. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Import Policy Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Creating Vault Admin token. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Create Token Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Creating Vault Kong token. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Create Token Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Successful on reading certificate from v1/secret/edgex/pki/tls/edgex-kong. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Cert&key are not in the secret store yet, will need to upload them. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Load cert&key pair from volume successfully, now will upload to secret store. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Trying to upload cert&key to secret store. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Successful to add certificate to the secret store. Display and inspect \"vault\" service logs: important lines are highlighted sh> docker-compose logs vault Sample output: Attaching to edgex-vault edgex-vault | ==> Vault server configuration: edgex-vault | edgex-vault | Api Address: https://edgex-vault:8200 edgex-vault | Cgo: disabled edgex-vault | Cluster Address: https://edgex-vault:8201 edgex-vault | Listener 1: tcp (addr: \"edgex-vault:8200\", cluster address: \"edgex-vault:8201\", tls: enabled\") edgex-vault | Log Level: info edgex-vault | Mlock: supported: true, enabled: true edgex-vault | Storage: consul (HA available) edgex-vault | Version: Vault v0.10.2 edgex-vault | Version Sha: 3ee0802ed08cb7f4046c2151ec4671a076b76166 edgex-vault | edgex-vault | ==> Vault server started! Log data will stream in below: edgex-vault | edgex-vault | 2019-01-13T13:35:42.549Z [INFO ] core: security barrier not initialized edgex-vault | 2019-01-13T13:35:42.551Z [INFO ] core: security barrier not initialized edgex-vault | 2019-01-13T13:35:42.554Z [INFO ] core: security barrier initialized: shares=1 threshold=1 edgex-vault | 2019-01-13T13:35:42.575Z [INFO ] core: post-unseal setup starting edgex-vault | 2019-01-13T13:35:42.583Z [INFO ] core: loaded wrapping token key edgex-vault | 2019-01-13T13:35:42.583Z [INFO ] core: successfully setup plugin catalog: plugin-directory= edgex-vault | 2019-01-13T13:35:42.584Z [INFO ] core: no mounts; adding default mount table edgex-vault | 2019-01-13T13:35:42.585Z [INFO ] core: successfully mounted backend: type=kv path=secret/ edgex-vault | 2019-01-13T13:35:42.586Z [INFO ] core: successfully mounted backend: type=cubbyhole path=cubbyhole/ edgex-vault | 2019-01-13T13:35:42.586Z [INFO ] core: successfully mounted backend: type=system path=sys/ edgex-vault | 2019-01-13T13:35:42.586Z [INFO ] core: successfully mounted backend: type=identity path=identity/ edgex-vault | 2019-01-13T13:35:42.593Z [INFO ] core: restoring leases edgex-vault | 2019-01-13T13:35:42.593Z [INFO ] rollback: starting rollback manager edgex-vault | 2019-01-13T13:35:42.594Z [INFO ] expiration: lease restore complete edgex-vault | 2019-01-13T13:35:42.596Z [INFO ] identity: entities restored edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] identity: groups restored edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] core: post-unseal setup complete edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] core: core/startClusterListener: starting listener: listener_address=172.19.0.4:8201 edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] core: core/startClusterListener: serving cluster requests: cluster_listen_address=172.19.0.4:8201 edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: root token generated edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: pre-seal teardown starting edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: stopping cluster listeners edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: shutting down forwarding rpc listeners edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: forwarding rpc listeners stopped edgex-vault | 2019-01-13T13:35:43.099Z [INFO ] core: rpc listeners successfully shut down edgex-vault | 2019-01-13T13:35:43.099Z [INFO ] core: cluster listeners successfully shut down edgex-vault | 2019-01-13T13:35:43.100Z [INFO ] rollback: stopping rollback manager edgex-vault | 2019-01-13T13:35:43.100Z [INFO ] core: pre-seal teardown complete edgex-vault | 2019-01-13T13:35:43.105Z [INFO ] core: vault is unsealed edgex-vault | 2019-01-13T13:35:43.105Z [INFO ] core: entering standby mode edgex-vault | 2019-01-13T13:35:43.109Z [INFO ] core: acquired lock, enabling active operation edgex-vault | 2019-01-13T13:35:43.134Z [INFO ] core: post-unseal setup starting edgex-vault | 2019-01-13T13:35:43.135Z [INFO ] core: loaded wrapping token key edgex-vault | 2019-01-13T13:35:43.135Z [INFO ] core: successfully setup plugin catalog: plugin-directory= edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=kv path=secret/ edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=system path=sys/ edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=identity path=identity/ edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=cubbyhole path=cubbyhole/ edgex-vault | 2019-01-13T13:35:43.141Z [INFO ] core: restoring leases edgex-vault | 2019-01-13T13:35:43.142Z [INFO ] rollback: starting rollback manager edgex-vault | 2019-01-13T13:35:43.142Z [INFO ] expiration: lease restore complete edgex-vault | 2019-01-13T13:35:43.143Z [INFO ] identity: entities restored edgex-vault | 2019-01-13T13:35:43.143Z [INFO ] identity: groups restored edgex-vault | 2019-01-13T13:35:43.144Z [INFO ] core: post-unseal setup complete edgex-vault | 2019-01-13T13:35:43.144Z [INFO ] core: core/startClusterListener: starting listener: listener_address=172.19.0.4:8201 edgex-vault | 2019-01-13T13:35:43.144Z [INFO ] core: core/startClusterListener: serving cluster requests: cluster_listen_address=172.19.0.4:8201 Note Line 18: Vault initialization successful. Line 35: Vault root token generated. Line 44: Vault unsealing successful. Line 50: Vault key/value store secret successfully mounted . Line 60 & 61: Vault successfully started . Display and inspect the created container states: important lines are highlighted sh> docker-compose ps Sample output: Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------------------edgex-config-seed /bin/sh -c /edgex/cmd/conf ... Exit 0 edgex-core-consul docker-entrypoint.sh agent ... Up 8300/tcp, 8301/tcp, 8301/udp, 8302/tcp, 8302/udp, 0.0.0.0:8400->8400/tcp, 0.0.0.0:8500->8500/tcp, 0.0.0.0:8600->8600/tcp, 8600/udp edgex-files /bin/sh -c /usr/bin/tail - ... Up edgex-vault docker-entrypoint.sh serve ... Up 0.0.0.0:8200->8200/tcp edgex-vault-worker ./edgex-vault-worker --ini ... Exit 0 Note Line 9: edgex-vault-worker service has exited after successful processing (exit code 0) Display and inspect the created container volumes: important lines are highlighted sh> docker volume ls DRIVER VOLUME NAME local security-secret-store_consul-config local security-secret-store_consul-data local security-secret-store_db-data local security-secret-store_log-data local security-secret-store_vault-config local security-secret-store_vault-file local security-secret-store_vault-logs Display and inspect the container network ( security-secret-store_edgex-network ): important lines are highlighted sh> docker network ls NETWORK ID NAME DRIVER SCOPE 63227826fbc7 bridge bridge local 60763abffde3 host host local 1d236ab1dbbd none null local 0a7f7266d102 security-secret-store_edgex-network bridge local Using Consul Web UI For learning and verification purposes one might use the Consul Web UI interface to gather and double check specific Vault informations. Consul Web UI endpoint port is exposed by the Docker compose file. EdgeX Foundry platform uses the Consul default port number 8500. It is normally not recommended to expose Consul UI port number in production, at least the UI should not be accessible from outside the platform environment. However, because all the Vault secrets are encrypted before being transmitted and stored in the Consul backend, having access to Consul is not sufficient to access any secrets, the vault data encryption/decryption key would be absolutely necessary. Open a Web browser on http://<EdgeX Consul Server>:8500/ui . On the screenshot below, after selecting SERVICES and Vault , the UI will show the various Vault status (heartbeat and init/unseal states), coloring the boxes in green, orange or red depending on the level of importance (info, warning, error). By clicking each of the right side status indicators, more information will be accessible in order to better inspect any situation. As a practical example, we are going to navigate the Consul structure for Vault in order to check if the API Gateway (Kong) TLS certificate and private key were fetched and stored accordingly during the vault-worker process. First select KEY/VALUE menu, and then select vault root structure: We are now going to navigate deeper in the vault tree structure to reach and display the EdgeX Kong TLS assets. Continue by selecting logical/ : {.align-center width=\"348px\" height=\"287px\"} Then select d7809b... an arbitrary UID generated and created by Consul during Vault registration: {.align-center width=\"377px\" height=\"216px\"} Select edgex/ : {.align-center width=\"419px\" height=\"228px\"} Select pki/ : {.align-center width=\"417px\" height=\"222px\"} Select tls/ : {.align-center width=\"418px\" height=\"237px\"} Select edgex-kong/ : {.align-center width=\"423px\" height=\"233px\"} And we are now finally able to display the encrypted Vault secret containing the API Gateway (Kong) TLS server certificate and its corresponding private key. As you can see on the screenshot below the Vault key/value is encrypted and totally opaque to Consul, the Vault data encryption key (DEK) would be necessary to decrypt these secrets. Each Vault secret is encrypted before being transmitted to Consul node(s). Shell Access to Consul Container and Using Consul CLI sh> docker exec -it -e PS1 = '\\u@\\h:\\w \\$ ' edgex-core-consul sh root@edgex-core-consul:/ # consul members Node Address Status Type Build Protocol DC Segment edgex-core-consul 127 .0.0.1:8301 alive server 1 .1.0 2 dc1 <all> root@edgex-core-consul:/ # consul catalog nodes Node ID Address DC edgex-core-consul e49af36a 127 .0.0.1 dc1 root@edgex-core-consul:/ # consul catalog services consul edgex-mongo vault Note Line 5: Shows the Consul node status alive (1 node in EdgeX default configuration). Line 9: Shows the Consul nodes (1 node in EdgeX default configuration). Lines 12-14: Show the Consul registered services. Configuring the Secret Store Vault server configuration is essentially concentrated in one JSON file named local.json . This file was prepared during the Vault Docker image build process. In the eventuality of a change, the Vault server container should be accessed to then modify the JSON file. The absolute path being /vault/config/local.json . To reload the new configuration simply send Vault PID a HUP signal to trigger a configuration reload. Sample Vault server configuration file: listener \"tcp\" { address = \"edgex-vault:8200\" tls_disable = \"0\" cluster_address = \"edgex-vault:8201\" tls_min_version = \"tls12\" tls_client_ca_file =\"/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem\" tls_cert_file =\"/vault/config/pki/EdgeXFoundryCA/edgex-vault.pem\" tls_key_file = \"/vault/config/pki/EdgeXFoundryCA/edgex-vault.priv.key\" } backend \"consul\" { path = \"vault/\" address = \"edgex-core-consul:8500\" scheme = \"http\" redirect_addr = \"https://edgex-vault:8200\" cluster_addr = \"https://edgex-vault:8201\" } default_lease_ttl = \"168h\" max_lease_ttl = \"720h\" The listener clause refers to Vault server process (port, TLS and server name), the backend clause refers to the storage backend (i.e. Consul). To modify this configuration file, execute a shell session in the running Vault container: sh> docker exec -it -e PS1 = '\\u@\\h:\\w \\$ ' -e VAULT_CAPATH = '/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem' edgex-vault sh root@edgex-vault:/vault # ls -l total 12 drwxr-xr-x 4 vault vault 4096 Jan 13 13 :34 config drwxr-xr-x 2 vault vault 4096 Jun 7 2018 file drwxr-xr-x 2 vault vault 4096 Jun 7 2018 logs Pay attention to the VAULT_CAPATH environment variable passed to the session. This is necessary in order to run succesful Vault CLI command. Every Vault CLI command is a wrapper of the Vault HTTP API. The Vault server is configured with TLS using X.509 PKI materials generated and signed by a local self-signed CA (EdgeXFoundryCA). Therefore, in order for each Vault CLI command (or to that extent cURL commands) to verify the Vault server TLS certificate, the self-signing CA root certificate would have to be known by the CLI command interpreter. This VAULT_CAPATH variable is checked by every Vault CLI commands, alternatively each Vault CLI command can specify an option with the same certificate path if the variable is not set. The self-signed Root CA certificate path can be found in the Vault configuration file (see above local.json), with parameter tls_client_ca_file =\"/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem\" . The local.json configuration file can be read and modified within the running container: <root@edgex-vault>:/vault \\# cat config/local.json listener \"tcp\" { address = \"edgex-vault:8200\" tls\\_disable = \"0\" cluster\\_address =\"edgex-vault:8201\" tls\\_min\\_version = \"tls12\" tls\\_client\\_ca\\_file=\"/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem\" tls\\_cert\\_file=\"/vault/config/pki/EdgeXFoundryCA/edgex-vault.pem\" tls\\_key\\_file = \"/vault/config/pki/EdgeXFoundryCA/edgex-vault.priv.key\" } backend \"consul\" { path = \"vault/\" address = \"edgex-core-consul:8500\" scheme = \"http\" redirect\\_addr =\"<https://edgex-vault:8200>\" cluster\\_addr =\"<https://edgex-vault:8201>\" } default\\_lease\\_ttl = \"168h\" max\\_lease\\_ttl = \"720h\" A sample Vault CLI command to check Vault status: root@edgex-vault:/vault # vault status Key Value --- ----- Seal Type shamir Sealed false Total Shares 1 Threshold 1 Version 0 .10.2 Cluster Name vault-cluster-57b3c4ed Cluster ID fe6d18bf-fa9c-0d52-3278-bca0390af023 HA Enabled true HA Cluster https://edgex-vault:8201 HA Mode active All the X.509 PKI materials including the self-signing CA are located under /vault/config/pki/EdgeXFoundryCA . root@edgex-vault:/vault # ls -l config/pki/EdgeXFoundryCA/ total 24 -rw-r--r-- 1 vault vault 956 Dec 5 14 :05 EdgeXFoundryCA.pem -r-------- 1 vault vault 306 Dec 5 14 :05 EdgeXFoundryCA.priv.key -rw-r--r-- 1 vault vault 989 Dec 5 14 :05 edgex-kong.pem -rw------- 1 vault vault 306 Dec 5 14 :05 edgex-kong.priv.key -rw-r--r-- 1 vault vault 1001 Dec 5 14 :05 edgex-vault.pem -rw------- 1 vault vault 306 Dec 5 14 :05 edgex-vault.priv.key Note Line 3: self-signing root CA certificate. Line 4: self-signing root CA private key. Line 5: API Gateway (Kong) TLS server certificate. Line 6: API Gateway (Kong) TLS server certificate private key. Line 7: Vault TLS server certificate. Line 8: Vault TLS server certificate private key. The CA name (EdgeXFoundryCA) was defined by the pkisetup tool during the Vault image build process. This tool is also responsible for all the TLS server configuration and creation tasks. If you are willing to change any of the Vault X.509 PKI assets or configuration parameters you will have to modify the pkisetup-vault.json file and rebuild a new Vault Docker image. Similarly to Vault, each EdgeX Foundry service having a TLS server certificate and private key had its X.509 PKI assets generated and signed during the Vault Docker image build process. Therefore, the API Gateway (Kong) configuration file named pkisetup-kong.json would have to be modified accordingly. A new Vault Docker image would have to be built. The Vault Dockerfile contains the pkisetup executions, see below for a corresponding excerpt (highlighted lines): # Create assets folder (needed for unseal key/s, root token and tmp) # Run CA/Vault and Kong PKI/TLS setups and peform housekeeping tasks RUN mkdir /vault/config/assets && \\ chown -R vault:vault /vault && \\ chmod 644 /vault/config/local.json && \\ chmod 744 pkisetup* && \\ ./pkisetup --config pkisetup-vault.json && \\ echo \"\" && \\ ./pkisetup --config pkisetup-kong.json && \\ chown -R vault:vault /vault/config/pki && \\ rm -f /vault/pkisetup /vault/pkisetup-vault.json /vault/pkisetup-kong.json EdgeX Foundry Docker environment implements a basic Vault/Consul architecture that does not provide high availability guaranties. Only one Consul server and one Vault server will be running. In a more sophisticated production environment it would be possible to build a reliable high availability infrastructure regarding Consul and Vault. To facilitate the setup of a minimal failover architecture the security-secret-store repository provides a sample folder named Full-Architecture-Prototype that contains necessary materials (scripts, helpers, configurations, etc.) to achieve that goal. These samples describe an architecture design with two Vault servers in failover mode (active/standby), using each one a Consul client, which subsequently connects to a Consul cluster of 3 nodes (minimal Raft concensus quorum). The Consul clients and servers (nodes) have redundant paths. Using the Secret Store 1st alternative: executing a shell session in the active Vault container to run Vault CLI commands. See paragraph Configuring the Secret Store to have more details on the VAULT_CAPATH environment variable. See HashiCorp Vault API documentation for further details on syntax and usage ( https://www.vaultproject.io/api/ ). Execute a shell session in the running Vault container: sh> docker exec -it -e PS1 = '\\u@\\h:\\w \\$ ' -e VAULT_CAPATH = '/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem' edgex-vault sh Locate the assets folder, and the resp-init.json file: root@edgex-vault:/vault # ls -l config/assets/ total 12 -rw-r--r-- 1 root root 366 Jan 13 13 :35 admin-token.json -rw-r--r-- 1 root root 365 Jan 13 13 :35 kong-token.json -rw-r--r-- 1 root root 241 Jan 13 13 :35 resp-init.json Inspect the resp-init.json file to grab the Vault Root Token: root@edgex-vault:/vault # cat config/assets/resp-init.json { \"keys\" : [ \"564b9444eebe28b393c21a4dca1e32835b7dc27f5da03b73d22b666cb20224a9\" ] , \"keys_base64\" : [ \"VkuURO6+KLOTwhpNyh4yg1t9wn9doDtz0itmbLICJKk=\" ] , \"recovery_keys\" :null, \"recovery_keys_base64\" :null, \"root_token\" : \"01dbbae4-353a-8cdf-8189-4d50e5535a6f\" } Login to Vault using Vault CLI and the gathered Root Token: root@edgex-vault:/vault # vault login 01dbbae4-353a-8cdf-8189-4d50e5535a6f Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \"vault login\" again. Future Vault requests will automatically use this token. Key Value --- ----- token 01dbbae4-353a-8cdf-8189-4d50e5535a6f token_accessor 4d5eabf7-8710-81b1-b6a4-9ba17fdfdeb7 token_duration \u221e token_renewable false token_policies [ root ] Perform an introspection lookup on the current token login: root@edgex-vault:/vault # vault token lookup Key Value --- ----- accessor 4d5eabf7-8710-81b1-b6a4-9ba17fdfdeb7 creation_time 1547386542 creation_ttl 0 display_name root entity_id n/a expire_time <nil> explicit_max_ttl 0 id 01dbbae4-353a-8cdf-8189-4d50e5535a6f meta <nil> num_uses 0 orphan true path auth/token/root policies [ root ] ttl 0 Note Lines 9 & 10: the Root Token is the only token that has no expiration enforcement rules (Time to Live TTL counter). Perform a check on the current token login to display the corresponding capabilities (policies): root@edgex-vault:/vault # vault token capabilities 01dbbae4-353a-8cdf-8189-4d50e5535a6f root Perform a list request to display the currently mounted secret backends: root@edgex-vault:/vault # vault secrets list Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_ad070930 per-token private secret storage identity/ identity identity_5397dc2f identity store secret/ kv kv_2362c227 key/value secret storage sys/ system system_410e4276 system endpoints used for control, policy and debugging Note Line 5: EdgeX Foundry platform is using the Key/Value secret storage named secret Let's drill down into the secret k/v storage and walk through a predefined hierarchical tree structure (path). Note the pkisetup tool used during the Vault Docker image build process generates all the related X.509 TLS materials. The vault-worker service is storing each service materials into Vault using arbitrary paths, setting up access policies accordingly. For example, the API Gateway (Kong) service X.509 TLS materials: root@edgex-vault:/vault # vault list secret Keys ---- edgex/ root@edgex-vault:/vault # vault list secret/edgex Keys ---- pki/ root@edgex-vault:/vault # vault list secret/edgex/pki Keys ---- tls/ root@edgex-vault:/vault # vault list secret/edgex/pki/tls Keys ---- edgex-kong Displaying the API gateway (Kong) service X.509 TLS materials (TLS certificate cert & corresponding private key key ): root@edgex-vault:/vault # vault read secret/edgex/pki/tls/edgex-kong Key Value --- ----- refresh_interval 168h cert -----BEGIN CERTIFICATE----- MIICrjCCAjWgAwIBAgIQDvZxhmU3nyG4cwXlQesMFDAKBggqhkjOPQQDAzB7MQsw CQYDVQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28x FzAVBgNVBAoTDkVkZ2VYRm91bmRyeUNBMRUwEwYDVQQLEwxFZGdlWEZvdW5kcnkxt FzAVBgNVBAMTDkVkZ2VYRm91bmRyeUNBMB4XDTE4MTIwNTE0MDUyOFoXDTI4MTIw NTE0MDUyOFowazELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkNBMRYwFAYDVQQHEw1T YW4gRnJhbmNpc2NvMRMwEQYDVQQKEwplZGdleC1rb25nMQ0wCwYDVQQLEwRLb25n MRMwEQYDVQQDEwplZGdleC1rb25nMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAE2dnb EboXET1TjzmWKFv3A0wklwNbs9t9JLT0ecpQr64a277UnTAQhgCv2e2/x9EP4eta gSlz5PCqdAykWW0URIEPSwUKWmx4x1DBwyUD2oDOPsFrywIVEC3DlqQAL6huo4GN MIGKMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATAMBgNVHRMB Af8EAjAAMB8GA1UdIwQYMBaAFFX63XbmPNpLceOJYyt2Y+LfW/gxMDQGA1UdEQQt MCuCCmVkZ2V4LWtvbmeCEGVkZ2V4LWtvbmcubG9jYWyBC2FkbWluQGxvY2FsMAoG CCqGSM49BAMDA2cAMGQCMCaH3sSKq6nlr6hBJx82wYEiK4slMbySiQZg5mLcwrsQ tIPGcQ2lgBdQYzI3ymOS5gIwNhpQmo/p3hkoFzA4rxIAZx/GUgZan51JlXW0rpgz 4HerRLe55EmvF10mF7VCGOXe -----END CERTIFICATE----- key -----BEGIN PRIVATE KEY----- MIG2AgEAMBAGByqGSM49AgEGBSuBBAAiBIGeMIGbAgEBBDC6BRUXqkJbey765+8b Oib2qG/jbai2rzp0+NQyJv4ijAyYjJlxhVGggZqPPBy8baqhZANiAATZ2dsRuhcR PVOPOZYoW/cDTCSXA1uz230ktPR5ylCvrhrbvtSdMBCGAK/Z7b/H0Q/h61qBKXPk 8Kp0DKRZbRREgQ9LBQpabHjHUMHDJQPagM4+wWvLAhUQLcOWpAAvqG4 = -----END PRIVATE KEY----- Note These two key values are in PEM format. 2nd alternative: using the Vault Web UI. Open a browser session on https://<EdgeX Vault Server>:8200 , accept the self-signed TLS server certificate and sign-in with the Root Token (see above 1st alternative to learn how to fetch this token): {.align-center width=\"606px\" height=\"504px\"} Upper left corner of the current Vault UI session, the sign-out menu displaying the current token name: {.align-center width=\"275px\" height=\"156px\"} Select the Vault secret backend: {.align-center} Navigate the API Gateway (Kong) service X.509 TLS materials path (edgex/pki/tls/edgex-kong): {.align-center} The Vault UI also allows entering Vault CLI commands (see above 1st alternative ) using an embedded console: {.align-center} 3rd alternative: directly using the Vault HTTP API with cURL commands. See paragraph Configuring the Secret Store to have more details on the --cacert option (identical purpose as the VAULT_CAPATH environment variable for Vault CLI). See paragraph Using the Secret Store to have more details on gathering the Vault Root Token (ref: /vault/config/assets/resp-init.json ). See HashiCorp Vault API documentation for further details on syntax and usage ( https://www.vaultproject.io/api/ ). Displaying (GET) the API gateway (Kong) service X.509 TLS materials (TLS certificate cert & corresponding private key key): curl -s --cacert /vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem \\ --location \\ --header \"X-Vault-Token: 01dbbae4-353a-8cdf-8189-4d50e5535a6f\" \\ --request GET \\ https://edgex-vault:8200/v1/secret/edgex/pki/tls/edgex-kong | jq Note Line 2: the --location option allows following a redirection (necessary when using a Vault cluster) Line 5: the Vault API path prefix /v1/secret and the API Gateway X.509 TLS materials k/v /edgex/pki/tls/edgex-kong . Line 5: the jq tool is a lightweight and flexible command-line JSON processor ( https://stedolan.github.io/jq/ ) allowing JSON pretty printing in the terminal. Sample JSON returned: { \"request_id\" : \"eaa80a1b-0d31-8d11-6ce1-8d9aa3ac6a19\" , \"lease_id\" : \"\" , \"renewable\" : false , \"lease_duration\" : 604800 , \"data\" : { \"cert\" : \"-----BEGIN CERTIFICATE-----\\nMIICrjCCAjWgAwIBAgIQDvZxhmU3nyG4cwXlQesMFDAKBggqhkjOPQQDAzB7MQsw\\nCQYDVQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28x\\nFzAVBgNVBAoTDkVkZ2VYRm91bmRyeUNBMRUwEwYDVQQLEwxFZGdlWEZvdW5kcnkx\\nFzAVBgNVBAMTDkVkZ2VYRm91bmRyeUNBMB4XDTE4MTIwNTE0MDUyOFoXDTI4MTIw\\nNTE0MDUyOFowazELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkNBMRYwFAYDVQQHEw1T\\nYW4gRnJhbmNpc2NvMRMwEQYDVQQKEwplZGdleC1rb25nMQ0wCwYDVQQLEwRLb25n\\nMRMwEQYDVQQDEwplZGdleC1rb25nMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAE2dnb\\nEboXET1TjzmWKFv3A0wklwNbs9t9JLT0ecpQr64a277UnTAQhgCv2e2/x9EP4eta\\ngSlz5PCqdAykWW0URIEPSwUKWmx4x1DBwyUD2oDOPsFrywIVEC3DlqQAL6huo4GN\\nMIGKMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATAMBgNVHRMB\\nAf8EAjAAMB8GA1UdIwQYMBaAFFX63XbmPNpLceOJYyt2Y+LfW/gxMDQGA1UdEQQt\\nMCuCCmVkZ2V4LWtvbmeCEGVkZ2V4LWtvbmcubG9jYWyBC2FkbWluQGxvY2FsMAoG\\nCCqGSM49BAMDA2cAMGQCMCaH3sSKq6nlr6hBJx82wYEiK4slMbySiQZg5mLcwrsQ\\ntIPGcQ2lgBdQYzI3ymOS5gIwNhpQmo/p3hkoFzA4rxIAZx/GUgZan51JlXW0rpgz\\n4HerRLe55EmvF10mF7VCGOXe\\n-----END CERTIFICATE-----\\n\" , \"key\" : \"-----BEGIN PRIVATE KEY-----\\nMIG2AgEAMBAGByqGSM49AgEGBSuBBAAiBIGeMIGbAgEBBDC6BRUXqkJbey765+8b\\nOib2qG/jbai2rzp0+NQyJv4ijAyYjJlxhVGggZqPPBy8baqhZANiAATZ2dsRuhcR\\nPVOPOZYoW/cDTCSXA1uz230ktPR5ylCvrhrbvtSdMBCGAK/Z7b/H0Q/h61qBKXPk\\n8Kp0DKRZbRREgQ9LBQpabHjHUMHDJQPagM4+wWvLAhUQLcOWpAAvqG4=\\n-----END PRIVATE KEY-----\\n\" }, \"wrap_info\" : null , \"warnings\" : null , \"auth\" : null } Note Lines 7 & 8: the two key values (TLS certificate cert & corresponding private key key ) are in PEM format ( https://tools.ietf.org/html/rfc1421 ). Displaying (LIST) the root key path in the Vault secret backend for the EdgeX Foudry platform ( edgex ): curl -s --cacert /vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem \\ --location \\ --header \"X-Vault-Token: 01dbbae4-353a-8cdf-8189-4d50e5535a6f\" \\ --request LIST \\ https://edgex-vault:8200/v1/secret | jq Sample JSON returned: { \"request_id\" : \"0e0ea024-176d-21b3-73cb-99f17729b230\" , \"lease_id\" : \"\" , \"renewable\" : false , \"lease_duration\" : 0 , \"data\" : { \"keys\" : [ \"edgex/\" ] }, \"wrap_info\" : null , \"warnings\" : null , \"auth\" : null } Displaying (GET) the Vault seal status ( API path: /v1/sys/seal-status ): curl -s --cacert /vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem \\ --location \\ --header \"X-Vault-Token: 01dbbae4-353a-8cdf-8189-4d50e5535a6f\" \\ --request GET \\ https://edgex-vault:8200/v1/sys/seal-status | jq Sample JSON returned: { \"type\" : \"shamir\" , \"sealed\" : false , \"t\" : 1 , \"n\" : 1 , \"progress\" : 0 , \"nonce\" : \"\" , \"version\" : \"0.10.2\" , \"cluster_name\" : \"vault-cluster-57b3c4ed\" , \"cluster_id\" : \"fe6d18bf-fa9c-0d52-3278-bca0390af023\" } Note Line 3: Vault is unsealed therefore available and ready for requests. Line 4 & 5: Vault Shamir Secret Sharing default configuration for EdgeX Foundry: 1 share with threshold 1 (no sharding). See also Some of the command used in implementing security services have man-style documentation: security-file-token-provider - Generate Vault tokens for EdgeX services security-secrets-setup - Creates an on-device public-key infrastructure (PKI) to secure microservice secret management","title":"Secret Store"},{"location":"microservices/security/Ch-SecretStore/#secret-store","text":"There are all kinds of secrets used within EdgeX Foundry micro services, such as tokens, passwords, certificates etc. The secret store serves as the central repository to keep these secrets. The developers of other EdgeX Foundry micro services utilize the secret store to create, store and retrieve secrets relevant to their corresponding micro service. The communications the between secret store and other micro services are secured by TLS. Currently the EdgeX Foundry secret store is implemented with Vault , a HashiCorp open source software product. Vault is a tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, database credentials, service credentials, or certificates. Vault provides a unified interface to any secret, while providing tight access control and multiple authentication mechanisms (token, LDAP, etc.). Vault adds on key rolling, revocation rules, time-to-live access tokens, secure storage, Shamir Secret Sharing based unlocking mechanism, high availability and detailed auditing. Vault can use several backend systems (filesystem, databases, Consul, Etcd, S3, Azure, etc.) to securely store every sensitive asset. The current EdgeX Foundry implementation of Vault is using Consul , another HashiCorp open source software product. Consul is a distributed service mesh to connect, secure, and configure services across any runtime platform and public or private cloud. Consul uses a consensus protocol to provide Consistency as defined by CAP . The consensus protocol is based on \"Raft: In search of an Understandable Consensus Algorithm\" . For a visual explanation of Raft, see The Secret Lives of Data . The seamless integration of Vault and Consul provides a strong yet simple infrastructure to setup a reliable high availability architecture (Vault failover nodes, Consul Clustering) for the EdgeX Foundry Security services in production. The key features of Vault are: Secure Secret Storage: Arbitrary key/value secrets can be stored in Vault. Vault encrypts these secrets prior to writing them to persistent storage, so gaining access to the raw storage isn't enough to access your secrets. Authentication mechanisms (internal and/or external) and authorizations based upon policies provide access management. Dynamic Secrets: Vault can generate secrets on-demand for some systems, automatically revoking them after the lease is up. Data Encryption: Vault can encrypt and decrypt data without storing it. Leasing and Renewal: All secrets in Vault have a lease associated with them (automatic revocation). However, clients can renew leases via built-in renew APIs. Revocation: Vault has built-in support for secret revocation. Single secrets, but also a tree of secrets. Revocation assists in key rolling as well as locking down systems in the case of an intrusion.","title":"Secret Store"},{"location":"microservices/security/Ch-SecretStore/#start-the-secret-store","text":"Start the Secret Store with Docker Compose and a Docker Compose manifest file. The whole set of Docker Compose files for Geneva release can be found here: https://github.com/edgexfoundry/developer-scripts/blob/master/releases/geneva/compose-files The Compose file starts the entire EdgeX Foundry platform with Redis including the security services. The command to start EdgeX Foundry platform including the Secret Store and API gateway related services is: sh> docker-compose up -d For a pristine run, it is strongly recommended to thoroughly clean up any Docker artifacts remaining from the current run. sh> docker-compose down -v The \"down\" operation will remove containers, network and adding the -v option it will also remove persistent volumes: Stopping edgex-vault ... done Stopping edgex-core-consul ... done Stopping edgex-files ... done Removing edgex-vault-worker ... done Removing edgex-vault ... done Removing edgex-config-seed ... done Removing edgex-core-consul ... done Removing edgex-files ... done Removing network security-secret-store_edgex-network Removing volume security-secret-store_db-data Removing volume security-secret-store_log-data Removing volume security-secret-store_consul-config Removing volume security-secret-store_consul-data Removing volume security-secret-store_vault-config Removing volume security-secret-store_vault-file Removing volume security-secret-store_vault-logs","title":"Start the Secret Store"},{"location":"microservices/security/Ch-SecretStore/#troubleshooting-steps","text":"For debugging purpose, the Secret Store services can be started individually with these commands. A pre-requisite being to carefully follow the invocation sequence in order to avoid any dependency failure. Lines starting with # (hashtag) are contextual comments to explicit the purpose of the above corresponding command:","title":"Troubleshooting steps"},{"location":"microservices/security/Ch-SecretStore/#clean-up","text":"sh> cd <path-to-EdgeX-Foundry-Secret-Store> # <...>/security-secret-store/ sh> docker-compose down -v sh> docker-compose ps # Check no previous container is running sh> docker volume ls # Check and remove any previous persistent and/or unused volumes sh> docker volume prune sh> docker volume rm <volume-name> sh> docker network ls # Check and remove the previous EdgeX Foundry Docker network sh> docker network rm edgex-network","title":"Clean-up"},{"location":"microservices/security/Ch-SecretStore/#start-the-first-service-volume-platform-volume-initializations","text":"sh> docker-compose up -d volume Sample output: Creating network \"security-secret-store_edgex-network\" with driver \"bridge\" Creating volume \"security-secret-store_db-data\" with default driver Creating volume \"security-secret-store_log-data\" with default driver Creating volume \"security-secret-store_consul-config\" with default driver Creating volume \"security-secret-store_consul-data\" with default driver Creating volume \"security-secret-store_vault-config\" with default driver Creating volume \"security-secret-store_vault-file\" with default driver Creating volume \"security-secret-store_vault-logs\" with default driver Creating edgex-files ... done","title":"Start the first service: volume (platform volume initializations)"},{"location":"microservices/security/Ch-SecretStore/#start-the-second-service-consul-consul-is-vault-store-backend","text":"sh> docker-compose up -d consul Sample output: edgex-files is up-to-date Creating edgex-core-consul ... done Display and inspect consul service logs: important lines are highlighted sh> docker-compose logs consul Sample output: Attaching to edgex-core-consul edgex-core-consul | ==> Starting Consul agent... edgex-core-consul | ==> Consul agent running! edgex-core-consul | Version: 'v1.1.0' edgex-core-consul | Node ID: '371cbce6-02a8-65f6-ddea-6df5c40a4c50' edgex-core-consul | Node name: 'edgex-core-consul' edgex-core-consul | Datacenter: 'dc1' (Segment: '<all>') edgex-core-consul | Server: true (Bootstrap: false) edgex-core-consul | Client Addr: [0.0.0.0] (HTTP: 8500, HTTPS: -1, DNS: 8600) edgex-core-consul | Cluster Addr: 127.0.0.1 (LAN: 8301, WAN: 8302) edgex-core-consul | Encrypt: Gossip: false, TLS-Outgoing: false, TLS-Incoming: false edgex-core-consul | edgex-core-consul | ==> Log data will now stream in as it occurs: edgex-core-consul | edgex-core-consul | 2019/01/13 13:25:06 [DEBUG] agent: Using random ID \"371cbce6-02a8-65f6-ddea-6df5c40a4c50\" as node ID edgex-core-consul | 2019/01/13 13:25:06 [INFO] raft: Initial configuration (index=1): [{Suffrage:Voter ID:371cbce6-02a8-65f6-ddea-6df5c40a4c50 Address:127.0.0.1:8300}] edgex-core-consul | 2019/01/13 13:25:06 [INFO] raft: Node at 127.0.0.1:8300 [Follower] entering Follower state (Leader: \"\") edgex-core-consul | 2019/01/13 13:25:06 [INFO] serf: EventMemberJoin: edgex-core-consul.dc1 127.0.0.1 edgex-core-consul | 2019/01/13 13:25:06 [INFO] serf: EventMemberJoin: edgex-core-consul 127.0.0.1 edgex-core-consul | 2019/01/13 13:25:06 [INFO] consul: Adding LAN server edgex-core-consul (Addr: tcp/127.0.0.1:8300) (DC: dc1) edgex-core-consul | 2019/01/13 13:25:06 [INFO] consul: Handled member-join event for server \"edgex-core-consul.dc1\" in area \"wan\" edgex-core-consul | 2019/01/13 13:25:06 [INFO] agent: Started DNS server 0.0.0.0:8600 (tcp) edgex-core-consul | 2019/01/13 13:25:06 [INFO] agent: Started DNS server 0.0.0.0:8600 (udp) edgex-core-consul | 2019/01/13 13:25:06 [INFO] agent: Started HTTP server on [::]:8500 (tcp) edgex-core-consul | 2019/01/13 13:25:06 [INFO] agent: started state syncer edgex-core-consul | 2019/01/13 13:25:06 [WARN] raft: Heartbeat timeout from \"\" reached, starting election edgex-core-consul | 2019/01/13 13:25:06 [INFO] raft: Node at 127.0.0.1:8300 [Candidate] entering Candidate state in term 2 edgex-core-consul | 2019/01/13 13:25:06 [DEBUG] raft: Votes needed: 1 edgex-core-consul | 2019/01/13 13:25:06 [DEBUG] raft: Vote granted from 371cbce6-02a8-65f6-ddea-6df5c40a4c50 in term 2. Tally: 1 edgex-core-consul | 2019/01/13 13:25:06 [INFO] raft: Election won. Tally: 1 edgex-core-consul | 2019/01/13 13:25:06 [INFO] raft: Node at 127.0.0.1:8300 [Leader] entering Leader state edgex-core-consul | 2019/01/13 13:25:06 [INFO] consul: cluster leadership acquired","title":"Start the second service: consul (Consul is Vault store backend)"},{"location":"microservices/security/Ch-SecretStore/#start-the-third-service-config-seed-platform-configuration-initializations","text":"sh> docker-compose up -d config-seed Sample output: edgex-files is up-to-date edgex-core-consul is up-to-date Creating edgex-config-seed ... done Display and inspect the created container states: important lines are highlighted sh> docker-compose ps Sample output: Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------------------edgex-config-seed /bin/sh -c /edgex/cmd/conf ... Exit 0 edgex-core-consul docker-entrypoint.sh agent ... Up 8300/tcp, 8301/tcp, 8301/udp, 8302/tcp, 8302/udp, 0.0.0.0:8400->8400/tcp, 0.0.0.0:8500->8500/tcp, 0.0.0.0:8600->8600/tcp, 8600/udp edgex-files /bin/sh -c /usr/bin/tail - ... Up Note Line 3: edgex-config-seed service has exited after successful processing (exit code 0)","title":"Start the third service: config-seed (platform configuration initializations)"},{"location":"microservices/security/Ch-SecretStore/#start-the-fourth-service-vault-vault-tool","text":"Note Vault will be uninitialized and unsealed upon success. The vault-worker service will process the initialization and unsealing tasks. sh> docker-compose up -d vault Sample output: edgex-files is up-to-date edgex-core-consul is up-to-date Creating edgex-vault ... done Display and inspect \"vault\" service logs: important lines are highlighted sh> docker-compose logs vault Sample output: Attaching to edgex-vault edgex-vault | ==> Vault server configuration: edgex-vault | edgex-vault | Api Address: https://edgex-vault:8200 edgex-vault | Cgo: disabled edgex-vault | Cluster Address: https://edgex-vault:8201 edgex-vault | Listener 1: tcp (addr: \"edgex-vault:8200\", cluster address: \"edgex-vault:8201\", tls: \"enabled\") edgex-vault | Log Level: info edgex-vault | Mlock: supported: true, enabled: true edgex-vault | Storage: consul (HA available) edgex-vault | Version: Vault v0.10.2 edgex-vault | Version Sha: 3ee0802ed08cb7f4046c2151ec4671a076b76166 edgex-vault | edgex-vault | ==> Vault server started! Log data will stream in below: edgex-vault | Note Line 4 & 7: Vault API endpoint on port 8200 (lines 4 and 7). Line 7: Vault has TLS enabled. Line 10: Vault backend storage is Consul .","title":"Start the fourth service: vault (Vault tool)"},{"location":"microservices/security/Ch-SecretStore/#start-the-fifth-service-vault-worker-vault-initunseal-process-and-setups","text":"sh> docker-compose up -d vault-worker Sample output: edgex-files is up-to-date edgex-core-consul is up-to-date edgex-vault is up-to-date Creating edgex-vault-worker ... done Display and inspect \"vault-worker\" service logs: important lines are highlighted sh> docker-compose logs vault-worker Sample output: Attaching to edgex-vault-worker edgex-vault-worker | INFO: 2019/01/13 13:35:42 successful loading the rootCA cert. edgex-vault-worker | INFO: 2019/01/13 13:35:43 {\"keys\":[\"564b9444eebe28b393c21a4dca1e32835b7dc27f5da03b73d22b666cb20224a9\"],\"keys_base64\":[\"VkuURO6+KLOTwhpNyh4yg1t9wn9doDtz0itmbLICJKk=\"],\"recovery_keys\":null,\"recovery_keys_base64\":null,\"root_token\":\"01dbbae4-353a-8cdf-8189-4d50e5535a6f\"} edgex-vault-worker | INFO: 2019/01/13 13:35:43 Vault has been initialized successfully. edgex-vault-worker | INFO: 2019/01/13 13:35:43 Vault has been unsealed successfully. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Vault Health Check HTTP Status: 200 OK (StatusCode: 200) edgex-vault-worker | INFO: 2019/01/13 13:35:48 Verifying Admin policy file hash (SHA256). edgex-vault-worker | INFO: 2019/01/13 13:35:48 Vault policy file checksum (SHA256): 5ce8d58cf7d931735f6532742f677c109a91a263bcefe9aef73ab2a69f4b43d3 edgex-vault-worker | INFO: 2019/01/13 13:35:48 Reading Admin policy file. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Importing Vault Admin policy. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Import Policy Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Reading Kong policy file. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Importing Vault Kong policy. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Import Policy Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Creating Vault Admin token. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Create Token Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Creating Vault Kong token. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Create Token Successfull. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Successful on reading certificate from v1/secret/edgex/pki/tls/edgex-kong. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Cert&key are not in the secret store yet, will need to upload them. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Load cert&key pair from volume successfully, now will upload to secret store. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Trying to upload cert&key to secret store. edgex-vault-worker | INFO: 2019/01/13 13:35:48 Successful to add certificate to the secret store. Display and inspect \"vault\" service logs: important lines are highlighted sh> docker-compose logs vault Sample output: Attaching to edgex-vault edgex-vault | ==> Vault server configuration: edgex-vault | edgex-vault | Api Address: https://edgex-vault:8200 edgex-vault | Cgo: disabled edgex-vault | Cluster Address: https://edgex-vault:8201 edgex-vault | Listener 1: tcp (addr: \"edgex-vault:8200\", cluster address: \"edgex-vault:8201\", tls: enabled\") edgex-vault | Log Level: info edgex-vault | Mlock: supported: true, enabled: true edgex-vault | Storage: consul (HA available) edgex-vault | Version: Vault v0.10.2 edgex-vault | Version Sha: 3ee0802ed08cb7f4046c2151ec4671a076b76166 edgex-vault | edgex-vault | ==> Vault server started! Log data will stream in below: edgex-vault | edgex-vault | 2019-01-13T13:35:42.549Z [INFO ] core: security barrier not initialized edgex-vault | 2019-01-13T13:35:42.551Z [INFO ] core: security barrier not initialized edgex-vault | 2019-01-13T13:35:42.554Z [INFO ] core: security barrier initialized: shares=1 threshold=1 edgex-vault | 2019-01-13T13:35:42.575Z [INFO ] core: post-unseal setup starting edgex-vault | 2019-01-13T13:35:42.583Z [INFO ] core: loaded wrapping token key edgex-vault | 2019-01-13T13:35:42.583Z [INFO ] core: successfully setup plugin catalog: plugin-directory= edgex-vault | 2019-01-13T13:35:42.584Z [INFO ] core: no mounts; adding default mount table edgex-vault | 2019-01-13T13:35:42.585Z [INFO ] core: successfully mounted backend: type=kv path=secret/ edgex-vault | 2019-01-13T13:35:42.586Z [INFO ] core: successfully mounted backend: type=cubbyhole path=cubbyhole/ edgex-vault | 2019-01-13T13:35:42.586Z [INFO ] core: successfully mounted backend: type=system path=sys/ edgex-vault | 2019-01-13T13:35:42.586Z [INFO ] core: successfully mounted backend: type=identity path=identity/ edgex-vault | 2019-01-13T13:35:42.593Z [INFO ] core: restoring leases edgex-vault | 2019-01-13T13:35:42.593Z [INFO ] rollback: starting rollback manager edgex-vault | 2019-01-13T13:35:42.594Z [INFO ] expiration: lease restore complete edgex-vault | 2019-01-13T13:35:42.596Z [INFO ] identity: entities restored edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] identity: groups restored edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] core: post-unseal setup complete edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] core: core/startClusterListener: starting listener: listener_address=172.19.0.4:8201 edgex-vault | 2019-01-13T13:35:42.597Z [INFO ] core: core/startClusterListener: serving cluster requests: cluster_listen_address=172.19.0.4:8201 edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: root token generated edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: pre-seal teardown starting edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: stopping cluster listeners edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: shutting down forwarding rpc listeners edgex-vault | 2019-01-13T13:35:42.600Z [INFO ] core: forwarding rpc listeners stopped edgex-vault | 2019-01-13T13:35:43.099Z [INFO ] core: rpc listeners successfully shut down edgex-vault | 2019-01-13T13:35:43.099Z [INFO ] core: cluster listeners successfully shut down edgex-vault | 2019-01-13T13:35:43.100Z [INFO ] rollback: stopping rollback manager edgex-vault | 2019-01-13T13:35:43.100Z [INFO ] core: pre-seal teardown complete edgex-vault | 2019-01-13T13:35:43.105Z [INFO ] core: vault is unsealed edgex-vault | 2019-01-13T13:35:43.105Z [INFO ] core: entering standby mode edgex-vault | 2019-01-13T13:35:43.109Z [INFO ] core: acquired lock, enabling active operation edgex-vault | 2019-01-13T13:35:43.134Z [INFO ] core: post-unseal setup starting edgex-vault | 2019-01-13T13:35:43.135Z [INFO ] core: loaded wrapping token key edgex-vault | 2019-01-13T13:35:43.135Z [INFO ] core: successfully setup plugin catalog: plugin-directory= edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=kv path=secret/ edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=system path=sys/ edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=identity path=identity/ edgex-vault | 2019-01-13T13:35:43.137Z [INFO ] core: successfully mounted backend: type=cubbyhole path=cubbyhole/ edgex-vault | 2019-01-13T13:35:43.141Z [INFO ] core: restoring leases edgex-vault | 2019-01-13T13:35:43.142Z [INFO ] rollback: starting rollback manager edgex-vault | 2019-01-13T13:35:43.142Z [INFO ] expiration: lease restore complete edgex-vault | 2019-01-13T13:35:43.143Z [INFO ] identity: entities restored edgex-vault | 2019-01-13T13:35:43.143Z [INFO ] identity: groups restored edgex-vault | 2019-01-13T13:35:43.144Z [INFO ] core: post-unseal setup complete edgex-vault | 2019-01-13T13:35:43.144Z [INFO ] core: core/startClusterListener: starting listener: listener_address=172.19.0.4:8201 edgex-vault | 2019-01-13T13:35:43.144Z [INFO ] core: core/startClusterListener: serving cluster requests: cluster_listen_address=172.19.0.4:8201 Note Line 18: Vault initialization successful. Line 35: Vault root token generated. Line 44: Vault unsealing successful. Line 50: Vault key/value store secret successfully mounted . Line 60 & 61: Vault successfully started . Display and inspect the created container states: important lines are highlighted sh> docker-compose ps Sample output: Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------------------edgex-config-seed /bin/sh -c /edgex/cmd/conf ... Exit 0 edgex-core-consul docker-entrypoint.sh agent ... Up 8300/tcp, 8301/tcp, 8301/udp, 8302/tcp, 8302/udp, 0.0.0.0:8400->8400/tcp, 0.0.0.0:8500->8500/tcp, 0.0.0.0:8600->8600/tcp, 8600/udp edgex-files /bin/sh -c /usr/bin/tail - ... Up edgex-vault docker-entrypoint.sh serve ... Up 0.0.0.0:8200->8200/tcp edgex-vault-worker ./edgex-vault-worker --ini ... Exit 0 Note Line 9: edgex-vault-worker service has exited after successful processing (exit code 0) Display and inspect the created container volumes: important lines are highlighted sh> docker volume ls DRIVER VOLUME NAME local security-secret-store_consul-config local security-secret-store_consul-data local security-secret-store_db-data local security-secret-store_log-data local security-secret-store_vault-config local security-secret-store_vault-file local security-secret-store_vault-logs Display and inspect the container network ( security-secret-store_edgex-network ): important lines are highlighted sh> docker network ls NETWORK ID NAME DRIVER SCOPE 63227826fbc7 bridge bridge local 60763abffde3 host host local 1d236ab1dbbd none null local 0a7f7266d102 security-secret-store_edgex-network bridge local","title":"Start the fifth service: vault-worker (Vault init/unseal process and setups)"},{"location":"microservices/security/Ch-SecretStore/#using-consul-web-ui","text":"For learning and verification purposes one might use the Consul Web UI interface to gather and double check specific Vault informations. Consul Web UI endpoint port is exposed by the Docker compose file. EdgeX Foundry platform uses the Consul default port number 8500. It is normally not recommended to expose Consul UI port number in production, at least the UI should not be accessible from outside the platform environment. However, because all the Vault secrets are encrypted before being transmitted and stored in the Consul backend, having access to Consul is not sufficient to access any secrets, the vault data encryption/decryption key would be absolutely necessary. Open a Web browser on http://<EdgeX Consul Server>:8500/ui . On the screenshot below, after selecting SERVICES and Vault , the UI will show the various Vault status (heartbeat and init/unseal states), coloring the boxes in green, orange or red depending on the level of importance (info, warning, error). By clicking each of the right side status indicators, more information will be accessible in order to better inspect any situation. As a practical example, we are going to navigate the Consul structure for Vault in order to check if the API Gateway (Kong) TLS certificate and private key were fetched and stored accordingly during the vault-worker process. First select KEY/VALUE menu, and then select vault root structure: We are now going to navigate deeper in the vault tree structure to reach and display the EdgeX Kong TLS assets. Continue by selecting logical/ : {.align-center width=\"348px\" height=\"287px\"} Then select d7809b... an arbitrary UID generated and created by Consul during Vault registration: {.align-center width=\"377px\" height=\"216px\"} Select edgex/ : {.align-center width=\"419px\" height=\"228px\"} Select pki/ : {.align-center width=\"417px\" height=\"222px\"} Select tls/ : {.align-center width=\"418px\" height=\"237px\"} Select edgex-kong/ : {.align-center width=\"423px\" height=\"233px\"} And we are now finally able to display the encrypted Vault secret containing the API Gateway (Kong) TLS server certificate and its corresponding private key. As you can see on the screenshot below the Vault key/value is encrypted and totally opaque to Consul, the Vault data encryption key (DEK) would be necessary to decrypt these secrets. Each Vault secret is encrypted before being transmitted to Consul node(s).","title":"Using Consul Web UI"},{"location":"microservices/security/Ch-SecretStore/#shell-access-to-consul-container-and-using-consul-cli","text":"sh> docker exec -it -e PS1 = '\\u@\\h:\\w \\$ ' edgex-core-consul sh root@edgex-core-consul:/ # consul members Node Address Status Type Build Protocol DC Segment edgex-core-consul 127 .0.0.1:8301 alive server 1 .1.0 2 dc1 <all> root@edgex-core-consul:/ # consul catalog nodes Node ID Address DC edgex-core-consul e49af36a 127 .0.0.1 dc1 root@edgex-core-consul:/ # consul catalog services consul edgex-mongo vault Note Line 5: Shows the Consul node status alive (1 node in EdgeX default configuration). Line 9: Shows the Consul nodes (1 node in EdgeX default configuration). Lines 12-14: Show the Consul registered services.","title":"Shell Access to Consul Container and Using Consul CLI"},{"location":"microservices/security/Ch-SecretStore/#configuring-the-secret-store","text":"Vault server configuration is essentially concentrated in one JSON file named local.json . This file was prepared during the Vault Docker image build process. In the eventuality of a change, the Vault server container should be accessed to then modify the JSON file. The absolute path being /vault/config/local.json . To reload the new configuration simply send Vault PID a HUP signal to trigger a configuration reload. Sample Vault server configuration file: listener \"tcp\" { address = \"edgex-vault:8200\" tls_disable = \"0\" cluster_address = \"edgex-vault:8201\" tls_min_version = \"tls12\" tls_client_ca_file =\"/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem\" tls_cert_file =\"/vault/config/pki/EdgeXFoundryCA/edgex-vault.pem\" tls_key_file = \"/vault/config/pki/EdgeXFoundryCA/edgex-vault.priv.key\" } backend \"consul\" { path = \"vault/\" address = \"edgex-core-consul:8500\" scheme = \"http\" redirect_addr = \"https://edgex-vault:8200\" cluster_addr = \"https://edgex-vault:8201\" } default_lease_ttl = \"168h\" max_lease_ttl = \"720h\" The listener clause refers to Vault server process (port, TLS and server name), the backend clause refers to the storage backend (i.e. Consul). To modify this configuration file, execute a shell session in the running Vault container: sh> docker exec -it -e PS1 = '\\u@\\h:\\w \\$ ' -e VAULT_CAPATH = '/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem' edgex-vault sh root@edgex-vault:/vault # ls -l total 12 drwxr-xr-x 4 vault vault 4096 Jan 13 13 :34 config drwxr-xr-x 2 vault vault 4096 Jun 7 2018 file drwxr-xr-x 2 vault vault 4096 Jun 7 2018 logs Pay attention to the VAULT_CAPATH environment variable passed to the session. This is necessary in order to run succesful Vault CLI command. Every Vault CLI command is a wrapper of the Vault HTTP API. The Vault server is configured with TLS using X.509 PKI materials generated and signed by a local self-signed CA (EdgeXFoundryCA). Therefore, in order for each Vault CLI command (or to that extent cURL commands) to verify the Vault server TLS certificate, the self-signing CA root certificate would have to be known by the CLI command interpreter. This VAULT_CAPATH variable is checked by every Vault CLI commands, alternatively each Vault CLI command can specify an option with the same certificate path if the variable is not set. The self-signed Root CA certificate path can be found in the Vault configuration file (see above local.json), with parameter tls_client_ca_file =\"/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem\" . The local.json configuration file can be read and modified within the running container: <root@edgex-vault>:/vault \\# cat config/local.json listener \"tcp\" { address = \"edgex-vault:8200\" tls\\_disable = \"0\" cluster\\_address =\"edgex-vault:8201\" tls\\_min\\_version = \"tls12\" tls\\_client\\_ca\\_file=\"/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem\" tls\\_cert\\_file=\"/vault/config/pki/EdgeXFoundryCA/edgex-vault.pem\" tls\\_key\\_file = \"/vault/config/pki/EdgeXFoundryCA/edgex-vault.priv.key\" } backend \"consul\" { path = \"vault/\" address = \"edgex-core-consul:8500\" scheme = \"http\" redirect\\_addr =\"<https://edgex-vault:8200>\" cluster\\_addr =\"<https://edgex-vault:8201>\" } default\\_lease\\_ttl = \"168h\" max\\_lease\\_ttl = \"720h\" A sample Vault CLI command to check Vault status: root@edgex-vault:/vault # vault status Key Value --- ----- Seal Type shamir Sealed false Total Shares 1 Threshold 1 Version 0 .10.2 Cluster Name vault-cluster-57b3c4ed Cluster ID fe6d18bf-fa9c-0d52-3278-bca0390af023 HA Enabled true HA Cluster https://edgex-vault:8201 HA Mode active All the X.509 PKI materials including the self-signing CA are located under /vault/config/pki/EdgeXFoundryCA . root@edgex-vault:/vault # ls -l config/pki/EdgeXFoundryCA/ total 24 -rw-r--r-- 1 vault vault 956 Dec 5 14 :05 EdgeXFoundryCA.pem -r-------- 1 vault vault 306 Dec 5 14 :05 EdgeXFoundryCA.priv.key -rw-r--r-- 1 vault vault 989 Dec 5 14 :05 edgex-kong.pem -rw------- 1 vault vault 306 Dec 5 14 :05 edgex-kong.priv.key -rw-r--r-- 1 vault vault 1001 Dec 5 14 :05 edgex-vault.pem -rw------- 1 vault vault 306 Dec 5 14 :05 edgex-vault.priv.key Note Line 3: self-signing root CA certificate. Line 4: self-signing root CA private key. Line 5: API Gateway (Kong) TLS server certificate. Line 6: API Gateway (Kong) TLS server certificate private key. Line 7: Vault TLS server certificate. Line 8: Vault TLS server certificate private key. The CA name (EdgeXFoundryCA) was defined by the pkisetup tool during the Vault image build process. This tool is also responsible for all the TLS server configuration and creation tasks. If you are willing to change any of the Vault X.509 PKI assets or configuration parameters you will have to modify the pkisetup-vault.json file and rebuild a new Vault Docker image. Similarly to Vault, each EdgeX Foundry service having a TLS server certificate and private key had its X.509 PKI assets generated and signed during the Vault Docker image build process. Therefore, the API Gateway (Kong) configuration file named pkisetup-kong.json would have to be modified accordingly. A new Vault Docker image would have to be built. The Vault Dockerfile contains the pkisetup executions, see below for a corresponding excerpt (highlighted lines): # Create assets folder (needed for unseal key/s, root token and tmp) # Run CA/Vault and Kong PKI/TLS setups and peform housekeeping tasks RUN mkdir /vault/config/assets && \\ chown -R vault:vault /vault && \\ chmod 644 /vault/config/local.json && \\ chmod 744 pkisetup* && \\ ./pkisetup --config pkisetup-vault.json && \\ echo \"\" && \\ ./pkisetup --config pkisetup-kong.json && \\ chown -R vault:vault /vault/config/pki && \\ rm -f /vault/pkisetup /vault/pkisetup-vault.json /vault/pkisetup-kong.json EdgeX Foundry Docker environment implements a basic Vault/Consul architecture that does not provide high availability guaranties. Only one Consul server and one Vault server will be running. In a more sophisticated production environment it would be possible to build a reliable high availability infrastructure regarding Consul and Vault. To facilitate the setup of a minimal failover architecture the security-secret-store repository provides a sample folder named Full-Architecture-Prototype that contains necessary materials (scripts, helpers, configurations, etc.) to achieve that goal. These samples describe an architecture design with two Vault servers in failover mode (active/standby), using each one a Consul client, which subsequently connects to a Consul cluster of 3 nodes (minimal Raft concensus quorum). The Consul clients and servers (nodes) have redundant paths.","title":"Configuring the Secret Store"},{"location":"microservices/security/Ch-SecretStore/#using-the-secret-store","text":"","title":"Using the Secret Store"},{"location":"microservices/security/Ch-SecretStore/#1st-alternative-executing-a-shell-session-in-the-active-vault-container-to-run-vault-cli-commands","text":"See paragraph Configuring the Secret Store to have more details on the VAULT_CAPATH environment variable. See HashiCorp Vault API documentation for further details on syntax and usage ( https://www.vaultproject.io/api/ ). Execute a shell session in the running Vault container: sh> docker exec -it -e PS1 = '\\u@\\h:\\w \\$ ' -e VAULT_CAPATH = '/vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem' edgex-vault sh Locate the assets folder, and the resp-init.json file: root@edgex-vault:/vault # ls -l config/assets/ total 12 -rw-r--r-- 1 root root 366 Jan 13 13 :35 admin-token.json -rw-r--r-- 1 root root 365 Jan 13 13 :35 kong-token.json -rw-r--r-- 1 root root 241 Jan 13 13 :35 resp-init.json Inspect the resp-init.json file to grab the Vault Root Token: root@edgex-vault:/vault # cat config/assets/resp-init.json { \"keys\" : [ \"564b9444eebe28b393c21a4dca1e32835b7dc27f5da03b73d22b666cb20224a9\" ] , \"keys_base64\" : [ \"VkuURO6+KLOTwhpNyh4yg1t9wn9doDtz0itmbLICJKk=\" ] , \"recovery_keys\" :null, \"recovery_keys_base64\" :null, \"root_token\" : \"01dbbae4-353a-8cdf-8189-4d50e5535a6f\" } Login to Vault using Vault CLI and the gathered Root Token: root@edgex-vault:/vault # vault login 01dbbae4-353a-8cdf-8189-4d50e5535a6f Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \"vault login\" again. Future Vault requests will automatically use this token. Key Value --- ----- token 01dbbae4-353a-8cdf-8189-4d50e5535a6f token_accessor 4d5eabf7-8710-81b1-b6a4-9ba17fdfdeb7 token_duration \u221e token_renewable false token_policies [ root ] Perform an introspection lookup on the current token login: root@edgex-vault:/vault # vault token lookup Key Value --- ----- accessor 4d5eabf7-8710-81b1-b6a4-9ba17fdfdeb7 creation_time 1547386542 creation_ttl 0 display_name root entity_id n/a expire_time <nil> explicit_max_ttl 0 id 01dbbae4-353a-8cdf-8189-4d50e5535a6f meta <nil> num_uses 0 orphan true path auth/token/root policies [ root ] ttl 0 Note Lines 9 & 10: the Root Token is the only token that has no expiration enforcement rules (Time to Live TTL counter). Perform a check on the current token login to display the corresponding capabilities (policies): root@edgex-vault:/vault # vault token capabilities 01dbbae4-353a-8cdf-8189-4d50e5535a6f root Perform a list request to display the currently mounted secret backends: root@edgex-vault:/vault # vault secrets list Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_ad070930 per-token private secret storage identity/ identity identity_5397dc2f identity store secret/ kv kv_2362c227 key/value secret storage sys/ system system_410e4276 system endpoints used for control, policy and debugging Note Line 5: EdgeX Foundry platform is using the Key/Value secret storage named secret Let's drill down into the secret k/v storage and walk through a predefined hierarchical tree structure (path). Note the pkisetup tool used during the Vault Docker image build process generates all the related X.509 TLS materials. The vault-worker service is storing each service materials into Vault using arbitrary paths, setting up access policies accordingly. For example, the API Gateway (Kong) service X.509 TLS materials: root@edgex-vault:/vault # vault list secret Keys ---- edgex/ root@edgex-vault:/vault # vault list secret/edgex Keys ---- pki/ root@edgex-vault:/vault # vault list secret/edgex/pki Keys ---- tls/ root@edgex-vault:/vault # vault list secret/edgex/pki/tls Keys ---- edgex-kong Displaying the API gateway (Kong) service X.509 TLS materials (TLS certificate cert & corresponding private key key ): root@edgex-vault:/vault # vault read secret/edgex/pki/tls/edgex-kong Key Value --- ----- refresh_interval 168h cert -----BEGIN CERTIFICATE----- MIICrjCCAjWgAwIBAgIQDvZxhmU3nyG4cwXlQesMFDAKBggqhkjOPQQDAzB7MQsw CQYDVQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28x FzAVBgNVBAoTDkVkZ2VYRm91bmRyeUNBMRUwEwYDVQQLEwxFZGdlWEZvdW5kcnkxt FzAVBgNVBAMTDkVkZ2VYRm91bmRyeUNBMB4XDTE4MTIwNTE0MDUyOFoXDTI4MTIw NTE0MDUyOFowazELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkNBMRYwFAYDVQQHEw1T YW4gRnJhbmNpc2NvMRMwEQYDVQQKEwplZGdleC1rb25nMQ0wCwYDVQQLEwRLb25n MRMwEQYDVQQDEwplZGdleC1rb25nMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAE2dnb EboXET1TjzmWKFv3A0wklwNbs9t9JLT0ecpQr64a277UnTAQhgCv2e2/x9EP4eta gSlz5PCqdAykWW0URIEPSwUKWmx4x1DBwyUD2oDOPsFrywIVEC3DlqQAL6huo4GN MIGKMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATAMBgNVHRMB Af8EAjAAMB8GA1UdIwQYMBaAFFX63XbmPNpLceOJYyt2Y+LfW/gxMDQGA1UdEQQt MCuCCmVkZ2V4LWtvbmeCEGVkZ2V4LWtvbmcubG9jYWyBC2FkbWluQGxvY2FsMAoG CCqGSM49BAMDA2cAMGQCMCaH3sSKq6nlr6hBJx82wYEiK4slMbySiQZg5mLcwrsQ tIPGcQ2lgBdQYzI3ymOS5gIwNhpQmo/p3hkoFzA4rxIAZx/GUgZan51JlXW0rpgz 4HerRLe55EmvF10mF7VCGOXe -----END CERTIFICATE----- key -----BEGIN PRIVATE KEY----- MIG2AgEAMBAGByqGSM49AgEGBSuBBAAiBIGeMIGbAgEBBDC6BRUXqkJbey765+8b Oib2qG/jbai2rzp0+NQyJv4ijAyYjJlxhVGggZqPPBy8baqhZANiAATZ2dsRuhcR PVOPOZYoW/cDTCSXA1uz230ktPR5ylCvrhrbvtSdMBCGAK/Z7b/H0Q/h61qBKXPk 8Kp0DKRZbRREgQ9LBQpabHjHUMHDJQPagM4+wWvLAhUQLcOWpAAvqG4 = -----END PRIVATE KEY----- Note These two key values are in PEM format.","title":"1st alternative: executing a shell session in the active Vault container to run Vault CLI commands."},{"location":"microservices/security/Ch-SecretStore/#2nd-alternative-using-the-vault-web-ui","text":"Open a browser session on https://<EdgeX Vault Server>:8200 , accept the self-signed TLS server certificate and sign-in with the Root Token (see above 1st alternative to learn how to fetch this token): {.align-center width=\"606px\" height=\"504px\"} Upper left corner of the current Vault UI session, the sign-out menu displaying the current token name: {.align-center width=\"275px\" height=\"156px\"} Select the Vault secret backend: {.align-center} Navigate the API Gateway (Kong) service X.509 TLS materials path (edgex/pki/tls/edgex-kong): {.align-center} The Vault UI also allows entering Vault CLI commands (see above 1st alternative ) using an embedded console: {.align-center}","title":"2nd alternative: using the Vault Web UI."},{"location":"microservices/security/Ch-SecretStore/#3rd-alternative-directly-using-the-vault-http-api-with-curl-commands","text":"See paragraph Configuring the Secret Store to have more details on the --cacert option (identical purpose as the VAULT_CAPATH environment variable for Vault CLI). See paragraph Using the Secret Store to have more details on gathering the Vault Root Token (ref: /vault/config/assets/resp-init.json ). See HashiCorp Vault API documentation for further details on syntax and usage ( https://www.vaultproject.io/api/ ). Displaying (GET) the API gateway (Kong) service X.509 TLS materials (TLS certificate cert & corresponding private key key): curl -s --cacert /vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem \\ --location \\ --header \"X-Vault-Token: 01dbbae4-353a-8cdf-8189-4d50e5535a6f\" \\ --request GET \\ https://edgex-vault:8200/v1/secret/edgex/pki/tls/edgex-kong | jq Note Line 2: the --location option allows following a redirection (necessary when using a Vault cluster) Line 5: the Vault API path prefix /v1/secret and the API Gateway X.509 TLS materials k/v /edgex/pki/tls/edgex-kong . Line 5: the jq tool is a lightweight and flexible command-line JSON processor ( https://stedolan.github.io/jq/ ) allowing JSON pretty printing in the terminal. Sample JSON returned: { \"request_id\" : \"eaa80a1b-0d31-8d11-6ce1-8d9aa3ac6a19\" , \"lease_id\" : \"\" , \"renewable\" : false , \"lease_duration\" : 604800 , \"data\" : { \"cert\" : \"-----BEGIN CERTIFICATE-----\\nMIICrjCCAjWgAwIBAgIQDvZxhmU3nyG4cwXlQesMFDAKBggqhkjOPQQDAzB7MQsw\\nCQYDVQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28x\\nFzAVBgNVBAoTDkVkZ2VYRm91bmRyeUNBMRUwEwYDVQQLEwxFZGdlWEZvdW5kcnkx\\nFzAVBgNVBAMTDkVkZ2VYRm91bmRyeUNBMB4XDTE4MTIwNTE0MDUyOFoXDTI4MTIw\\nNTE0MDUyOFowazELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkNBMRYwFAYDVQQHEw1T\\nYW4gRnJhbmNpc2NvMRMwEQYDVQQKEwplZGdleC1rb25nMQ0wCwYDVQQLEwRLb25n\\nMRMwEQYDVQQDEwplZGdleC1rb25nMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAE2dnb\\nEboXET1TjzmWKFv3A0wklwNbs9t9JLT0ecpQr64a277UnTAQhgCv2e2/x9EP4eta\\ngSlz5PCqdAykWW0URIEPSwUKWmx4x1DBwyUD2oDOPsFrywIVEC3DlqQAL6huo4GN\\nMIGKMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATAMBgNVHRMB\\nAf8EAjAAMB8GA1UdIwQYMBaAFFX63XbmPNpLceOJYyt2Y+LfW/gxMDQGA1UdEQQt\\nMCuCCmVkZ2V4LWtvbmeCEGVkZ2V4LWtvbmcubG9jYWyBC2FkbWluQGxvY2FsMAoG\\nCCqGSM49BAMDA2cAMGQCMCaH3sSKq6nlr6hBJx82wYEiK4slMbySiQZg5mLcwrsQ\\ntIPGcQ2lgBdQYzI3ymOS5gIwNhpQmo/p3hkoFzA4rxIAZx/GUgZan51JlXW0rpgz\\n4HerRLe55EmvF10mF7VCGOXe\\n-----END CERTIFICATE-----\\n\" , \"key\" : \"-----BEGIN PRIVATE KEY-----\\nMIG2AgEAMBAGByqGSM49AgEGBSuBBAAiBIGeMIGbAgEBBDC6BRUXqkJbey765+8b\\nOib2qG/jbai2rzp0+NQyJv4ijAyYjJlxhVGggZqPPBy8baqhZANiAATZ2dsRuhcR\\nPVOPOZYoW/cDTCSXA1uz230ktPR5ylCvrhrbvtSdMBCGAK/Z7b/H0Q/h61qBKXPk\\n8Kp0DKRZbRREgQ9LBQpabHjHUMHDJQPagM4+wWvLAhUQLcOWpAAvqG4=\\n-----END PRIVATE KEY-----\\n\" }, \"wrap_info\" : null , \"warnings\" : null , \"auth\" : null } Note Lines 7 & 8: the two key values (TLS certificate cert & corresponding private key key ) are in PEM format ( https://tools.ietf.org/html/rfc1421 ). Displaying (LIST) the root key path in the Vault secret backend for the EdgeX Foudry platform ( edgex ): curl -s --cacert /vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem \\ --location \\ --header \"X-Vault-Token: 01dbbae4-353a-8cdf-8189-4d50e5535a6f\" \\ --request LIST \\ https://edgex-vault:8200/v1/secret | jq Sample JSON returned: { \"request_id\" : \"0e0ea024-176d-21b3-73cb-99f17729b230\" , \"lease_id\" : \"\" , \"renewable\" : false , \"lease_duration\" : 0 , \"data\" : { \"keys\" : [ \"edgex/\" ] }, \"wrap_info\" : null , \"warnings\" : null , \"auth\" : null } Displaying (GET) the Vault seal status ( API path: /v1/sys/seal-status ): curl -s --cacert /vault/config/pki/EdgeXFoundryCA/EdgeXFoundryCA.pem \\ --location \\ --header \"X-Vault-Token: 01dbbae4-353a-8cdf-8189-4d50e5535a6f\" \\ --request GET \\ https://edgex-vault:8200/v1/sys/seal-status | jq Sample JSON returned: { \"type\" : \"shamir\" , \"sealed\" : false , \"t\" : 1 , \"n\" : 1 , \"progress\" : 0 , \"nonce\" : \"\" , \"version\" : \"0.10.2\" , \"cluster_name\" : \"vault-cluster-57b3c4ed\" , \"cluster_id\" : \"fe6d18bf-fa9c-0d52-3278-bca0390af023\" } Note Line 3: Vault is unsealed therefore available and ready for requests. Line 4 & 5: Vault Shamir Secret Sharing default configuration for EdgeX Foundry: 1 share with threshold 1 (no sharding).","title":"3rd alternative: directly using the Vault HTTP API with cURL commands."},{"location":"microservices/security/Ch-SecretStore/#see-also","text":"Some of the command used in implementing security services have man-style documentation: security-file-token-provider - Generate Vault tokens for EdgeX services security-secrets-setup - Creates an on-device public-key infrastructure (PKI) to secure microservice secret management","title":"See also"},{"location":"microservices/security/Ch-Security/","text":"Security Security elements, both inside and outside of EdgeX Foundry, protect the data and control of devices, sensors, and other IoT objects managed by EdgeX Foundry. Based on the fact that EdgeX is a \"vendor-neutral open source software platform at the edge of the network\", the EdgeX security features are also built on a foundation of open interfaces and pluggable, replaceable modules. With security service enabled, the administrator of the EdgeX would be able to initialize the security components, set up running environment for security services, manage user access control, and create JWT( JSON Web Token) for resource access for other EdgeX business services. There are two major EdgeX security components. The first is a security store, which is used to provide a safe place to keep the EdgeX secrets. The second is an API gateway, which is used as a reverse proxy to restrict access to EdgeX REST resources and perform access control related works. In summary, the current features are as below: Secret creation, store and retrieve (password, cert, access key etc.) API gateway for other existing EdgeX microservice REST APIs User account creation with optional either OAuth2 or JWT authentication User account with arbitrary Access Control List groups (ACL)","title":"Security"},{"location":"microservices/security/Ch-Security/#security","text":"Security elements, both inside and outside of EdgeX Foundry, protect the data and control of devices, sensors, and other IoT objects managed by EdgeX Foundry. Based on the fact that EdgeX is a \"vendor-neutral open source software platform at the edge of the network\", the EdgeX security features are also built on a foundation of open interfaces and pluggable, replaceable modules. With security service enabled, the administrator of the EdgeX would be able to initialize the security components, set up running environment for security services, manage user access control, and create JWT( JSON Web Token) for resource access for other EdgeX business services. There are two major EdgeX security components. The first is a security store, which is used to provide a safe place to keep the EdgeX secrets. The second is an API gateway, which is used as a reverse proxy to restrict access to EdgeX REST resources and perform access control related works. In summary, the current features are as below: Secret creation, store and retrieve (password, cert, access key etc.) API gateway for other existing EdgeX microservice REST APIs User account creation with optional either OAuth2 or JWT authentication User account with arbitrary Access Control List groups (ACL)","title":"Security"},{"location":"microservices/security/Ch-SecurityIssues/","text":"Security Issues This page describes how to report EdgeX Foundry security issues and how they are handled. Security Announcements Join the edgexfoundry-announce group at: https://groups.google.com/d/forum/edgexfoundry-announce ) for emails about security and major API announcements. Vulnerability Reporting The EdgeX Foundry Open Source Community is grateful for all security reports made by users and security researchers. All reports are thoroughly investigated by a set of community volunteers. To make a report, please email the private list: security-issues@edgexfoundry.org , providing as much detail as possible. Use the security issue template: security_issue_template . At this time we do not yet offer an encrypted bug reporting option. When to Report a Vulnerability? You think you discovered a potential security vulnerability in EdgeX Foundry You are unsure how a vulnerability affects EdgeX Foundry You think you discovered a vulnerability in another project that EdgeX Foundry depends upon (e.g. docker, MongoDB, Redis,..) When NOT to Report a Vulnerability? You need help tuning EdgeX Foundry components for security You need help applying security related updates Your issue is not security related Security Vulnerability Response Each report is acknowledged and analyzed by Security Issue Review (SIR) team within one week. Any vulnerability information shared with SIR stays private, and is shared with sub-projects as necessary to get the issue fixed. As the security issue moves from triage, to identified fix, to release planning we will keep the reporter updated. In the case of 3 rd party dependency (code or library not managed and maintained by the EdgeX community) related security issues, while the issue report triggers the same response workflow, the EdgeX community will defer to owning community for fixes. On receipt of a security issue report, SIR: Discusses the issue privately to understand it Uses the Common Vulnerability Scoring System to grade the issue Determines the sub-projects and developers to involve Develops a fix In conjunction with the product group determines when to release the fix Communicates the fix 7. Uploads a Common Vulnerabilities and Exposures (CVE) style report of the issue and associated threat The issue reporter will be kept in the loop as appropriate. Note that a critical or high severity issue can delay a scheduled release to incorporate a fix or mitigation. Public Disclosure Timing A public disclosure date is negotiated by the EdgeX Product Security Committee and the bug submitter. We prefer to fully disclose the bug as soon as possible AFTER a mitigation is available. It is reasonable to delay disclosure when the bug or the fix is not yet fully understood, the solution is not well-tested, or for vendor coordination. The timeframe for disclosure may be immediate (especially publicly known issues) to a few weeks. The EdgeX Foundry Product Security Committee holds the final say when setting a disclosure date.","title":"Security Issues"},{"location":"microservices/security/Ch-SecurityIssues/#security-issues","text":"This page describes how to report EdgeX Foundry security issues and how they are handled.","title":"Security Issues"},{"location":"microservices/security/Ch-SecurityIssues/#security-announcements","text":"Join the edgexfoundry-announce group at: https://groups.google.com/d/forum/edgexfoundry-announce ) for emails about security and major API announcements.","title":"Security Announcements"},{"location":"microservices/security/Ch-SecurityIssues/#vulnerability-reporting","text":"The EdgeX Foundry Open Source Community is grateful for all security reports made by users and security researchers. All reports are thoroughly investigated by a set of community volunteers. To make a report, please email the private list: security-issues@edgexfoundry.org , providing as much detail as possible. Use the security issue template: security_issue_template . At this time we do not yet offer an encrypted bug reporting option.","title":"Vulnerability Reporting"},{"location":"microservices/security/Ch-SecurityIssues/#when-to-report-a-vulnerability","text":"You think you discovered a potential security vulnerability in EdgeX Foundry You are unsure how a vulnerability affects EdgeX Foundry You think you discovered a vulnerability in another project that EdgeX Foundry depends upon (e.g. docker, MongoDB, Redis,..)","title":"When to Report a Vulnerability?"},{"location":"microservices/security/Ch-SecurityIssues/#when-not-to-report-a-vulnerability","text":"You need help tuning EdgeX Foundry components for security You need help applying security related updates Your issue is not security related","title":"When NOT to Report a Vulnerability?"},{"location":"microservices/security/Ch-SecurityIssues/#security-vulnerability-response","text":"Each report is acknowledged and analyzed by Security Issue Review (SIR) team within one week. Any vulnerability information shared with SIR stays private, and is shared with sub-projects as necessary to get the issue fixed. As the security issue moves from triage, to identified fix, to release planning we will keep the reporter updated. In the case of 3 rd party dependency (code or library not managed and maintained by the EdgeX community) related security issues, while the issue report triggers the same response workflow, the EdgeX community will defer to owning community for fixes. On receipt of a security issue report, SIR: Discusses the issue privately to understand it Uses the Common Vulnerability Scoring System to grade the issue Determines the sub-projects and developers to involve Develops a fix In conjunction with the product group determines when to release the fix Communicates the fix 7. Uploads a Common Vulnerabilities and Exposures (CVE) style report of the issue and associated threat The issue reporter will be kept in the loop as appropriate. Note that a critical or high severity issue can delay a scheduled release to incorporate a fix or mitigation.","title":"Security Vulnerability Response"},{"location":"microservices/security/Ch-SecurityIssues/#public-disclosure-timing","text":"A public disclosure date is negotiated by the EdgeX Product Security Committee and the bug submitter. We prefer to fully disclose the bug as soon as possible AFTER a mitigation is available. It is reasonable to delay disclosure when the bug or the fix is not yet fully understood, the solution is not well-tested, or for vendor coordination. The timeframe for disclosure may be immediate (especially publicly known issues) to a few weeks. The EdgeX Foundry Product Security Committee holds the final say when setting a disclosure date.","title":"Public Disclosure Timing"},{"location":"microservices/security/Ch-StartingSecurity/","text":"Starting security services within EdgeX Similar to other EdgeX services, the security service can be started with Docker Compose. The security services can be started automatically with docker-compose up --d with proper docker compose file. An working sample docker compose file can be found from the edgex repo of Github at https://github.com/edgexfoundry/security-api-gateway/ . If the user prefers to start the security service manually, the commands are described below. docker-compose up -d volume docker-compose up -d config-seed docker-compose up -d consul docker-compose up -d vault docker-compose up -d vault-worker docker-compose up -d kong-db docker-compose up -d kong-migrations docker-compose up -d kong docker-compose up -d edgex-proxy","title":"Starting security services within EdgeX"},{"location":"microservices/security/Ch-StartingSecurity/#starting-security-services-within-edgex","text":"Similar to other EdgeX services, the security service can be started with Docker Compose. The security services can be started automatically with docker-compose up --d with proper docker compose file. An working sample docker compose file can be found from the edgex repo of Github at https://github.com/edgexfoundry/security-api-gateway/ . If the user prefers to start the security service manually, the commands are described below. docker-compose up -d volume docker-compose up -d config-seed docker-compose up -d consul docker-compose up -d vault docker-compose up -d vault-worker docker-compose up -d kong-db docker-compose up -d kong-migrations docker-compose up -d kong docker-compose up -d edgex-proxy","title":"Starting security services within EdgeX"},{"location":"microservices/security/security-file-token-provider.1/","text":"NAME security-file-token-provider -- Generate Vault tokens for EdgeX services SYNOPSIS security-file-token-provider [-h--confdir \\<confdir>] [-p|--profile \\<name>] DESCRIPTION security-file-token-provider generates per-service Vault tokens for EdgeX services so that they can make authenticated connections to Vault to retrieve application secrets. security-file-token-provider implements a generic secret seeding mechanism based on pre-created files and is designed for maximum portability. security-file-token-provider takes a configuration file that specifies the services for which tokens shall be generated and the Vault access policy that shall be applied to those tokens. security-file-token-provider assumes that there is some underlying protection mechanism that will be used to prevent EdgeX services from reading each other's tokens. OPTIONS -h, --help : Display help text -c, --confdir \\<confdir> : Look in this directory for configuration.toml instead. -p, --profile \\<name> : Indicate configuration profile other than default FILES configuration.toml This file specifies the TCP/IP location of the Vault service and parameters used for Vault token generation. [SecretService] Scheme = \"https\" Server = \"localhost\" Port = 8200 [TokenFileProvider] PrivilegedTokenPath = /run/edgex/secrets/security-file-token-provider/secrets-token.json ConfigFile = token-config.json OutputDir = /run/edgex/secrets/ OutputFilename = secrets-token.json secrets-token.json This file contains a token used to authenticate to Vault. The filename is customizable via OutputFilename . { \"auth\": { \"client_token\": \"s.wOrq9dO9kzOcuvB06CMviJhZ\" } } token-config.json This configuration file tells security-file-token-provider which tokens to generate. In order to avoid a directory full of .hcl files, this configuration file uses the JSON serialization of HCL, documented at https://github.com/hashicorp/hcl/blob/master/README.md . Note that all paths are keys under the \"path\" object. { \"service-name\": { \"edgex_use_defaults\": true, \"custom_policy\": { \"path\": { \"secret/non/standard/location/*\": { \"capabilities\": [ \"list\", \"read\" ] } } }, \"custom_token_parameters\": { } } } When edgex-use-default is true (the default), the following is added to the policy specification for the auto-generated policy. The auto-generated policy is named edgex-secrets-XYZ where XYZ is service-name from the JSON key above. Thus, the final policy created for the token will be the union of the policy below (if using the default policy) plus the custom_policy defined above. { \"path\": { \"secret/edgex/service-name/*\": { \"capabilities\": [ \"create\", \"update\", \"delete\", \"list\", \"read\" ] } } } When edgex-use-default is true (the default), the following is inserted (if not overridden) to the token parameters for the generated token. (See https://www.vaultproject.io/api/auth/token/index.html#create-token .) \"display_name\": token-service-name \"no_parent\": true \"policies\": [ \"edgex-service-service-name\" ] Note that display_name is set by vault to be \"token-\" + the specified display name. This is hard-coded in Vault from versions 0.6 to 1.2.3 and cannot be changed. Additionally, a meta property, edgex-service-name is set to service-name . The edgex-service-name property may be used by clients to infer the location in the secret store where service-specific secrets are held. \"meta\": { \"edgex-service-name\": service-name } {OutputDir}/{service-name}/{OutputFilename} For example: /run/edgex/secrets/edgex-security-proxy-setup/secrets-token.json For each \"service-name\" in {ConfigFile} , a matching directory is created under {OutputDir} and the corresponding Vault token is stored as {OutputFilename} . This file contains the authorization token generated to allow the indicated EdgeX service to retrieve its secrets. PREREQUISITES PrivilegedTokenPath points to a non-expired Vault token that the security-file-token-provider will use to install policies and create per-service tokens. It will create policies with the naming convention \"edgex-service-service-name\" where service-name comes from JSON keys in the configuration file and the Vault policy will be configured to allow creation and modification of policies using this naming convention. This token must have the following policy ( edgex-privileged-token-creator ) configured. path \"auth/token/create\" { capabilities = [ \"create\" , \"update\" , \"sudo\" ] } path \"auth/token/create-orphan\" { capabilities = [ \"create\" , \"update\" , \"sudo\" ] } path \"auth/token/create/*\" { capabilities = [ \"create\" , \"update\" , \"sudo\" ] } path \"sys/policies/acl/edgex-service-*\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"delete\" ] } path \"sys/policies/acl\" { capabilities = [ \"list\" ] } AUTHOR EdgeX Foundry \\< info@edgexfoundry.org >","title":"NAME"},{"location":"microservices/security/security-file-token-provider.1/#name","text":"security-file-token-provider -- Generate Vault tokens for EdgeX services","title":"NAME"},{"location":"microservices/security/security-file-token-provider.1/#synopsis","text":"security-file-token-provider [-h--confdir \\<confdir>] [-p|--profile \\<name>]","title":"SYNOPSIS"},{"location":"microservices/security/security-file-token-provider.1/#description","text":"security-file-token-provider generates per-service Vault tokens for EdgeX services so that they can make authenticated connections to Vault to retrieve application secrets. security-file-token-provider implements a generic secret seeding mechanism based on pre-created files and is designed for maximum portability. security-file-token-provider takes a configuration file that specifies the services for which tokens shall be generated and the Vault access policy that shall be applied to those tokens. security-file-token-provider assumes that there is some underlying protection mechanism that will be used to prevent EdgeX services from reading each other's tokens.","title":"DESCRIPTION"},{"location":"microservices/security/security-file-token-provider.1/#options","text":"-h, --help : Display help text -c, --confdir \\<confdir> : Look in this directory for configuration.toml instead. -p, --profile \\<name> : Indicate configuration profile other than default","title":"OPTIONS"},{"location":"microservices/security/security-file-token-provider.1/#files","text":"","title":"FILES"},{"location":"microservices/security/security-file-token-provider.1/#configurationtoml","text":"This file specifies the TCP/IP location of the Vault service and parameters used for Vault token generation. [SecretService] Scheme = \"https\" Server = \"localhost\" Port = 8200 [TokenFileProvider] PrivilegedTokenPath = /run/edgex/secrets/security-file-token-provider/secrets-token.json ConfigFile = token-config.json OutputDir = /run/edgex/secrets/ OutputFilename = secrets-token.json","title":"configuration.toml"},{"location":"microservices/security/security-file-token-provider.1/#secrets-tokenjson","text":"This file contains a token used to authenticate to Vault. The filename is customizable via OutputFilename . { \"auth\": { \"client_token\": \"s.wOrq9dO9kzOcuvB06CMviJhZ\" } }","title":"secrets-token.json"},{"location":"microservices/security/security-file-token-provider.1/#token-configjson","text":"This configuration file tells security-file-token-provider which tokens to generate. In order to avoid a directory full of .hcl files, this configuration file uses the JSON serialization of HCL, documented at https://github.com/hashicorp/hcl/blob/master/README.md . Note that all paths are keys under the \"path\" object. { \"service-name\": { \"edgex_use_defaults\": true, \"custom_policy\": { \"path\": { \"secret/non/standard/location/*\": { \"capabilities\": [ \"list\", \"read\" ] } } }, \"custom_token_parameters\": { } } } When edgex-use-default is true (the default), the following is added to the policy specification for the auto-generated policy. The auto-generated policy is named edgex-secrets-XYZ where XYZ is service-name from the JSON key above. Thus, the final policy created for the token will be the union of the policy below (if using the default policy) plus the custom_policy defined above. { \"path\": { \"secret/edgex/service-name/*\": { \"capabilities\": [ \"create\", \"update\", \"delete\", \"list\", \"read\" ] } } } When edgex-use-default is true (the default), the following is inserted (if not overridden) to the token parameters for the generated token. (See https://www.vaultproject.io/api/auth/token/index.html#create-token .) \"display_name\": token-service-name \"no_parent\": true \"policies\": [ \"edgex-service-service-name\" ] Note that display_name is set by vault to be \"token-\" + the specified display name. This is hard-coded in Vault from versions 0.6 to 1.2.3 and cannot be changed. Additionally, a meta property, edgex-service-name is set to service-name . The edgex-service-name property may be used by clients to infer the location in the secret store where service-specific secrets are held. \"meta\": { \"edgex-service-name\": service-name }","title":"token-config.json"},{"location":"microservices/security/security-file-token-provider.1/#outputdirservice-nameoutputfilename","text":"For example: /run/edgex/secrets/edgex-security-proxy-setup/secrets-token.json For each \"service-name\" in {ConfigFile} , a matching directory is created under {OutputDir} and the corresponding Vault token is stored as {OutputFilename} . This file contains the authorization token generated to allow the indicated EdgeX service to retrieve its secrets.","title":"{OutputDir}/{service-name}/{OutputFilename}"},{"location":"microservices/security/security-file-token-provider.1/#prerequisites","text":"PrivilegedTokenPath points to a non-expired Vault token that the security-file-token-provider will use to install policies and create per-service tokens. It will create policies with the naming convention \"edgex-service-service-name\" where service-name comes from JSON keys in the configuration file and the Vault policy will be configured to allow creation and modification of policies using this naming convention. This token must have the following policy ( edgex-privileged-token-creator ) configured. path \"auth/token/create\" { capabilities = [ \"create\" , \"update\" , \"sudo\" ] } path \"auth/token/create-orphan\" { capabilities = [ \"create\" , \"update\" , \"sudo\" ] } path \"auth/token/create/*\" { capabilities = [ \"create\" , \"update\" , \"sudo\" ] } path \"sys/policies/acl/edgex-service-*\" { capabilities = [ \"create\" , \"read\" , \"update\" , \"delete\" ] } path \"sys/policies/acl\" { capabilities = [ \"list\" ] }","title":"PREREQUISITES"},{"location":"microservices/security/security-file-token-provider.1/#author","text":"EdgeX Foundry \\< info@edgexfoundry.org >","title":"AUTHOR"},{"location":"microservices/security/security-secrets-setup.1/","text":"NAME security-secrets-setup --- Creates an on-device public-key infrastructure (PKI) to secure microservice secret management SYNOPSIS | security-secrets-setup generate [-confdir \\<confdir>] [-p|--profile \\<name>] | security-secrets-setup cache [-confdir \\<confdir>] [-p|--profile \\<name>] | security-secrets-setup import [-confdir \\<confdir>] [-p|--profile \\<name>] | security-secrets-setup [-h|--help] DESCRIPTION The Vault secret management component of EdgeX Foundry requires TLS encryption of secrets over the wire via a pre-created PKI. security-secrets-setup is responsible for creating a certificate authority and any needed TLS leaf certificates in order to secure the EdgeX security services. security-secrets-setup supports several modes of operation as defined in the OPTIONS section. As the PKI is security-sensitive, this tool takes a number of precautions to safeguard the PKI: The PKI can be deployed to transient storage to address potential attacks to the PKI at-rest. The PKI is deployed such that each service has its own assets folder, which is amenable to security controls imposed by container runtimes such as mandatory access controls or file system namespaces. The private key of the certificate authority (CA) is shredded (securely erased) prior to caching or deployment to block issuance of new CA descendants (this is most relevant in caching mode). Modes of operation generate : Causes a PKI to be generated afresh every time and deployed. Typically, this will be whenever the framework is started. cache : Causes a PKI to be generated exactly once and then copied to a designated cache location for future use. The PKI is then deployed from the cached location. import : This option is similar to cache in that it deploys a PKI from CacheDir to DeployDir , but it forces an error if CacheDir is empty instead of triggering PKI generation. This enables usage models for deploying a pre-populated PKI such as a Kong certificate signed by an external certificate authority or TLS keys signed by an offline enterprise certificate authority. OPTIONS -h, --help : Display help text --confdir \\<confdir> : Look in this directory for configuration.toml instead. -p, --profile \\<name> : Indicate configuration profile other than default FILES pkisetup-vault.json, pkisetup-kong.json --------------------------------- Configuration files for certificate parameters. These files conform to the following schema: { \"create_new_rootca\": \"true|false\", \"working_dir\": \"./config\", \"pki_setup_dir\": \"pki\", \"dump_config\": \"true\", \"key_scheme\": { \"dump_keys\": \"false\", \"rsa\": \"false\", \"rsa_key_size\": \"4096\", \"ec\": \"true\", \"ec_curve\": \"384\" }, \"x509_root_ca_parameters\": { \"ca_name\": \"EdgeXFoundryCA\", \"ca_c\": \"US\", \"ca_st\": \"CA\", \"ca_l\": \"San Francisco\", \"ca_o\": \"EdgeXFoundry\" }, \"x509_tls_server_parameters\": { \"tls_host\": \"edgex-vault|edgex-kong\", \"tls_domain\": \"local\", \"tls_c\": \"US\", \"tls_st\": \"CA\", \"tls_l\": \"San Francisco\", \"tls_o\": \"Kong\" } } When generating or caching, the utility hard-codes the names of the configuration files and always processes pkisetup-vault.json first and pkisetup-kong.json second. This will be configurable in the future; at that time the basename of the file would correspond to a directory under DeployDir . configuration.toml Configuration file for configurable directories that the options use. This file conforms to the following schema: [SecretsSetup] WorkDir = \"/path/to/temp/files\" CacheDir = \"/path/to/cached-or-importing/pki\" DeployDir = \"/path/to/deployed/pki\" WorkDir A work area (preferably on a ramdisk) to place working files during certificate generation. If not supplied, temporary files will be generated to a subdirectory ( /edgex/security-secrets-setup ) of $XDG_RUNTIME_DIR . If $XDG_RUNTIME_DIR is undefined, uses /tmp instead. DeployDir Points to the base directory for the final deployment location of the PKI. If not specified, defaults to /run/edgex/secrets/ . For example, if DeployDir was set to /edgex and the service name was edgex-vault then the following files would be placed in /edgex/edgex-vault/ : server.crt for a PEM-encoded end-entity TLS certificate and server.key for the corresponding private key .security-secrets-setup.complete is a sentinel file created after assets are deployed CacheDir Points to a base directory to hold the cached PKI. Identical in structure to that created in DeployDir . Defaults to /etc/edgex/pki if not specified. The PKI is deployed from here when the tool is run in caching or importing. ENVIRONMENT XDG_RUNTIME_DIR Used as default value for WorkDir if not otherwise specified. NOTES As security-secrets-setup is a helper utility to ensure that a PKI is created on first launch, it is intended that security-secrets-setup is always invoked with the same command. Changing from cache to generate will cause the cache to be ignored when deploying a PKI and changing it back will cause a reversion to a stale CA. Changing from cache to import mode of operation is not noticeable by the tool: the PKI that is in the cache will be the one deployed. To force regeneration of the PKI cache after the first launch, the PKI cache must be manually cleaned. The easiest way in Docker would be to delete the Docker volume holding the cached PKI.","title":"NAME"},{"location":"microservices/security/security-secrets-setup.1/#name","text":"security-secrets-setup --- Creates an on-device public-key infrastructure (PKI) to secure microservice secret management","title":"NAME"},{"location":"microservices/security/security-secrets-setup.1/#synopsis","text":"| security-secrets-setup generate [-confdir \\<confdir>] [-p|--profile \\<name>] | security-secrets-setup cache [-confdir \\<confdir>] [-p|--profile \\<name>] | security-secrets-setup import [-confdir \\<confdir>] [-p|--profile \\<name>] | security-secrets-setup [-h|--help]","title":"SYNOPSIS"},{"location":"microservices/security/security-secrets-setup.1/#description","text":"The Vault secret management component of EdgeX Foundry requires TLS encryption of secrets over the wire via a pre-created PKI. security-secrets-setup is responsible for creating a certificate authority and any needed TLS leaf certificates in order to secure the EdgeX security services. security-secrets-setup supports several modes of operation as defined in the OPTIONS section. As the PKI is security-sensitive, this tool takes a number of precautions to safeguard the PKI: The PKI can be deployed to transient storage to address potential attacks to the PKI at-rest. The PKI is deployed such that each service has its own assets folder, which is amenable to security controls imposed by container runtimes such as mandatory access controls or file system namespaces. The private key of the certificate authority (CA) is shredded (securely erased) prior to caching or deployment to block issuance of new CA descendants (this is most relevant in caching mode).","title":"DESCRIPTION"},{"location":"microservices/security/security-secrets-setup.1/#modes-of-operation","text":"generate : Causes a PKI to be generated afresh every time and deployed. Typically, this will be whenever the framework is started. cache : Causes a PKI to be generated exactly once and then copied to a designated cache location for future use. The PKI is then deployed from the cached location. import : This option is similar to cache in that it deploys a PKI from CacheDir to DeployDir , but it forces an error if CacheDir is empty instead of triggering PKI generation. This enables usage models for deploying a pre-populated PKI such as a Kong certificate signed by an external certificate authority or TLS keys signed by an offline enterprise certificate authority.","title":"Modes of operation"},{"location":"microservices/security/security-secrets-setup.1/#options","text":"-h, --help : Display help text --confdir \\<confdir> : Look in this directory for configuration.toml instead. -p, --profile \\<name> : Indicate configuration profile other than default","title":"OPTIONS"},{"location":"microservices/security/security-secrets-setup.1/#files","text":"pkisetup-vault.json, pkisetup-kong.json --------------------------------- Configuration files for certificate parameters. These files conform to the following schema: { \"create_new_rootca\": \"true|false\", \"working_dir\": \"./config\", \"pki_setup_dir\": \"pki\", \"dump_config\": \"true\", \"key_scheme\": { \"dump_keys\": \"false\", \"rsa\": \"false\", \"rsa_key_size\": \"4096\", \"ec\": \"true\", \"ec_curve\": \"384\" }, \"x509_root_ca_parameters\": { \"ca_name\": \"EdgeXFoundryCA\", \"ca_c\": \"US\", \"ca_st\": \"CA\", \"ca_l\": \"San Francisco\", \"ca_o\": \"EdgeXFoundry\" }, \"x509_tls_server_parameters\": { \"tls_host\": \"edgex-vault|edgex-kong\", \"tls_domain\": \"local\", \"tls_c\": \"US\", \"tls_st\": \"CA\", \"tls_l\": \"San Francisco\", \"tls_o\": \"Kong\" } } When generating or caching, the utility hard-codes the names of the configuration files and always processes pkisetup-vault.json first and pkisetup-kong.json second. This will be configurable in the future; at that time the basename of the file would correspond to a directory under DeployDir .","title":"FILES"},{"location":"microservices/security/security-secrets-setup.1/#configurationtoml","text":"Configuration file for configurable directories that the options use. This file conforms to the following schema: [SecretsSetup] WorkDir = \"/path/to/temp/files\" CacheDir = \"/path/to/cached-or-importing/pki\" DeployDir = \"/path/to/deployed/pki\"","title":"configuration.toml"},{"location":"microservices/security/security-secrets-setup.1/#workdir","text":"A work area (preferably on a ramdisk) to place working files during certificate generation. If not supplied, temporary files will be generated to a subdirectory ( /edgex/security-secrets-setup ) of $XDG_RUNTIME_DIR . If $XDG_RUNTIME_DIR is undefined, uses /tmp instead.","title":"WorkDir"},{"location":"microservices/security/security-secrets-setup.1/#deploydir","text":"Points to the base directory for the final deployment location of the PKI. If not specified, defaults to /run/edgex/secrets/ . For example, if DeployDir was set to /edgex and the service name was edgex-vault then the following files would be placed in /edgex/edgex-vault/ : server.crt for a PEM-encoded end-entity TLS certificate and server.key for the corresponding private key .security-secrets-setup.complete is a sentinel file created after assets are deployed","title":"DeployDir"},{"location":"microservices/security/security-secrets-setup.1/#cachedir","text":"Points to a base directory to hold the cached PKI. Identical in structure to that created in DeployDir . Defaults to /etc/edgex/pki if not specified. The PKI is deployed from here when the tool is run in caching or importing.","title":"CacheDir"},{"location":"microservices/security/security-secrets-setup.1/#environment","text":"XDG_RUNTIME_DIR Used as default value for WorkDir if not otherwise specified.","title":"ENVIRONMENT"},{"location":"microservices/security/security-secrets-setup.1/#notes","text":"As security-secrets-setup is a helper utility to ensure that a PKI is created on first launch, it is intended that security-secrets-setup is always invoked with the same command. Changing from cache to generate will cause the cache to be ignored when deploying a PKI and changing it back will cause a reversion to a stale CA. Changing from cache to import mode of operation is not noticeable by the tool: the PKI that is in the cache will be the one deployed. To force regeneration of the PKI cache after the first launch, the PKI cache must be manually cleaned. The easiest way in Docker would be to delete the Docker volume holding the cached PKI.","title":"NOTES"},{"location":"microservices/support/Ch-SupportingServices/","text":"Supporting Services Microservices","title":"Supporting Services Microservices"},{"location":"microservices/support/Ch-SupportingServices/#supporting-services-microservices","text":"","title":"Supporting Services Microservices"},{"location":"microservices/support/Kuiper/Ch-Kuiper/","text":"Kuiper Rules Engine Overview EMQ X Kuiper is the new EdgeX reference implementation rules engine implementation (replacing the Support Rules Engine - which wrapped the Java Drools engine). What is EMQ X Kuiper? EMQ X Kuiper is a lightweight open source software (Apache 2.0 open source license agreement) package for IoT edge analytics and stream processing implemented in Go lang, which can run on various resource constrained edge devices. Users can realize fast data processing on the edge and write rules in SQL. The Kuiper rules engine is based on three components Source , SQL and Sink . Source: Source of stream data, such as data from an MQTT server. For EdgeX, the data source is an EdgeX message bus, which can be implemented by ZeroMQ or MQTT; SQL: SQL is where the specified business logic is processed. Kuiper provides SQL statements to extract, filter, and transform data; Sink: Used to send the analysis result to a specific target, such as sending the analysis results to EdgeX's Command service, or an MQTT broker in the cloud; The relationship among Source, SQL and Sink in Kuiper is shown below. Kuiper runs very efficiently on resource constrained edge devices. For common IoT data processing, the throughput can reach 12k per second. Readers can refer to here to get more performance benchmark data for Kuiper. Kuiper rules engine of EdgeX An extension mechanism allows Kuiper to be customized to analyze and process data from different data sources. By default for the EdgeX configuration, Kuiper analyzes data coming from the EdgeX message bus . EdgeX provides an abstract message bus interface, and implements the ZeroMQ and MQTT protocols respectively to support information exchange between different micro-services. The integration of Kuiper and EdgeX mainly includes the following: Extend an EdgeX message bus source to support receiving data from the EdgeX message bus. By default, Kuiper listens to the port 5566 on which the Application Service publishes messages. After the data from the Core Data Service is processed by the Application Service, it will flow into the Kuiper rules engine for processing. Read the data type definition from Core Contract Service, convert EdgeX data to Kuiper data type, and process it according to the rules specified by the user. Kuiper supports sending analysis results to different Sink: The users can choose to send the analysis results to Command Service to control the equipment; The analysis results can be sent to the EdgeX message bus sink for further processing by other micro-services. Learn more EdgeX Kuiper Rules Engine Tutorial : A 10-minute quick start tutorial, readers can refer to this article to start trying out the rules engine. Control the device with the EdgeX Kuiper rules engine : This article describes how to use the Kuiper rule engine in EdgeX to control the device based on the analysis results. Read EdgeX Source to get more detailed information, and type conversions. How to use the meta function to extract more information sent in the EdgeX message bus? When the device service sends data to the bus, some additional information is also sent, such as creation time and id. If you want to use this information in SQL statements, please refer to this article. EdgeX Message Bus Sink : This document describes how to use the EdgeX message bus sink. If you want to send the analysis results to the message bus, you may be interested in this article. For more information on the EMQ X Kuiper project, please refer to the following resources. Kuiper Github Code library Kuiper Reference","title":"Kuiper Rules Engine"},{"location":"microservices/support/Kuiper/Ch-Kuiper/#kuiper-rules-engine","text":"","title":"Kuiper Rules Engine"},{"location":"microservices/support/Kuiper/Ch-Kuiper/#overview","text":"EMQ X Kuiper is the new EdgeX reference implementation rules engine implementation (replacing the Support Rules Engine - which wrapped the Java Drools engine).","title":"Overview"},{"location":"microservices/support/Kuiper/Ch-Kuiper/#what-is-emq-x-kuiper","text":"EMQ X Kuiper is a lightweight open source software (Apache 2.0 open source license agreement) package for IoT edge analytics and stream processing implemented in Go lang, which can run on various resource constrained edge devices. Users can realize fast data processing on the edge and write rules in SQL. The Kuiper rules engine is based on three components Source , SQL and Sink . Source: Source of stream data, such as data from an MQTT server. For EdgeX, the data source is an EdgeX message bus, which can be implemented by ZeroMQ or MQTT; SQL: SQL is where the specified business logic is processed. Kuiper provides SQL statements to extract, filter, and transform data; Sink: Used to send the analysis result to a specific target, such as sending the analysis results to EdgeX's Command service, or an MQTT broker in the cloud; The relationship among Source, SQL and Sink in Kuiper is shown below. Kuiper runs very efficiently on resource constrained edge devices. For common IoT data processing, the throughput can reach 12k per second. Readers can refer to here to get more performance benchmark data for Kuiper.","title":"What is EMQ X Kuiper?"},{"location":"microservices/support/Kuiper/Ch-Kuiper/#kuiper-rules-engine-of-edgex","text":"An extension mechanism allows Kuiper to be customized to analyze and process data from different data sources. By default for the EdgeX configuration, Kuiper analyzes data coming from the EdgeX message bus . EdgeX provides an abstract message bus interface, and implements the ZeroMQ and MQTT protocols respectively to support information exchange between different micro-services. The integration of Kuiper and EdgeX mainly includes the following: Extend an EdgeX message bus source to support receiving data from the EdgeX message bus. By default, Kuiper listens to the port 5566 on which the Application Service publishes messages. After the data from the Core Data Service is processed by the Application Service, it will flow into the Kuiper rules engine for processing. Read the data type definition from Core Contract Service, convert EdgeX data to Kuiper data type, and process it according to the rules specified by the user. Kuiper supports sending analysis results to different Sink: The users can choose to send the analysis results to Command Service to control the equipment; The analysis results can be sent to the EdgeX message bus sink for further processing by other micro-services.","title":"Kuiper rules engine of EdgeX"},{"location":"microservices/support/Kuiper/Ch-Kuiper/#learn-more","text":"EdgeX Kuiper Rules Engine Tutorial : A 10-minute quick start tutorial, readers can refer to this article to start trying out the rules engine. Control the device with the EdgeX Kuiper rules engine : This article describes how to use the Kuiper rule engine in EdgeX to control the device based on the analysis results. Read EdgeX Source to get more detailed information, and type conversions. How to use the meta function to extract more information sent in the EdgeX message bus? When the device service sends data to the bus, some additional information is also sent, such as creation time and id. If you want to use this information in SQL statements, please refer to this article. EdgeX Message Bus Sink : This document describes how to use the EdgeX message bus sink. If you want to send the analysis results to the message bus, you may be interested in this article. For more information on the EMQ X Kuiper project, please refer to the following resources. Kuiper Github Code library Kuiper Reference","title":"Learn more"},{"location":"microservices/support/logging/Ch-Logging/","text":"Logging (Deprecated in Geneva Release) Deprecation Notice Please note that the logging service has been deprecated with the Geneva release (v1.2). The EdgeX community feels that there are better log aggregation services available in the open source community or by deployment/orchestration tools. Starting with the Geneva release, logging service will no longer be started as part of the reference implementations provided through the EdgeX Docker Compose files (the service is still available but commented out in those files). By default, all services now log to standard out (EnableRemote is set to false and File is set to ''). If users wish to still use the central logging service, they must configure each service to use it (set EnableRemote=true). Users can still alternately choose to have the services log to a file with additional configuration changes (set File to the appropriate file location). The Support Logging Service will removed in a future release of EdgeX Foundry. Introduction Logging is critical for all modern software applications. Proper logging provides the users with the following benefits: Ability to monitor and understand what systems are doing Ability to understand how services interact with each other Problems are detected and fixed quickly Monitoring to foster performance improvements The graphic shows the high-level design architecture of EdgeX Foundry including the Logging Service. Minimum Product Feature Set Provides a RESTful API for other microservices to request log entries with the following characteristics: The RESTful calls should be non-blocking, meaning calling services should fire logging requests without waiting for any response from the log service to achieve minimal impact to the speed and performance to the services. Support multiple logging levels, for example trace, debug, info, warn, error, fatal, and so forth. Each log entry should be associated with its originating service. Provide RESTful APIs to query, clear, or prune log entries based on any combination of following parameters: Timestamp from Timestamp to Log level Originating service Log entries should be persisted in either file or database, and the persistence storage should be managed at configurable levels. Querying via the REST API is only supported for deployments using database storage. Take advantage of an existing logging framework internally and provide the \"wrapper\" for use by EdgeX Foundry Follow applicable standards for logging where possible and not onerous to use on the gateway High Level Design Architecture The above diagram shows the high-level architecture for EdgeX Foundry Logging Service. Other microservices interact with EdgeX Foundry Logging Service through RESTful APIs to submit their logging requests, query historical logging, and remove historical logging. Internally, EdgeX Foundry's Logging Service utilizes the GoKit logger as its internal logging framework. Two configurable persistence options exist supported by EdgeX Foundry Logging Service: file or MongoDB. Configuration Properties The following configuration properties are specific to the support-logging service. Please refer to the general Configuration documentation for configuration properties common across all services. Configuration Default Value Dependencies Entries in the Writable section of the configuration can be changed on the fly while the service is running if the service is running with the \u2013configProvider/ -cp=<url> flag Writable Persistence database \"file\" to save logging in file; \"database\" to save logging in MongoDB Logging Service Client Library for Go As the reference implementation of EdgeX Foundry microservices is written in Go, we provide a Client Library for Go so that Go-based microservices could directly switch their Loggers to use the EdgeX Foundry Logging Service. The Go LoggingClient is part of the go-mod-core-contracts module . This module can be imported into your project by including a reference in your go.mod. You can either do this manually or by executing \"go get github.com/edgexfoundry/go-mod-core-contracts\" from your project directory will add a reference to the latest tagged version of the module. After that, simply import \"github.com/edgexfoundry/go-mod-core-contracts/clients/logger\" into a given package where your functionality will be implemented. Declare a variable or type member as logger.LoggingClient and it's ready for use. package main import \"github.com/edgexfoundry/go-mod-core-contracts/clients/logger\" func main () { client : = logger . LoggingClient // LoggingClient is now ready for use . A method is exposed for each LogLevel client . Trace ( \"some info\" ) client . Debug ( \"some info\" ) client . Info ( \"some info\" ) client . Warn ( \"some info\" ) client . Error ( \"some info\" ) } Log statements will only be written to the log if they match or exceed the minimum LogLevel set in the configuration (described above). This setting can be changed on the fly without restarting the service to help with real-time troubleshooting. Log statements are currently output in a simple key/value format. For example: level=INFO ts=2019-05-16T22:23:44.424176Z app=edgex-support-notifications source=cleanup.go:32 msg=\"Cleaning up of notifications and transmissions\" Everything up to the \"msg\" key is handled by the logging infrastructure. You get the log level, timestamp, service name and the location in the source code of the logging statement for free with every method invocation on the LoggingClient. The \"msg\" key's value is the first parameter passed to one of the Logging Client methods shown above. So to extend the usage example a bit, the above calls would result in something like: level=INFO ts=2019-05-16T22:23:44.424176Z app=logging-demo source=main.go:11 msg=\"some info\" You can add as many custom key/value pairs as you like by simply adding them to the method call: client.Info(\"some info\",\"key1\",\"abc\",\"key2\",\"def\") This would result in: level=INFO ts=2019-05-16T22:23:44.424176Z app=logging-demo source=main.go:11 msg=\"some info\" key1=abc key2=def Quotes are only put around values that contain spaces. EdgeX Logging Keys Within the EdgeX Go reference implementation, log entries are currently written as a set of key/value pairs. We may change this later to be more of a struct type than can be formatted according to the user's requirements (JSON, XML, system, etc). In that case, the targeted struct should contain properties that support the keys utilized by the system and described below. Key Intent level Indicates the log level of the individual log entry (INFO, DEBUG, ERROR, etc) ts The timestamp of the log entry, recorded in UTC app This should contain the service key of the service writing the log entry source The file and line number where the log entry was written msg A field for custom information accompanying the log entry. You do not need to specify this explicitly as it is the first parameter when calling one of the LoggingClient's functions. correlation-id Records the correlation-id header value that is scoped to a given request. It has two sub-ordinate, associated fields (see below). correlation-id path This field records the API route being requested and is utilized when the service begins handling a request. * Example: path=/api/v1/event When beginning the request handling, by convention set \"msg\" to \"Begin request\". correlation-id duration This field records the amount of time taken to handle a given request. When completing the request handling, by convention set \"msg\" to \"Response complete\". Additional keys can be added as need warrants. This document should be kept updated to reflect their inclusion and purpose.","title":"Logging"},{"location":"microservices/support/logging/Ch-Logging/#logging","text":"","title":"Logging"},{"location":"microservices/support/logging/Ch-Logging/#deprecated-in-geneva-release","text":"Deprecation Notice Please note that the logging service has been deprecated with the Geneva release (v1.2). The EdgeX community feels that there are better log aggregation services available in the open source community or by deployment/orchestration tools. Starting with the Geneva release, logging service will no longer be started as part of the reference implementations provided through the EdgeX Docker Compose files (the service is still available but commented out in those files). By default, all services now log to standard out (EnableRemote is set to false and File is set to ''). If users wish to still use the central logging service, they must configure each service to use it (set EnableRemote=true). Users can still alternately choose to have the services log to a file with additional configuration changes (set File to the appropriate file location). The Support Logging Service will removed in a future release of EdgeX Foundry.","title":"(Deprecated in Geneva Release)"},{"location":"microservices/support/logging/Ch-Logging/#introduction","text":"Logging is critical for all modern software applications. Proper logging provides the users with the following benefits: Ability to monitor and understand what systems are doing Ability to understand how services interact with each other Problems are detected and fixed quickly Monitoring to foster performance improvements The graphic shows the high-level design architecture of EdgeX Foundry including the Logging Service.","title":"Introduction"},{"location":"microservices/support/logging/Ch-Logging/#minimum-product-feature-set","text":"Provides a RESTful API for other microservices to request log entries with the following characteristics: The RESTful calls should be non-blocking, meaning calling services should fire logging requests without waiting for any response from the log service to achieve minimal impact to the speed and performance to the services. Support multiple logging levels, for example trace, debug, info, warn, error, fatal, and so forth. Each log entry should be associated with its originating service. Provide RESTful APIs to query, clear, or prune log entries based on any combination of following parameters: Timestamp from Timestamp to Log level Originating service Log entries should be persisted in either file or database, and the persistence storage should be managed at configurable levels. Querying via the REST API is only supported for deployments using database storage. Take advantage of an existing logging framework internally and provide the \"wrapper\" for use by EdgeX Foundry Follow applicable standards for logging where possible and not onerous to use on the gateway","title":"Minimum Product Feature Set"},{"location":"microservices/support/logging/Ch-Logging/#high-level-design-architecture","text":"The above diagram shows the high-level architecture for EdgeX Foundry Logging Service. Other microservices interact with EdgeX Foundry Logging Service through RESTful APIs to submit their logging requests, query historical logging, and remove historical logging. Internally, EdgeX Foundry's Logging Service utilizes the GoKit logger as its internal logging framework. Two configurable persistence options exist supported by EdgeX Foundry Logging Service: file or MongoDB.","title":"High Level Design Architecture"},{"location":"microservices/support/logging/Ch-Logging/#configuration-properties","text":"The following configuration properties are specific to the support-logging service. Please refer to the general Configuration documentation for configuration properties common across all services. Configuration Default Value Dependencies Entries in the Writable section of the configuration can be changed on the fly while the service is running if the service is running with the \u2013configProvider/ -cp=<url> flag Writable Persistence database \"file\" to save logging in file; \"database\" to save logging in MongoDB","title":"Configuration Properties"},{"location":"microservices/support/logging/Ch-Logging/#logging-service-client-library-for-go","text":"As the reference implementation of EdgeX Foundry microservices is written in Go, we provide a Client Library for Go so that Go-based microservices could directly switch their Loggers to use the EdgeX Foundry Logging Service. The Go LoggingClient is part of the go-mod-core-contracts module . This module can be imported into your project by including a reference in your go.mod. You can either do this manually or by executing \"go get github.com/edgexfoundry/go-mod-core-contracts\" from your project directory will add a reference to the latest tagged version of the module. After that, simply import \"github.com/edgexfoundry/go-mod-core-contracts/clients/logger\" into a given package where your functionality will be implemented. Declare a variable or type member as logger.LoggingClient and it's ready for use. package main import \"github.com/edgexfoundry/go-mod-core-contracts/clients/logger\" func main () { client : = logger . LoggingClient // LoggingClient is now ready for use . A method is exposed for each LogLevel client . Trace ( \"some info\" ) client . Debug ( \"some info\" ) client . Info ( \"some info\" ) client . Warn ( \"some info\" ) client . Error ( \"some info\" ) } Log statements will only be written to the log if they match or exceed the minimum LogLevel set in the configuration (described above). This setting can be changed on the fly without restarting the service to help with real-time troubleshooting. Log statements are currently output in a simple key/value format. For example: level=INFO ts=2019-05-16T22:23:44.424176Z app=edgex-support-notifications source=cleanup.go:32 msg=\"Cleaning up of notifications and transmissions\" Everything up to the \"msg\" key is handled by the logging infrastructure. You get the log level, timestamp, service name and the location in the source code of the logging statement for free with every method invocation on the LoggingClient. The \"msg\" key's value is the first parameter passed to one of the Logging Client methods shown above. So to extend the usage example a bit, the above calls would result in something like: level=INFO ts=2019-05-16T22:23:44.424176Z app=logging-demo source=main.go:11 msg=\"some info\" You can add as many custom key/value pairs as you like by simply adding them to the method call: client.Info(\"some info\",\"key1\",\"abc\",\"key2\",\"def\") This would result in: level=INFO ts=2019-05-16T22:23:44.424176Z app=logging-demo source=main.go:11 msg=\"some info\" key1=abc key2=def Quotes are only put around values that contain spaces.","title":"Logging Service Client Library for Go"},{"location":"microservices/support/logging/Ch-Logging/#edgex-logging-keys","text":"Within the EdgeX Go reference implementation, log entries are currently written as a set of key/value pairs. We may change this later to be more of a struct type than can be formatted according to the user's requirements (JSON, XML, system, etc). In that case, the targeted struct should contain properties that support the keys utilized by the system and described below. Key Intent level Indicates the log level of the individual log entry (INFO, DEBUG, ERROR, etc) ts The timestamp of the log entry, recorded in UTC app This should contain the service key of the service writing the log entry source The file and line number where the log entry was written msg A field for custom information accompanying the log entry. You do not need to specify this explicitly as it is the first parameter when calling one of the LoggingClient's functions. correlation-id Records the correlation-id header value that is scoped to a given request. It has two sub-ordinate, associated fields (see below). correlation-id path This field records the API route being requested and is utilized when the service begins handling a request. * Example: path=/api/v1/event When beginning the request handling, by convention set \"msg\" to \"Begin request\". correlation-id duration This field records the amount of time taken to handle a given request. When completing the request handling, by convention set \"msg\" to \"Response complete\". Additional keys can be added as need warrants. This document should be kept updated to reflect their inclusion and purpose.","title":"EdgeX Logging Keys"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/","text":"Alerts & Notifications Introduction When notification to another system or to a person, needs to occur to notify of something discovered on the node by another microservice, the Alerts and Notifications microservice delivers that information. Examples of Alerts and Notifications that other services could need to broadcast, include sensor data detected outside of certain parameters (usually detected by a Rules Engine service) or system or service malfunctions (usually detected by System Management services). Terminology Notifications are informative, whereas Alerts are typically of a more important, critical, or urgent nature, possibly requiring immediate action. The diagram shows the high-level architecture of Alerts and Notifications. On the left side, the APIs are provided for other microservices, on-box applications, and off-box applications to use, and the APIs could be in REST, AMQP, MQTT, or any standard application protocols. Currently in EdgeX Foundry, the RESTful interface is provided. On the right side, the notification receiver could be a person or an application system on Cloud or in a server room. By invoking the Subscription RESTful interface to subscribe the specific types of notifications, the receiver obtains the appropriate notifications through defined receiving channels when events occur. The receiving channels include SMS message, e-mail, REST callback, AMQP, MQTT, and so on. Currently in EdgeX Foundry, e-mail and REST callback channels are provided. When Alerts and Notifications receive notifications from any interface, the notifications are passed to the Notifications Handler internally. The Notifications Handler persists the received notifications first, and passes them to the Distribution Coordinator immediately when a given notification is either critical (severity = \u201cCRITICAL\u201d) or when it is normal (severity = \u201cNORMAL\u201d). When the Distribution Coordinator receives a notification, it first queries the subscription to acquire receivers who need to obtain this notification and their receiving channel information. According to the channel information, the Distribution Coordinator passes this notification to the corresponding channel senders. Then, the channel senders send out the notifications to the subscribed receivers. Data Model MongoDB is selected for the persistence of Alerts and Notifications, so the data model design is without foreign key and based on the paradigm of document structure. Data Dictionary Class Name Description Channel The object used to describe the Notification end point. Notification The object used to describe the message and sender content of a Notification. Transmission The object used for grouping of Notifications. High Level Interaction Diagrams This section shows the sequence diagrams for some of the more critical or complex events regarding Alerts and Notifications. Critical Notifications Sequence When receiving a critical notification (SEVERITY = \"CRITICAL\"), it persists first and triggers the distribution process immediately. After updating the notification status, Alerts and Notifications respond to the client to indicate the notification has been accepted. Normal Notifications Sequence When receiving a normal notification (SEVERITY = \"NORMAL\"), it persists first and responds to the client to indicate the notification has been accepted immediately. After a configurable duration, a scheduler triggers the distribution process in batch. Critical Resend Sequence When encountering any error during sending critical notification, an individual resend task is scheduled, and each transmission record persists. If the resend tasks keeps failing and the resend count exceeds the configurable limit, the escalation process is triggered. The escalated notification is sent to particular receivers of a special subscription (slug = \"ESCALATION\"). Resend Sequence For other non-critical notifications, the resend operation is triggered by a scheduler. Cleanup Sequence Cleanup service removes old notification and transmission records. Configuration Properties Please refer to the general Configuration documentation for configuration properties common across all services. Configuration specific to the Support-Notifications service is as follows. Changes made to any of these properties while the service is running will not be reflected until the service is restarted. Configuration Default Value Dependencies Following config apply to using the SMTP service Smtp Host smtp.gmail.com SMTP service host name Smtp Port 587 SMTP service port number Smtp EnableSelfSignedCert false Indicates whether a self-signed cert can be used for secure connectivity. Smtp Username username@mail.example.com A username for authentications with the Smtp server, if requied. Smtp Password mypassword SMTP service host access password Smtp Sender jdoe@gmail.com SMTP service sendor/username Smtp Subject EdgeX Notification SMTP alert message subject Configure Mail Server All the properties with prefix \"smtp\" are for mail server configuration. Configure the mail server appropriately to send Alerts and Notifications. The correct values depend on which mail server is used. Gmail Before using Gmail to send Alerts and Notifications, configure the sign-in security settings through one of the following two methods: Enable 2-Step Verification and use an App Password (Recommended). An App password is a 16-digit passcode that gives an app or device permission to access your Google Account. For more detail about this topic, please refer to this Google official document: https://support.google.com/accounts/answer/185833 . Allow less secure apps: If the 2-Step Verification is not enabled, you may need to allow less secure apps to access the Gmail account. Please see the instruction from this Google official document on this topic: https://support.google.com/accounts/answer/6010255 . Then, use the following settings for the mail server properties: Smtp Port=25 Smtp Host=smtp.gmail.com Smtp Sender= ${ Gmail account } Smtp Password= ${ Gmail password or App password } Yahoo Mail Similar to Gmail, configure the sign-in security settings for Yahoo through one of the following two methods: Enable 2-Step Verification and use an App Password (Recommended). Please see this Yahoo official document for more detail: https://help.yahoo.com/kb/SLN15241.html . Allow apps that use less secure sign in. Please see this Yahoo official document for more detail on this topic: https://help.yahoo.com/kb/SLN27791.html . Then, use the following settings for the mail server properties: Smtp Port=25 Smtp Host=smtp.mail.yahoo.com Smtp Sender= ${ Yahoo account } Smtp Password= ${ Yahoo password or App password }","title":"Alerts & Notifications"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#alerts-notifications","text":"","title":"Alerts &amp; Notifications"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#introduction","text":"When notification to another system or to a person, needs to occur to notify of something discovered on the node by another microservice, the Alerts and Notifications microservice delivers that information. Examples of Alerts and Notifications that other services could need to broadcast, include sensor data detected outside of certain parameters (usually detected by a Rules Engine service) or system or service malfunctions (usually detected by System Management services). Terminology Notifications are informative, whereas Alerts are typically of a more important, critical, or urgent nature, possibly requiring immediate action. The diagram shows the high-level architecture of Alerts and Notifications. On the left side, the APIs are provided for other microservices, on-box applications, and off-box applications to use, and the APIs could be in REST, AMQP, MQTT, or any standard application protocols. Currently in EdgeX Foundry, the RESTful interface is provided. On the right side, the notification receiver could be a person or an application system on Cloud or in a server room. By invoking the Subscription RESTful interface to subscribe the specific types of notifications, the receiver obtains the appropriate notifications through defined receiving channels when events occur. The receiving channels include SMS message, e-mail, REST callback, AMQP, MQTT, and so on. Currently in EdgeX Foundry, e-mail and REST callback channels are provided. When Alerts and Notifications receive notifications from any interface, the notifications are passed to the Notifications Handler internally. The Notifications Handler persists the received notifications first, and passes them to the Distribution Coordinator immediately when a given notification is either critical (severity = \u201cCRITICAL\u201d) or when it is normal (severity = \u201cNORMAL\u201d). When the Distribution Coordinator receives a notification, it first queries the subscription to acquire receivers who need to obtain this notification and their receiving channel information. According to the channel information, the Distribution Coordinator passes this notification to the corresponding channel senders. Then, the channel senders send out the notifications to the subscribed receivers.","title":"Introduction"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#data-model","text":"MongoDB is selected for the persistence of Alerts and Notifications, so the data model design is without foreign key and based on the paradigm of document structure.","title":"Data Model"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#data-dictionary","text":"Class Name Description Channel The object used to describe the Notification end point. Notification The object used to describe the message and sender content of a Notification. Transmission The object used for grouping of Notifications.","title":"Data Dictionary"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#high-level-interaction-diagrams","text":"This section shows the sequence diagrams for some of the more critical or complex events regarding Alerts and Notifications. Critical Notifications Sequence When receiving a critical notification (SEVERITY = \"CRITICAL\"), it persists first and triggers the distribution process immediately. After updating the notification status, Alerts and Notifications respond to the client to indicate the notification has been accepted. Normal Notifications Sequence When receiving a normal notification (SEVERITY = \"NORMAL\"), it persists first and responds to the client to indicate the notification has been accepted immediately. After a configurable duration, a scheduler triggers the distribution process in batch. Critical Resend Sequence When encountering any error during sending critical notification, an individual resend task is scheduled, and each transmission record persists. If the resend tasks keeps failing and the resend count exceeds the configurable limit, the escalation process is triggered. The escalated notification is sent to particular receivers of a special subscription (slug = \"ESCALATION\"). Resend Sequence For other non-critical notifications, the resend operation is triggered by a scheduler. Cleanup Sequence Cleanup service removes old notification and transmission records.","title":"High Level Interaction Diagrams"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#configuration-properties","text":"Please refer to the general Configuration documentation for configuration properties common across all services. Configuration specific to the Support-Notifications service is as follows. Changes made to any of these properties while the service is running will not be reflected until the service is restarted. Configuration Default Value Dependencies Following config apply to using the SMTP service Smtp Host smtp.gmail.com SMTP service host name Smtp Port 587 SMTP service port number Smtp EnableSelfSignedCert false Indicates whether a self-signed cert can be used for secure connectivity. Smtp Username username@mail.example.com A username for authentications with the Smtp server, if requied. Smtp Password mypassword SMTP service host access password Smtp Sender jdoe@gmail.com SMTP service sendor/username Smtp Subject EdgeX Notification SMTP alert message subject","title":"Configuration Properties"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#configure-mail-server","text":"All the properties with prefix \"smtp\" are for mail server configuration. Configure the mail server appropriately to send Alerts and Notifications. The correct values depend on which mail server is used.","title":"Configure Mail Server"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#gmail","text":"Before using Gmail to send Alerts and Notifications, configure the sign-in security settings through one of the following two methods: Enable 2-Step Verification and use an App Password (Recommended). An App password is a 16-digit passcode that gives an app or device permission to access your Google Account. For more detail about this topic, please refer to this Google official document: https://support.google.com/accounts/answer/185833 . Allow less secure apps: If the 2-Step Verification is not enabled, you may need to allow less secure apps to access the Gmail account. Please see the instruction from this Google official document on this topic: https://support.google.com/accounts/answer/6010255 . Then, use the following settings for the mail server properties: Smtp Port=25 Smtp Host=smtp.gmail.com Smtp Sender= ${ Gmail account } Smtp Password= ${ Gmail password or App password }","title":"Gmail"},{"location":"microservices/support/notifications/Ch-AlertsNotifications/#yahoo-mail","text":"Similar to Gmail, configure the sign-in security settings for Yahoo through one of the following two methods: Enable 2-Step Verification and use an App Password (Recommended). Please see this Yahoo official document for more detail: https://help.yahoo.com/kb/SLN15241.html . Allow apps that use less secure sign in. Please see this Yahoo official document for more detail on this topic: https://help.yahoo.com/kb/SLN27791.html . Then, use the following settings for the mail server properties: Smtp Port=25 Smtp Host=smtp.mail.yahoo.com Smtp Sender= ${ Yahoo account } Smtp Password= ${ Yahoo password or App password }","title":"Yahoo Mail"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/","text":"Rules Engine Deprecation Notice Please note that the Support Rules Engine service has been deprecated with the Geneva release (v1.2). With the Geneva release, EdgeX has formed a partnership with the EMQ X Kuiper open source project and offers the Kuiper Rules Engine as the reference implementation rules engine. The EdgeX Rules Engine was Java-based and wrapped the open source Drools engine. This was the last of the EdgeX Java services to be replaced. Starting with the Geneva release, by default, the EdgeX reference implementations (provided through the EdgeX Docker Compose files) will use Kuiper with a dedicated application service providing the data feed to the Kuiper engine. The Support Rules Engine is still available but users must find and uncomment the Support Rules Engine in the Docker Compose file. The Support Rules Engine will removed in a future release of EdgeX Foundry. Java Drools Rule Engine Implementation This rules engine service monitors incoming sensor or device data for readings within target ranges and triggers immediate device actuation. Therefore, the rules engine provides \"intelligence\" at, or near, the network edge for faster response times. This rules engine uses a Drools ( https://www.drools.org/ ) rules engine at its core. Drools is an open source rules engine provided by the JBoss community. This microservice is able to be replaced or augmented by many other edge-analytic capabilities provided by 3rd parties. Rules Engine as Export Service Client This rules engine is an automatic export service client. When the service initiates, it automatically calls on the Export Client Registration microservice to register itself as a client of all device and sensor readings coming out of Core Data. As an Export Service client, this rules engine receives all events and readings through the Export Distribution microservice. Based on data, this rules engine is instructed to monitor each event and reading received through the Export Distribution microservice, and the rules engine triggers any actuation to a device through the Core Command microservice (which subsequently communicates the request through Device Service to communicate with the actual device). Rules Engine Direct Connect to Core Data In more time sensitive use cases or environment where a lot of data is being generated by the connected \"things\", it may be appropriate to connect the Rules Engine micro service to the data coming directly out of Core Data. That is, to by pass the Export Services for purposes of rule-based command activation. The rules engine has been programmed for this option. By default, the rules engine micro service registers itself as a client of the export service. This automatic registration can be turned off, and the rules engine can be connected directly to the ZeroMQ published data out of Core Data. Note: as the ZeroMQ pipe out of Core Data is a publish-subscribe mechanism, it allows for multiple subscribers. When rules engine is connected as a subscriber, Core Data is actually publishing simultaneously to two clients or subscribers: Export Services and Rules Engine. In order to disconnect the Rules Engine from the Export Services (as a client) and connect it directly to Core Data, the following Rules Engine micro service configuration parameters (found in application.properties) must be changed: export.client=true # this is normally false by default and is the indication to the Rules Engine micro service to register itself with the Export Services export.zeromq.port=5563 # this port is set to 5566 when connecting to the ZeroMQ pipe out of Export Services. Rules Client and High Level Interaction Diagram The Rules Engine microservice comes with a RESTful service that enables new rules to be added and removed. The RESTful API enables new rules, defined in JSON, to be dynamically added to the rules engine (through the REST POST). The JSON data provided is translated into Drools Rules files (.drl files) by the microservice. Each rule must be associated to a unique name that is used to identify the rule and the Drool file that holds it. Rules can also be requested to be removed by name. Note: Due to issues within Drools, a rule that is removed is only emptied of contents. The name of the rule (and the file that represents it) are still in the system. Therefore, rule names cannot be reused until the Rules Engine microservice is stopped and the empty Drool files are physically deleted. {.align-center} Rules (Defined), and Data Model Rules are provided to the Rules Engine microservice directly through the Rules Engine REST API or indirectly using the client UI. A rule is defined in 4 parts: name, log entry, condition, and action. {.align-center} Name The name uniquely identifies the rule. Log Entry The log entry, or simply log, is the text to be sent to the log when the rule condition is met and the action is triggered. Condition The condition specifies which data (from the Event/Reading supplied through the Export Service) to monitor. Specifically, the condition element of a rule specifies the device ID or name of the device to monitor, and the value checks (or simply \"checks\") to perform on sensor values collected on that device. The device ID or name must match the device ID or name specified in the Event object that is sent by Core Data to the Export Service and then relayed to the Rules Engine Service (through 0MQ). A value check specifies a parameter of the sensor to monitor and test to apply to the parameter. The ValueCheck parameter must match one of the the Reading names associated to the Event provided by CoreData (and is also the name of a legal ValueDescriptor). For example, on a thermostat sensor, the Reading may be reporting the current temperature. Therefore, the Reading name would be \"temperature\" and in order for rule to test the data from this sensor reading, a value check for the same device must contain a parameter of \"temperature\" as well. The operand1, operation, and operand2 must specify an equation around the parameter that the Rules Engine uses to determine whether to trigger the action. For example, a ValueCheck that wishes to specify to the Rules Engine to check all temperature reading \"value\" for any temperature above 72 degrees would specify it as follows: parameter : temperature operand1 : value operation : > operand2 : 72 Because the data in an Event's Reading may be reported in string form, the ValueCheck operands can and should be specified using Java syntax which is negotiated before the evaluation to create the appropriate type data for comparison. If temperature readings were represented as strings in Core Data, then the same ValueCheck would be specified as follows: parameter : temperature operand1 : Integer . parseInt ( value ) operation : > operand2 : 72 Lastly, the action specified in a rule specifies which command to trigger on a device or sensor and which data or parameters to send to the device as part of that call. The actual call is made through the Core Command microservice in REST form. Therefore, the action must specify the following items: The device identifier (per Metadata) that is to be called on The Command identifier (per Metadata) to be executed against the device The data supplied as part of the Command call Thus the data to be provided as part of the call is JSON data to be supplied in the body of the Command POST call. An example of the action properties is as follows: device: 56325f7ee4b05eaae5a89ce1 (the identifier of a device or sensor in Meta Data) command: 56325f6de4b05eaae5a89cdc (a command ID associated to the device per Meta Data) body: {\"value\":\"3\"} (the JSON data supplied in the REST message body). When creating a Rule in JSON to be POST submitted through the Rules Engine client, the entire Rule would be represented as shown below: { \"name\" : \"motortoofastsignal\" , \"condition\" : { \"device\" : \"562114e9e4b0385849b96cd8\" , \"checks\" : [ { \"parameter\" : \"RPM\" , \"operand1\" : \"Integer.parseInt(value)\" , \"operation\" : \">\" , \"operand2\" : \"1200\" } ] }, \"action\" : { \"device\" : \"56325f7ee4b05eaae5a89ce1\" , \"command\" : \"56325f6de4b05eaae5a89cdc\" , \"body\" : \"{\\\"value\\\":\\\"3\\\"}\" }, \"log\" : \"Patlite warning triggered for engine speed too high\" } Rules Engine Configuration The Rules Engine microservice has several configuration properties that are specific to rules engine operations. Additional configuration, such as the microservice's server port, are standard among EdgeX microservices and won't be covered here. The critical properties in the rules engine microservice are located in application.properties. Note that the source code contains an application.properties file in the /src/main/resources folder that serves as the default for development environments (typically) versus the application.properties in the docker-files folder of the source that provides the standard default for the Dockerized version of the microservice. The examples shown below are those from the /src/main/resources defaults. Automatic Rules Engine as an Export Distro client export.client=true When the rules engine microservice comes up, in order to receive data (the sensor Events/Readings) from EdgeX, it automatically registers as an export data client through the export client micro service. If you do not want the rules engine to automatically receive that data from the export services (namely export distro), set export.client to false. In particular, as outlined above, you may wish the rules engine microservice to receive data directly from core data versus the export services and thus may wish export.client set to false. Rules Engine Export Distribution Registration export.client.registration.url=http://localhost:48071/api/v1export.client.registration.name=EdgeXRulesEngine #how long to wait to retry registration export.client.registration.retry.time=10000 #how many times to try registration before exiting export.client.registration.retry.attempts=100 If export.client is set to true to have the rules engine microservice be a client of the export services, then additional properties need to be specified to indicate the location of the export client registration microservice (this may vary per environment -- like in a development versus docker environment), and the name to use for the rules engine with the export client when registering the rules engine. Core Data's Zero MQ Connection information export.zeromq.port=5566 export.zeromq.host=tcp://localhost As already indicated above in the Rules Engine Direct Connect to Core Data section, if Rules Engine is to be connected directly to the data feed (ZeroMQ) coming from Core Data, additional properties must be provided to specify the port and address for subscribing to the Core Data feed. Again, these may differ per environment (for instance local development versus a Dockerized environment). Location and Name of the Drools Template #Drools drl resource path rules.default.path=edgex/rules rules.packagename=org.edgexfoundry.rules rules.fileextension=.drl rules.template.path=edgex/templates rules.template.name=rule-template.drl rules.template.encoding=UTF-8 The rules engine is using Drools under the covers. When creating new rules via the rules engine microservice APIs, the rules engine must have access to a base template (a Drool file with a .drl extension by default) for creating new rules. The template carries certain imports and EdgeX device command call structure that is used by the rules engine to monitor the incoming data and actuate devices/sensors via the Command microservice. The location of the template, name of the template file and other properties associated to the template must be specified in the configuration properties. Typically, only the location of the template file changes per environment. Data Dictionary Class Name Description Action The command that is executed when a Condition is met. Condition The object describing the device and its ValueCheck condition that embodies a Rule. Rule The object containing the Condition and Action that define the Rule. ValueCheck The mathematical expression evaluated for a Condition.","title":"Rules Engine"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#rules-engine","text":"Deprecation Notice Please note that the Support Rules Engine service has been deprecated with the Geneva release (v1.2). With the Geneva release, EdgeX has formed a partnership with the EMQ X Kuiper open source project and offers the Kuiper Rules Engine as the reference implementation rules engine. The EdgeX Rules Engine was Java-based and wrapped the open source Drools engine. This was the last of the EdgeX Java services to be replaced. Starting with the Geneva release, by default, the EdgeX reference implementations (provided through the EdgeX Docker Compose files) will use Kuiper with a dedicated application service providing the data feed to the Kuiper engine. The Support Rules Engine is still available but users must find and uncomment the Support Rules Engine in the Docker Compose file. The Support Rules Engine will removed in a future release of EdgeX Foundry.","title":"Rules Engine"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#java-drools-rule-engine-implementation","text":"This rules engine service monitors incoming sensor or device data for readings within target ranges and triggers immediate device actuation. Therefore, the rules engine provides \"intelligence\" at, or near, the network edge for faster response times. This rules engine uses a Drools ( https://www.drools.org/ ) rules engine at its core. Drools is an open source rules engine provided by the JBoss community. This microservice is able to be replaced or augmented by many other edge-analytic capabilities provided by 3rd parties.","title":"Java Drools Rule Engine Implementation"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#rules-engine-as-export-service-client","text":"This rules engine is an automatic export service client. When the service initiates, it automatically calls on the Export Client Registration microservice to register itself as a client of all device and sensor readings coming out of Core Data. As an Export Service client, this rules engine receives all events and readings through the Export Distribution microservice. Based on data, this rules engine is instructed to monitor each event and reading received through the Export Distribution microservice, and the rules engine triggers any actuation to a device through the Core Command microservice (which subsequently communicates the request through Device Service to communicate with the actual device).","title":"Rules Engine as Export Service Client"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#rules-engine-direct-connect-to-core-data","text":"In more time sensitive use cases or environment where a lot of data is being generated by the connected \"things\", it may be appropriate to connect the Rules Engine micro service to the data coming directly out of Core Data. That is, to by pass the Export Services for purposes of rule-based command activation. The rules engine has been programmed for this option. By default, the rules engine micro service registers itself as a client of the export service. This automatic registration can be turned off, and the rules engine can be connected directly to the ZeroMQ published data out of Core Data. Note: as the ZeroMQ pipe out of Core Data is a publish-subscribe mechanism, it allows for multiple subscribers. When rules engine is connected as a subscriber, Core Data is actually publishing simultaneously to two clients or subscribers: Export Services and Rules Engine. In order to disconnect the Rules Engine from the Export Services (as a client) and connect it directly to Core Data, the following Rules Engine micro service configuration parameters (found in application.properties) must be changed: export.client=true # this is normally false by default and is the indication to the Rules Engine micro service to register itself with the Export Services export.zeromq.port=5563 # this port is set to 5566 when connecting to the ZeroMQ pipe out of Export Services.","title":"Rules Engine Direct Connect to Core Data"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#rules-client-and-high-level-interaction-diagram","text":"The Rules Engine microservice comes with a RESTful service that enables new rules to be added and removed. The RESTful API enables new rules, defined in JSON, to be dynamically added to the rules engine (through the REST POST). The JSON data provided is translated into Drools Rules files (.drl files) by the microservice. Each rule must be associated to a unique name that is used to identify the rule and the Drool file that holds it. Rules can also be requested to be removed by name. Note: Due to issues within Drools, a rule that is removed is only emptied of contents. The name of the rule (and the file that represents it) are still in the system. Therefore, rule names cannot be reused until the Rules Engine microservice is stopped and the empty Drool files are physically deleted. {.align-center}","title":"Rules Client and High Level Interaction Diagram"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#rules-defined-and-data-model","text":"Rules are provided to the Rules Engine microservice directly through the Rules Engine REST API or indirectly using the client UI. A rule is defined in 4 parts: name, log entry, condition, and action. {.align-center} Name The name uniquely identifies the rule. Log Entry The log entry, or simply log, is the text to be sent to the log when the rule condition is met and the action is triggered. Condition The condition specifies which data (from the Event/Reading supplied through the Export Service) to monitor. Specifically, the condition element of a rule specifies the device ID or name of the device to monitor, and the value checks (or simply \"checks\") to perform on sensor values collected on that device. The device ID or name must match the device ID or name specified in the Event object that is sent by Core Data to the Export Service and then relayed to the Rules Engine Service (through 0MQ). A value check specifies a parameter of the sensor to monitor and test to apply to the parameter. The ValueCheck parameter must match one of the the Reading names associated to the Event provided by CoreData (and is also the name of a legal ValueDescriptor). For example, on a thermostat sensor, the Reading may be reporting the current temperature. Therefore, the Reading name would be \"temperature\" and in order for rule to test the data from this sensor reading, a value check for the same device must contain a parameter of \"temperature\" as well. The operand1, operation, and operand2 must specify an equation around the parameter that the Rules Engine uses to determine whether to trigger the action. For example, a ValueCheck that wishes to specify to the Rules Engine to check all temperature reading \"value\" for any temperature above 72 degrees would specify it as follows: parameter : temperature operand1 : value operation : > operand2 : 72 Because the data in an Event's Reading may be reported in string form, the ValueCheck operands can and should be specified using Java syntax which is negotiated before the evaluation to create the appropriate type data for comparison. If temperature readings were represented as strings in Core Data, then the same ValueCheck would be specified as follows: parameter : temperature operand1 : Integer . parseInt ( value ) operation : > operand2 : 72 Lastly, the action specified in a rule specifies which command to trigger on a device or sensor and which data or parameters to send to the device as part of that call. The actual call is made through the Core Command microservice in REST form. Therefore, the action must specify the following items: The device identifier (per Metadata) that is to be called on The Command identifier (per Metadata) to be executed against the device The data supplied as part of the Command call Thus the data to be provided as part of the call is JSON data to be supplied in the body of the Command POST call. An example of the action properties is as follows: device: 56325f7ee4b05eaae5a89ce1 (the identifier of a device or sensor in Meta Data) command: 56325f6de4b05eaae5a89cdc (a command ID associated to the device per Meta Data) body: {\"value\":\"3\"} (the JSON data supplied in the REST message body). When creating a Rule in JSON to be POST submitted through the Rules Engine client, the entire Rule would be represented as shown below: { \"name\" : \"motortoofastsignal\" , \"condition\" : { \"device\" : \"562114e9e4b0385849b96cd8\" , \"checks\" : [ { \"parameter\" : \"RPM\" , \"operand1\" : \"Integer.parseInt(value)\" , \"operation\" : \">\" , \"operand2\" : \"1200\" } ] }, \"action\" : { \"device\" : \"56325f7ee4b05eaae5a89ce1\" , \"command\" : \"56325f6de4b05eaae5a89cdc\" , \"body\" : \"{\\\"value\\\":\\\"3\\\"}\" }, \"log\" : \"Patlite warning triggered for engine speed too high\" }","title":"Rules (Defined), and Data Model"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#rules-engine-configuration","text":"The Rules Engine microservice has several configuration properties that are specific to rules engine operations. Additional configuration, such as the microservice's server port, are standard among EdgeX microservices and won't be covered here. The critical properties in the rules engine microservice are located in application.properties. Note that the source code contains an application.properties file in the /src/main/resources folder that serves as the default for development environments (typically) versus the application.properties in the docker-files folder of the source that provides the standard default for the Dockerized version of the microservice. The examples shown below are those from the /src/main/resources defaults. Automatic Rules Engine as an Export Distro client export.client=true When the rules engine microservice comes up, in order to receive data (the sensor Events/Readings) from EdgeX, it automatically registers as an export data client through the export client micro service. If you do not want the rules engine to automatically receive that data from the export services (namely export distro), set export.client to false. In particular, as outlined above, you may wish the rules engine microservice to receive data directly from core data versus the export services and thus may wish export.client set to false. Rules Engine Export Distribution Registration export.client.registration.url=http://localhost:48071/api/v1export.client.registration.name=EdgeXRulesEngine #how long to wait to retry registration export.client.registration.retry.time=10000 #how many times to try registration before exiting export.client.registration.retry.attempts=100 If export.client is set to true to have the rules engine microservice be a client of the export services, then additional properties need to be specified to indicate the location of the export client registration microservice (this may vary per environment -- like in a development versus docker environment), and the name to use for the rules engine with the export client when registering the rules engine. Core Data's Zero MQ Connection information export.zeromq.port=5566 export.zeromq.host=tcp://localhost As already indicated above in the Rules Engine Direct Connect to Core Data section, if Rules Engine is to be connected directly to the data feed (ZeroMQ) coming from Core Data, additional properties must be provided to specify the port and address for subscribing to the Core Data feed. Again, these may differ per environment (for instance local development versus a Dockerized environment). Location and Name of the Drools Template #Drools drl resource path rules.default.path=edgex/rules rules.packagename=org.edgexfoundry.rules rules.fileextension=.drl rules.template.path=edgex/templates rules.template.name=rule-template.drl rules.template.encoding=UTF-8 The rules engine is using Drools under the covers. When creating new rules via the rules engine microservice APIs, the rules engine must have access to a base template (a Drool file with a .drl extension by default) for creating new rules. The template carries certain imports and EdgeX device command call structure that is used by the rules engine to monitor the incoming data and actuate devices/sensors via the Command microservice. The location of the template, name of the template file and other properties associated to the template must be specified in the configuration properties. Typically, only the location of the template file changes per environment.","title":"Rules Engine Configuration"},{"location":"microservices/support/rulesengine/Ch-RulesEngine/#data-dictionary","text":"Class Name Description Action The command that is executed when a Condition is met. Condition The object describing the device and its ValueCheck condition that embodies a Rule. Rule The object containing the Condition and Action that define the Rule. ValueCheck The mathematical expression evaluated for a Condition.","title":"Data Dictionary"},{"location":"microservices/support/scheduler/Ch-Scheduling/","text":"Scheduling Introduction The Support-scheduler service executes operations on a configured interval or schedule. Two default scheduled operations are: 1 - Clean up Core-data events/readings that have been moved (\"pushed\") to external applications/systems like a cloud provider. This is the \"Scrubbed Pushed\" operation. Scheduler parameters around this operation determine how often and where to call into core data to invoke this clean up of unneeded data operation. 2 - Clean up of Core-data events/readings that have been persisted for an extended period. In order to prevent the edge node from running out of space, these old events/readings are removed. This is the \"ScrubAged\" operation. Scheduler parameters around this operation determine how often and where to call into Core-data to invoke this operation to expunge of old data. The removal of both exported records and stale records occurs on a configurable schedule. By default, both of the default actions above are invoked once a day at midnight. Support-scheduler uses a data store to persist the Interval(s) and IntervalAction(s). Persistence is accomplished the Scheduler DB located in your current configured database for EdgeX. Data Dictionary Class Name Description Interval An object defining a specific \"period\" in time. IntervalAction The action taken by a Service when the Interval occurs. Configuration Properties The following are extra configuration parameters specific to the Support-Scheduler service. Please refer to the general Configuration documentation for configuration properties common across all services. Configuration Default Value Dependencies Intervals govern the timing of operations in the support-scheduler service. By default, only one interval is created to run a job at midnight. However you could follow the example to add as many as desired. Intervals Midnight Name midnight The name of the given interval. Intervals Midnight Start 20180101T000000 The start time of the given interval in ISO 8601 format. Intervals Midnight Frequency 24h Periodicity of the interval. IntervalActions govern the actions taken by the scheduler when a tick triggers an interval. IntervalActions ScrubPushed Name scrub-pushed-events The name of the given action. IntervalActions ScrubPushed Host localhost The host targeted by the action when it activates. IntervalActions ScrubPushed Port 48080 The port on the targeted host IntervalActions ScrubPushed Protocol http The protocol used when interacting with the targeted host. IntervalActions ScrubPushed Method DELETE Assuming a RESTful operation, the HTTP method to be invoked by the action. IntervalActions ScrubPushed Target core-data The name of the targeted application IntervalActions ScrubPushed Path /api/v1/event/scrub In the case of a RESTful operation, the path to be invoked in combination with the Method. IntervalActions ScrubPushed Interval midnight The interval to which the action is associated. When the interval is activated, all associated actions will be invoked. IntervalActions ScrubAged Name scrub-aged-events The name of the given action. IntervalActions ScrubAged Host localhost The host targeted by the action when it activates. IntervalActions ScrubAged Port 48080 The port on the targeted host IntervalActions ScrubAged Protocol http The protocol used when interacting with the targeted host. IntervalActions ScrubAged Method DELETE Assuming a RESTful operation, the HTTP method to be invoked by the action. IntervalActions ScrubAged Target core-data The name of the targeted application IntervalActions ScrubAged Path /api/v1/event/removeold/age/604800000 In the case of a RESTful operation, the path to be invoked in combination with the Method. IntervalActions ScrubAged Interval midnight The interval to which the action is associated. When the interval is activated, all associated actions will be invoked.","title":"Scheduling"},{"location":"microservices/support/scheduler/Ch-Scheduling/#scheduling","text":"","title":"Scheduling"},{"location":"microservices/support/scheduler/Ch-Scheduling/#introduction","text":"The Support-scheduler service executes operations on a configured interval or schedule. Two default scheduled operations are: 1 - Clean up Core-data events/readings that have been moved (\"pushed\") to external applications/systems like a cloud provider. This is the \"Scrubbed Pushed\" operation. Scheduler parameters around this operation determine how often and where to call into core data to invoke this clean up of unneeded data operation. 2 - Clean up of Core-data events/readings that have been persisted for an extended period. In order to prevent the edge node from running out of space, these old events/readings are removed. This is the \"ScrubAged\" operation. Scheduler parameters around this operation determine how often and where to call into Core-data to invoke this operation to expunge of old data. The removal of both exported records and stale records occurs on a configurable schedule. By default, both of the default actions above are invoked once a day at midnight. Support-scheduler uses a data store to persist the Interval(s) and IntervalAction(s). Persistence is accomplished the Scheduler DB located in your current configured database for EdgeX.","title":"Introduction"},{"location":"microservices/support/scheduler/Ch-Scheduling/#data-dictionary","text":"Class Name Description Interval An object defining a specific \"period\" in time. IntervalAction The action taken by a Service when the Interval occurs.","title":"Data Dictionary"},{"location":"microservices/support/scheduler/Ch-Scheduling/#configuration-properties","text":"The following are extra configuration parameters specific to the Support-Scheduler service. Please refer to the general Configuration documentation for configuration properties common across all services. Configuration Default Value Dependencies Intervals govern the timing of operations in the support-scheduler service. By default, only one interval is created to run a job at midnight. However you could follow the example to add as many as desired. Intervals Midnight Name midnight The name of the given interval. Intervals Midnight Start 20180101T000000 The start time of the given interval in ISO 8601 format. Intervals Midnight Frequency 24h Periodicity of the interval. IntervalActions govern the actions taken by the scheduler when a tick triggers an interval. IntervalActions ScrubPushed Name scrub-pushed-events The name of the given action. IntervalActions ScrubPushed Host localhost The host targeted by the action when it activates. IntervalActions ScrubPushed Port 48080 The port on the targeted host IntervalActions ScrubPushed Protocol http The protocol used when interacting with the targeted host. IntervalActions ScrubPushed Method DELETE Assuming a RESTful operation, the HTTP method to be invoked by the action. IntervalActions ScrubPushed Target core-data The name of the targeted application IntervalActions ScrubPushed Path /api/v1/event/scrub In the case of a RESTful operation, the path to be invoked in combination with the Method. IntervalActions ScrubPushed Interval midnight The interval to which the action is associated. When the interval is activated, all associated actions will be invoked. IntervalActions ScrubAged Name scrub-aged-events The name of the given action. IntervalActions ScrubAged Host localhost The host targeted by the action when it activates. IntervalActions ScrubAged Port 48080 The port on the targeted host IntervalActions ScrubAged Protocol http The protocol used when interacting with the targeted host. IntervalActions ScrubAged Method DELETE Assuming a RESTful operation, the HTTP method to be invoked by the action. IntervalActions ScrubAged Target core-data The name of the targeted application IntervalActions ScrubAged Path /api/v1/event/removeold/age/604800000 In the case of a RESTful operation, the path to be invoked in combination with the Method. IntervalActions ScrubAged Interval midnight The interval to which the action is associated. When the interval is activated, all associated actions will be invoked.","title":"Configuration Properties"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/","text":"System Management Agent (SMA) Introduction While the SMA serves several purposes, it is to be considered, first and foremost, as the single connection point of management control for an EdgeX instance. As such, the API calls related to system management are defined so as to interface with the SMA. Examples of API Calls To get an appreciation for some SMA API calls in action, it will be instructive to look at what responses the SMA provides to the caller, for the respective calls (Notice, too, the error messages returned by the SMA, should it encounter a problem). Thus, consider the following calls that the SMA handles: Metrics of a service Configuration of a service Start a service Stop a service Restart a service Health check on a service Let's look at the preceding calls (aka requests), one-by-one, in the following sections. Metrics of a service Example request: /api/v1/metrics/edgex-core-command,edgex-core-data Corresponding response, in JSON format: { \"Metrics\" :{ \"edgex-core-command\" :{ \"CpuBusyAvg\" : 2.224995150836366 , \"Memory\" :{ \"Alloc\" : 1403648 , \"Frees\" : 1504 , \"LiveObjects\" : 18280 , \"Mallocs\" : 19784 , \"Sys\" : 71891192 , \"TotalAlloc\" : 1403648 } }, \"edgex-core-data\" :{ \"CpuBusyAvg\" : 2.854720153816541 , \"Memory\" :{ \"Alloc\" : 929080 , \"Frees\" : 1453 , \"LiveObjects\" : 7700 , \"Mallocs\" : 9153 , \"Sys\" : 70451200 , \"TotalAlloc\" : 929080 } } } } Configuration of a service Example request: /api/v1/config/device-simple,edgex-core-data Corresponding response, in JSON format: { \"Configuration\" : { \"device-simple\" : \"device-simple service is not registered. Might not have started... \" , \"edgex-core-data\" : { \"Clients\" : { \"Logging\" : { \"Host\" : \"localhost\" , \"Port\" : 48061 , \"Protocol\" : \"http\" }, \"Metadata\" : { \"Host\" : \"localhost\" , \"Port\" : 48081 , \"Protocol\" : \"http\" } }, \"Databases\" : { \"Primary\" : { \"Host\" : \"localhost\" , \"Name\" : \"coredata\" , \"Password\" : \"\" , \"Port\" : 27017 , \"Timeout\" : 5000 , \"Type\" : \"mongodb\" , \"Username\" : \"\" } }, \"Logging\" : { \"EnableRemote\" : false , \"File\" : \"./logs/edgex-core-data.log\" }, \"MessageQueue\" : { \"Host\" : \"*\" , \"Port\" : 5563 , \"Protocol\" : \"tcp\" , \"Type\" : \"zero\" }, \"Registry\" : { \"Host\" : \"localhost\" , \"Port\" : 8500 , \"Type\" : \"consul\" }, \"Service\" : { \"BootTimeout\" : 30000 , \"CheckInterval\" : \"10s\" , \"ClientMonitor\" : 15000 , \"Host\" : \"localhost\" , \"Port\" : 48080 , \"Protocol\" : \"http\" , \"MaxResultCount\" : 50000 , \"StartupMsg\" : \"This is the Core Data Microservice\" , \"Timeout\" : 5000 }, \"Writable\" : { \"DeviceUpdateLastConnected\" : false , \"LogLevel\" : \"INFO\" , \"MetaDataCheck\" : false , \"PersistData\" : true , \"ServiceUpdateLastConnected\" : false , \"ValidateCheck\" : false } } } } Start a service Example request: /api/v1/operation Example (POST) body accompanying the \"start\" request: { \"action\" : \"start\" , \"services\" :[ \"edgex-core-data\" , ], \"params\" :[ \"graceful\" ] } Corresponding response, in JSON format, on success: \"Done. Started the requested services.\" Corresponding response, in JSON format, on failure: \"HTTP 500 - Internal Server Error\" Stop a service Example request: /api/v1/operation Example (POST) body accompanying the \"stop\" request: { \"action\" : \"stop\" , \"services\" :[ \"edgex-support-notifications\" ], \"params\" :[ \"graceful\" ] } Corresponding response, in JSON format, on success: \"Done. Stopped the requested service.\" Corresponding response, in JSON format, on failure: \"HTTP 500 - Internal Server Error\" Restart a service Example request: /api/v1/operation Example (POST) body accompanying the \"restart\" request: { \"action\" : \"restart\" , \"services\" :[ \"edgex-support-notifications\" , \"edgex-core-data\" , ], \"params\" :[ \"graceful\" ] } Corresponding response, in JSON format, on success: \"Done. Restarted the requested services.\" Corresponding response, in JSON format, on failure: \"HTTP 500 - Internal Server Error\" Health check on a service Example request: /api/v1/health/device-simple,edgex-core-data,support-notifications Corresponding response, in JSON format: { \"device-simple\" : \"device-simple service is not registered. Might not have started... \" , \"edgex-core-data\" : true , \"support-notifications\" : true }","title":"System Management Agent (SMA)"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#system-management-agent-sma","text":"","title":"System Management Agent (SMA)"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#introduction","text":"While the SMA serves several purposes, it is to be considered, first and foremost, as the single connection point of management control for an EdgeX instance. As such, the API calls related to system management are defined so as to interface with the SMA.","title":"Introduction"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#examples-of-api-calls","text":"To get an appreciation for some SMA API calls in action, it will be instructive to look at what responses the SMA provides to the caller, for the respective calls (Notice, too, the error messages returned by the SMA, should it encounter a problem). Thus, consider the following calls that the SMA handles: Metrics of a service Configuration of a service Start a service Stop a service Restart a service Health check on a service Let's look at the preceding calls (aka requests), one-by-one, in the following sections.","title":"Examples of API Calls"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#metrics-of-a-service","text":"Example request: /api/v1/metrics/edgex-core-command,edgex-core-data Corresponding response, in JSON format: { \"Metrics\" :{ \"edgex-core-command\" :{ \"CpuBusyAvg\" : 2.224995150836366 , \"Memory\" :{ \"Alloc\" : 1403648 , \"Frees\" : 1504 , \"LiveObjects\" : 18280 , \"Mallocs\" : 19784 , \"Sys\" : 71891192 , \"TotalAlloc\" : 1403648 } }, \"edgex-core-data\" :{ \"CpuBusyAvg\" : 2.854720153816541 , \"Memory\" :{ \"Alloc\" : 929080 , \"Frees\" : 1453 , \"LiveObjects\" : 7700 , \"Mallocs\" : 9153 , \"Sys\" : 70451200 , \"TotalAlloc\" : 929080 } } } }","title":"Metrics of a service"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#configuration-of-a-service","text":"Example request: /api/v1/config/device-simple,edgex-core-data Corresponding response, in JSON format: { \"Configuration\" : { \"device-simple\" : \"device-simple service is not registered. Might not have started... \" , \"edgex-core-data\" : { \"Clients\" : { \"Logging\" : { \"Host\" : \"localhost\" , \"Port\" : 48061 , \"Protocol\" : \"http\" }, \"Metadata\" : { \"Host\" : \"localhost\" , \"Port\" : 48081 , \"Protocol\" : \"http\" } }, \"Databases\" : { \"Primary\" : { \"Host\" : \"localhost\" , \"Name\" : \"coredata\" , \"Password\" : \"\" , \"Port\" : 27017 , \"Timeout\" : 5000 , \"Type\" : \"mongodb\" , \"Username\" : \"\" } }, \"Logging\" : { \"EnableRemote\" : false , \"File\" : \"./logs/edgex-core-data.log\" }, \"MessageQueue\" : { \"Host\" : \"*\" , \"Port\" : 5563 , \"Protocol\" : \"tcp\" , \"Type\" : \"zero\" }, \"Registry\" : { \"Host\" : \"localhost\" , \"Port\" : 8500 , \"Type\" : \"consul\" }, \"Service\" : { \"BootTimeout\" : 30000 , \"CheckInterval\" : \"10s\" , \"ClientMonitor\" : 15000 , \"Host\" : \"localhost\" , \"Port\" : 48080 , \"Protocol\" : \"http\" , \"MaxResultCount\" : 50000 , \"StartupMsg\" : \"This is the Core Data Microservice\" , \"Timeout\" : 5000 }, \"Writable\" : { \"DeviceUpdateLastConnected\" : false , \"LogLevel\" : \"INFO\" , \"MetaDataCheck\" : false , \"PersistData\" : true , \"ServiceUpdateLastConnected\" : false , \"ValidateCheck\" : false } } } }","title":"Configuration of a service"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#start-a-service","text":"Example request: /api/v1/operation Example (POST) body accompanying the \"start\" request: { \"action\" : \"start\" , \"services\" :[ \"edgex-core-data\" , ], \"params\" :[ \"graceful\" ] } Corresponding response, in JSON format, on success: \"Done. Started the requested services.\" Corresponding response, in JSON format, on failure: \"HTTP 500 - Internal Server Error\"","title":"Start a service"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#stop-a-service","text":"Example request: /api/v1/operation Example (POST) body accompanying the \"stop\" request: { \"action\" : \"stop\" , \"services\" :[ \"edgex-support-notifications\" ], \"params\" :[ \"graceful\" ] } Corresponding response, in JSON format, on success: \"Done. Stopped the requested service.\" Corresponding response, in JSON format, on failure: \"HTTP 500 - Internal Server Error\"","title":"Stop a service"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#restart-a-service","text":"Example request: /api/v1/operation Example (POST) body accompanying the \"restart\" request: { \"action\" : \"restart\" , \"services\" :[ \"edgex-support-notifications\" , \"edgex-core-data\" , ], \"params\" :[ \"graceful\" ] } Corresponding response, in JSON format, on success: \"Done. Restarted the requested services.\" Corresponding response, in JSON format, on failure: \"HTTP 500 - Internal Server Error\"","title":"Restart a service"},{"location":"microservices/system-management/agent/Ch_SysMgmtAgent/#health-check-on-a-service","text":"Example request: /api/v1/health/device-simple,edgex-core-data,support-notifications Corresponding response, in JSON format: { \"device-simple\" : \"device-simple service is not registered. Might not have started... \" , \"edgex-core-data\" : true , \"support-notifications\" : true }","title":"Health check on a service"},{"location":"walk-through/Ch-Walkthrough/","text":"EdgeX Demonstration API Walk Through In order to better appreciate the EdgeX Foundry micro services (what they do and how they work), how they inter-operate with each other, and some of the more important API calls that each micro service has to offer, this demonstration API walk through shows how a device service and device are established in EdgeX, how data is sent flowing through the various services, and how data is then shipped out of EdgeX to the cloud or enterprise system. Through this demonstration, you will play the part of various EdgeX micro services by manually making REST calls in a way that mimics EdgeX system behavior. After exploring this demonstration, and hopefully exercising the APIs yourself, you should have a much better understanding of how EdgeX Foundry works.","title":"EdgeX Demonstration API Walk Through"},{"location":"walk-through/Ch-Walkthrough/#edgex-demonstration-api-walk-through","text":"In order to better appreciate the EdgeX Foundry micro services (what they do and how they work), how they inter-operate with each other, and some of the more important API calls that each micro service has to offer, this demonstration API walk through shows how a device service and device are established in EdgeX, how data is sent flowing through the various services, and how data is then shipped out of EdgeX to the cloud or enterprise system. Through this demonstration, you will play the part of various EdgeX micro services by manually making REST calls in a way that mimics EdgeX system behavior. After exploring this demonstration, and hopefully exercising the APIs yourself, you should have a much better understanding of how EdgeX Foundry works.","title":"EdgeX Demonstration API Walk Through"},{"location":"walk-through/Ch-WalkthroughCommands/","text":"Calling commands Recall that the Device Profile (the camera monitor profile) included a number of Commands to get and put information from any Device of that type. Also recall that the Device (the countcamera1) was associated to the Device Profile (the camera monitor profile) when the Device was provisioned. List the Commands See APIs Core Services Command Now with all the setup complete, you can ask the Core Command micro service for the list of Commands associated to the Device (the countcamera1). GET to http://localhost:48082/api/v1/device/name/countcamera1 Note all of the URLs returned as part of this response! These are the URLs that clients (internal or external to EdgeX) can call to trigger the various get and put offerings on the Device. Check the Value Descriptors See APIs Core Services Core Data See that the Value Descriptors are in Core Data. There should be a total of 5 Value Descriptors in Core Data. Note that Value Descriptors are stored in Core Data, yet referenced in Metadata. This is because as data coming from a Device is sent to Core Data, Core Data may need to validate the incoming values against the associated Value Descriptor parameters (like min, max, etc.) but without having to make a trip to Core Metadata to do that validation. Getting data into Core Data is a key function of EdgeX and must be accomplished as quickly as possible (without having to make additional REST requests). GET to http://localhost:48080/api/v1/valuedescriptor While we're at it, check that no data has yet been shipped to Core Data. Since the Device Service and Device are in this demonstration wholly manually driven by you, no sensor data should yet have been collected. You can test this theory by asking for the count of Events in Core Data. GET to http://localhost:48080/api/v1/event/count Execute a Command While there is no real Device or Device Service in this walk through, EdgeX doesn't know that. Therefore, with all the configuration and setup you have performed, you can ask EdgeX to set the scan depth or set the snapshot duration to the camera, and EdgeX will dutifully try to perform the task. Of course, since no Device Service or Device exists, as expected EdgeX will ultimately responds with an error. However, through the log files, you can see a Command made of the Core Command micro service, attempts to call on the appropriate Command of the fictitious Device Service that manages our fictitious camera. For example sake, let's launch a Command to set the scan depth of countcamera1 (the name of the single human/dog counting camera Device in EdgeX right now). The first task to launch a request to set the scan depth is to get the URL for the Command to \"PUT\" or set a new scan depth on the Device. As seen above request a list of the Commands by the Device name with the following API on Core Command GET to http://localhost:48082/api/v1/device/name/countcamera1 Now locate and copy the URL for the PUT Depth Command. Because of the IDs used, this will be different on each system so a generic API call will not suffice here. Below is a picture containing a slice of the JSON returned by the GET request above and desired PUT Command URL highlighted - yours will vary based on IDs. Copy this URL into your REST client tool of choice and make a PUT to that URL on Core Command with the new depth as the parameter with that request. PUT to http : // localhost : 48082 / api / v1 / device /< system specific device id >/ command /< system specific command id > BODY : { \"depth\" : \"9\" } Again, because no Device Service (or Device) actually exists, Core Command will respond with an HTTP 502 Bad Gateway error. However, checking the logging output will prove that the Core Command micro service did receive the request and attempted to call on the non-existent Device Service to issue the actuating command. docker logs edgex - core - command INFO : 2019 / 02 / 15 19 : 32 : 13 Issuing GET command to : http : // 172 . 17 . 0 . 1 : 49977 / api / v1 / devices / 5 c6711419f8fc200010f4ada / scandepth ERROR : 2019 / 02 / 15 19 : 32 : 13 Get http : // 172 . 17 . 0 . 1 : 49977 / api / v1 / devices / 5 c6711419f8fc200010f4ada / scandepth : dial tcp 172 . 17 . 0 . 1 : 49977 : getsockopt : connection refused","title":"Calling commands"},{"location":"walk-through/Ch-WalkthroughCommands/#calling-commands","text":"Recall that the Device Profile (the camera monitor profile) included a number of Commands to get and put information from any Device of that type. Also recall that the Device (the countcamera1) was associated to the Device Profile (the camera monitor profile) when the Device was provisioned.","title":"Calling commands"},{"location":"walk-through/Ch-WalkthroughCommands/#list-the-commands","text":"See APIs Core Services Command Now with all the setup complete, you can ask the Core Command micro service for the list of Commands associated to the Device (the countcamera1). GET to http://localhost:48082/api/v1/device/name/countcamera1 Note all of the URLs returned as part of this response! These are the URLs that clients (internal or external to EdgeX) can call to trigger the various get and put offerings on the Device.","title":"List the Commands"},{"location":"walk-through/Ch-WalkthroughCommands/#check-the-value-descriptors","text":"See APIs Core Services Core Data See that the Value Descriptors are in Core Data. There should be a total of 5 Value Descriptors in Core Data. Note that Value Descriptors are stored in Core Data, yet referenced in Metadata. This is because as data coming from a Device is sent to Core Data, Core Data may need to validate the incoming values against the associated Value Descriptor parameters (like min, max, etc.) but without having to make a trip to Core Metadata to do that validation. Getting data into Core Data is a key function of EdgeX and must be accomplished as quickly as possible (without having to make additional REST requests). GET to http://localhost:48080/api/v1/valuedescriptor While we're at it, check that no data has yet been shipped to Core Data. Since the Device Service and Device are in this demonstration wholly manually driven by you, no sensor data should yet have been collected. You can test this theory by asking for the count of Events in Core Data. GET to http://localhost:48080/api/v1/event/count","title":"Check the Value Descriptors"},{"location":"walk-through/Ch-WalkthroughCommands/#execute-a-command","text":"While there is no real Device or Device Service in this walk through, EdgeX doesn't know that. Therefore, with all the configuration and setup you have performed, you can ask EdgeX to set the scan depth or set the snapshot duration to the camera, and EdgeX will dutifully try to perform the task. Of course, since no Device Service or Device exists, as expected EdgeX will ultimately responds with an error. However, through the log files, you can see a Command made of the Core Command micro service, attempts to call on the appropriate Command of the fictitious Device Service that manages our fictitious camera. For example sake, let's launch a Command to set the scan depth of countcamera1 (the name of the single human/dog counting camera Device in EdgeX right now). The first task to launch a request to set the scan depth is to get the URL for the Command to \"PUT\" or set a new scan depth on the Device. As seen above request a list of the Commands by the Device name with the following API on Core Command GET to http://localhost:48082/api/v1/device/name/countcamera1 Now locate and copy the URL for the PUT Depth Command. Because of the IDs used, this will be different on each system so a generic API call will not suffice here. Below is a picture containing a slice of the JSON returned by the GET request above and desired PUT Command URL highlighted - yours will vary based on IDs. Copy this URL into your REST client tool of choice and make a PUT to that URL on Core Command with the new depth as the parameter with that request. PUT to http : // localhost : 48082 / api / v1 / device /< system specific device id >/ command /< system specific command id > BODY : { \"depth\" : \"9\" } Again, because no Device Service (or Device) actually exists, Core Command will respond with an HTTP 502 Bad Gateway error. However, checking the logging output will prove that the Core Command micro service did receive the request and attempted to call on the non-existent Device Service to issue the actuating command. docker logs edgex - core - command INFO : 2019 / 02 / 15 19 : 32 : 13 Issuing GET command to : http : // 172 . 17 . 0 . 1 : 49977 / api / v1 / devices / 5 c6711419f8fc200010f4ada / scandepth ERROR : 2019 / 02 / 15 19 : 32 : 13 Get http : // 172 . 17 . 0 . 1 : 49977 / api / v1 / devices / 5 c6711419f8fc200010f4ada / scandepth : dial tcp 172 . 17 . 0 . 1 : 49977 : getsockopt : connection refused","title":"Execute a Command"},{"location":"walk-through/Ch-WalkthroughData/","text":"Defining your data When a new Device Service is first started in EdgeX, there are many many tasks to perform - all in preparation for the Device Service to manage one or more Devices, which are yet unknown to EdgeX. In general, the Device Service tasks when it first starts can be categorized into: Establish the reference information around the Device Service and Device. Make the Device Service itself known to the rest of EdgeX Provision the Devices the Device Service will manage with EdgeX Reference information includes things such as defining the address (called an Addressable ) of the Device and Device Service or establishing the new unit of measure (called a Value Descriptor in EdgeX) used by the Device. The term \"provision\" is the way we talk about establishing the initial connection to the physical Device and have it be known to and communication with EdgeX. After the first run of a Device Service, these steps are not repeated. For example, after its initial startup, a Device Service would not need to re-establish the reference information into EdgeX. Instead,it would simply check that these operations have been accomplished and do not need to be redone. Creating Reference Information in EdgeX There is a lot of background information that EdgeX needs to know about the Device and Device Service before it can start collecting data from the Device or send actuation commands to the Device. Say, for example, the camera Device wanted to report its human and canine counts. If it were to just start sending numbers into EdgeX, EdgeX would have no idea of what those numbers represented or even where they came from. Further, if someone/something wanted to send a command to the camera, it would not know how to reach the camera without some additional information like where the camera is located on the network. This background or reference information is what a Device Service must define in EdgeX when it first comes up. The API calls here give you a glimpse of this communication between the fledgling Device Service and the other EdgeX micro services. By the way, the order in which these calls are shown may not be the exact order that a Device Service does them. As you become more familiar with Device Services and the Device Service SDK, the small nuances and differences will become clear. Addressables See Core Metadata API RAML at APIs Core Services Metadata The Device Service will often establish at least two Addressable objects with the Core Metadata micro service. An Addressable is a flexible EdgeX object that specifies a physical address of something - in this case the physical address of the Device Service and the Device (the camera). While an Addressable could be created for a named MQTT pipe or other protocol endpoint, for this example, we will assume that both the Device Service and Device are able to be reached via HTTP REST calls. So in this case, the Device Service would make two calls to Core Metadata, to create the Addressable for the Device Service: POST to http : // localhost : 48081 / api / v1 / addressable BODY : { \"name\" : \"camera control\" , \"protocol\" : \"HTTP\" , \"address\" : \"172.17.0.1\" , \"port\" : 49977 , \"path\" : \"/cameracontrol\" , \"publisher\" : \"none\" , \"user\" : \"none\" , \"password\" : \"none\" , \"topic\" : \"none\" } and the Addressable for the Device (the camera in this case): POST to http : // localhost : 48081 / api / v1 / addressable BODY : { \"name\" : \"camera1 address\" , \"protocol\" : \"HTTP\" , \"address\" : \"172.17.0.1\" , \"port\" : 49999 , \"path\" : \"/camera1\" , \"publisher\" : \"none\" , \"user\" : \"none\" , \"password\" : \"none\" , \"topic\" : \"none\" } Note that for an Addressable, a unique name must be provided. Obviously, these address are phony and made up for the purposes of this exercise. This is OK and it will still allow you to see how your Device and Device Services will work going forward. Walk Through Alert! If you are using Postman, be sure that you are POSTing raw data, not form-encoded data. If your API call was successful, you will get a generated ID for your new Addressable that looks like this: 5b773ad19f8fc200012a802d Value Descriptors See Core Data API RAML at APIs Core Services Core Data Next, the Device Service needs to inform EdgeX about the type of data it will be sending on the behalf of the Devices. If you are given the number 5, what does that mean to you? Nothing, without some context and unit of measure. For example, if I was to say 5 feet is the scan depth of the camera right now, you have a much better understanding about what the number 5 represents. In EdgeX, Value Descriptors provide the context and unit of measure for any data (or values) sent to and from a Device. As the name implies, a Value Descriptor describes a value - its unit of measure, its min and max values (if there are any), the way to display the value when showing it on the screen, and more. Any data obtained from a Device (we call this \"get\" from the Device) or any data sent to the Device for actuation (we call this \"set\" or \"put\" to the Device) requires a Value Descriptor to be associated with that data. In this demo, there are four Value Descriptors required: human count, canine count, scan depth, and snapshot duration. The Device Service would make four POST requests to Core Data to establish these Value Descriptors. Walk Through alert! Pay attention to the port numbers. In the previous section you were calling the core-metadata service (port 48081), in these you will be calling core-data (port 48080) POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"humancount\" , \"description\" : \"people count\" , \"min\" : \"0\" , \"max\" : \"100\" , \"type\" : \"I\" , \"uomLabel\" : \"count\" , \"defaultValue\" : \"0\" , \"formatting\" : \"%s\" , \"labels\" :[ \"count\" , \"humans\" ] } POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"caninecount\" , \"description\" : \"dog count\" , \"min\" : \"0\" , \"max\" : \"100\" , \"type\" : \"I\" , \"uomLabel\" : \"count\" , \"defaultValue\" : \"0\" , \"formatting\" : \"%s\" , \"labels\" :[ \"count\" , \"canines\" ] } POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"depth\" , \"description\" : \"scan distance\" , \"min\" : \"1\" , \"max\" : \"10\" , \"type\" : \"I\" , \"uomLabel\" : \"feet\" , \"defaultValue\" : \"1\" , \"formatting\" : \"%s\" , \"labels\" :[ \"scan\" , \"distance\" ] } POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"duration\" , \"description\" : \"time between events\" , \"min\" : \"10\" , \"max\" : \"180\" , \"type\" : \"I\" , \"uomLabel\" : \"seconds\" , \"defaultValue\" : \"10\" , \"formatting\" : \"%s\" , \"labels\" :[ \"duration\" , \"time\" ] } An error can occur when communication with the camera. Therefore a fifth Value Descriptor is created for this eventuality. POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"cameraerror\" , \"description\" : \"error response message from a camera\" , \"min\" : \"\" , \"max\" : \"\" , \"type\" : \"S\" , \"uomLabel\" : \"\" , \"defaultValue\" : \"error\" , \"formatting\" : \"%s\" , \"labels\" :[ \"error\" , \"message\" ] } Again, the name of each Value Descriptor must be unique (within all of EdgeX). The type of a Value Descriptor indicates the type of the associated value: I (integer), F (floating point number), S (character or string), B (boolean), or J (JSON object). Formatting is used by UIs and should follow the printf formatting standard for how to represent the associated value. Walk Through alert! If you make a GET call to the http://localhost:48080/api/v1/valuedescriptor URL you will get a listing (in JSON) of all the Value Descriptors currently defined in your instance of EdgeX, including the ones you just added.","title":"Defining your data"},{"location":"walk-through/Ch-WalkthroughData/#defining-your-data","text":"When a new Device Service is first started in EdgeX, there are many many tasks to perform - all in preparation for the Device Service to manage one or more Devices, which are yet unknown to EdgeX. In general, the Device Service tasks when it first starts can be categorized into: Establish the reference information around the Device Service and Device. Make the Device Service itself known to the rest of EdgeX Provision the Devices the Device Service will manage with EdgeX Reference information includes things such as defining the address (called an Addressable ) of the Device and Device Service or establishing the new unit of measure (called a Value Descriptor in EdgeX) used by the Device. The term \"provision\" is the way we talk about establishing the initial connection to the physical Device and have it be known to and communication with EdgeX. After the first run of a Device Service, these steps are not repeated. For example, after its initial startup, a Device Service would not need to re-establish the reference information into EdgeX. Instead,it would simply check that these operations have been accomplished and do not need to be redone.","title":"Defining your data"},{"location":"walk-through/Ch-WalkthroughData/#creating-reference-information-in-edgex","text":"There is a lot of background information that EdgeX needs to know about the Device and Device Service before it can start collecting data from the Device or send actuation commands to the Device. Say, for example, the camera Device wanted to report its human and canine counts. If it were to just start sending numbers into EdgeX, EdgeX would have no idea of what those numbers represented or even where they came from. Further, if someone/something wanted to send a command to the camera, it would not know how to reach the camera without some additional information like where the camera is located on the network. This background or reference information is what a Device Service must define in EdgeX when it first comes up. The API calls here give you a glimpse of this communication between the fledgling Device Service and the other EdgeX micro services. By the way, the order in which these calls are shown may not be the exact order that a Device Service does them. As you become more familiar with Device Services and the Device Service SDK, the small nuances and differences will become clear.","title":"Creating Reference Information in EdgeX"},{"location":"walk-through/Ch-WalkthroughData/#addressables","text":"See Core Metadata API RAML at APIs Core Services Metadata The Device Service will often establish at least two Addressable objects with the Core Metadata micro service. An Addressable is a flexible EdgeX object that specifies a physical address of something - in this case the physical address of the Device Service and the Device (the camera). While an Addressable could be created for a named MQTT pipe or other protocol endpoint, for this example, we will assume that both the Device Service and Device are able to be reached via HTTP REST calls. So in this case, the Device Service would make two calls to Core Metadata, to create the Addressable for the Device Service: POST to http : // localhost : 48081 / api / v1 / addressable BODY : { \"name\" : \"camera control\" , \"protocol\" : \"HTTP\" , \"address\" : \"172.17.0.1\" , \"port\" : 49977 , \"path\" : \"/cameracontrol\" , \"publisher\" : \"none\" , \"user\" : \"none\" , \"password\" : \"none\" , \"topic\" : \"none\" } and the Addressable for the Device (the camera in this case): POST to http : // localhost : 48081 / api / v1 / addressable BODY : { \"name\" : \"camera1 address\" , \"protocol\" : \"HTTP\" , \"address\" : \"172.17.0.1\" , \"port\" : 49999 , \"path\" : \"/camera1\" , \"publisher\" : \"none\" , \"user\" : \"none\" , \"password\" : \"none\" , \"topic\" : \"none\" } Note that for an Addressable, a unique name must be provided. Obviously, these address are phony and made up for the purposes of this exercise. This is OK and it will still allow you to see how your Device and Device Services will work going forward. Walk Through Alert! If you are using Postman, be sure that you are POSTing raw data, not form-encoded data. If your API call was successful, you will get a generated ID for your new Addressable that looks like this: 5b773ad19f8fc200012a802d","title":"Addressables"},{"location":"walk-through/Ch-WalkthroughData/#value-descriptors","text":"See Core Data API RAML at APIs Core Services Core Data Next, the Device Service needs to inform EdgeX about the type of data it will be sending on the behalf of the Devices. If you are given the number 5, what does that mean to you? Nothing, without some context and unit of measure. For example, if I was to say 5 feet is the scan depth of the camera right now, you have a much better understanding about what the number 5 represents. In EdgeX, Value Descriptors provide the context and unit of measure for any data (or values) sent to and from a Device. As the name implies, a Value Descriptor describes a value - its unit of measure, its min and max values (if there are any), the way to display the value when showing it on the screen, and more. Any data obtained from a Device (we call this \"get\" from the Device) or any data sent to the Device for actuation (we call this \"set\" or \"put\" to the Device) requires a Value Descriptor to be associated with that data. In this demo, there are four Value Descriptors required: human count, canine count, scan depth, and snapshot duration. The Device Service would make four POST requests to Core Data to establish these Value Descriptors. Walk Through alert! Pay attention to the port numbers. In the previous section you were calling the core-metadata service (port 48081), in these you will be calling core-data (port 48080) POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"humancount\" , \"description\" : \"people count\" , \"min\" : \"0\" , \"max\" : \"100\" , \"type\" : \"I\" , \"uomLabel\" : \"count\" , \"defaultValue\" : \"0\" , \"formatting\" : \"%s\" , \"labels\" :[ \"count\" , \"humans\" ] } POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"caninecount\" , \"description\" : \"dog count\" , \"min\" : \"0\" , \"max\" : \"100\" , \"type\" : \"I\" , \"uomLabel\" : \"count\" , \"defaultValue\" : \"0\" , \"formatting\" : \"%s\" , \"labels\" :[ \"count\" , \"canines\" ] } POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"depth\" , \"description\" : \"scan distance\" , \"min\" : \"1\" , \"max\" : \"10\" , \"type\" : \"I\" , \"uomLabel\" : \"feet\" , \"defaultValue\" : \"1\" , \"formatting\" : \"%s\" , \"labels\" :[ \"scan\" , \"distance\" ] } POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"duration\" , \"description\" : \"time between events\" , \"min\" : \"10\" , \"max\" : \"180\" , \"type\" : \"I\" , \"uomLabel\" : \"seconds\" , \"defaultValue\" : \"10\" , \"formatting\" : \"%s\" , \"labels\" :[ \"duration\" , \"time\" ] } An error can occur when communication with the camera. Therefore a fifth Value Descriptor is created for this eventuality. POST to http : // localhost : 48080 / api / v1 / valuedescriptor BODY : { \"name\" : \"cameraerror\" , \"description\" : \"error response message from a camera\" , \"min\" : \"\" , \"max\" : \"\" , \"type\" : \"S\" , \"uomLabel\" : \"\" , \"defaultValue\" : \"error\" , \"formatting\" : \"%s\" , \"labels\" :[ \"error\" , \"message\" ] } Again, the name of each Value Descriptor must be unique (within all of EdgeX). The type of a Value Descriptor indicates the type of the associated value: I (integer), F (floating point number), S (character or string), B (boolean), or J (JSON object). Formatting is used by UIs and should follow the printf formatting standard for how to represent the associated value. Walk Through alert! If you make a GET call to the http://localhost:48080/api/v1/valuedescriptor URL you will get a listing (in JSON) of all the Value Descriptors currently defined in your instance of EdgeX, including the ones you just added.","title":"Value Descriptors"},{"location":"walk-through/Ch-WalkthroughDeviceProfile/","text":"Defining your device A Device Profile can be thought of as a template or as a type or classification of Device. General characteristics about the type of Device, the data theses Devices provide, and how to command them is all provided in a Device Profile. Other pages within these docs provide more details about a Device Profile and its purpose (see Core-Metadata to start). It is typical that as part of the reference information setup sequence, the Device Service provides the Device Profiles for the types of Devices it manages. Adding a Device Profile See Core Metadata API RAML at APIs Core Services Metadata Our fictitious Device Service will manage only the human/dog counting camera, so it only needs to make one POST request to create the monitoring camera Device Profile. Since Device Profiles are often represented in YAML, make a muti-part form-data POST with the Device Profile file below to create the Camera Monitor profile. POST to http://localhost:48081/api/v1/deviceprofile/uploadfile No headers FORM-DATA: key: \"file\" value: EdgeX_CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} Walk Through alert! In this step you will want to use the form-data POST format in Postman, with a key named \"file\" of type \"File\". Download and use the provided EdgeX_CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} for this. Each profile has a unique name along with a description, manufacturer, model and collection of labels to assist in queries for particular profiles. These are relatively straightforward attributes of a profile. Understanding Commands The Device Profile defines how to communicate with any Device that abides by the profile. In particular, it defines the Commands that can be sent to the Device (via the Device Service). Commands are named and have either a get (for retrieving data from the Device) or put (to send data to the Device) or both. Each Command can have a single get and single put. Both get and put are optional, but it would not make sense to have a Command without at least one get or at least one put. The Command name must be unique for that profile (the Command name does not have to be unique across all of EdgeX - for example, many profiles may contain a \"status\" Command). Understanding Command Gets and Puts The get and put each have a path which is used by EdgeX to call on the specific Command get or put at the URL address provided for the service. Hypothetically, if the address to a Device Service was \" http://abc:9999 \" and the get Command had a path of \"foo\", then internally, EdgeX would know to use \" http://abc:9999/foo \" to call on the get Command. Get and puts then have response objects (an array of response objects). A get must have at least one response object. A put is not required to have a response. Responses might be \"good\" or \"error\" responses. Each get should have at least one \"good\" response, but it may have several error responses depending on what problems or issues the Device Service may need to reply with. Each response is made up of a code (which suggests if it is a good or error response), a description (human readable information about what is in the response), and an array of expected values. For practical purposes, the code is usually an HTTP status code like 200 (for good responses), 404 or 503 (examples of bad responses). The expected values in a response are an array of Value Descriptor names. If a call to an get Command is expected to return back the human and dog count data, then the response's expected values would be: [humancount, caninecount]. When the actual call to the Device Service is made, the body of the return response from the service is expected to return a value for each of the expected values in a map where the Value Descriptor names are used as keys. Again, using the human and dog counts as an example, if the expected values were [humancount, caninecount] then the body of a good response from the service would contain a map that looks something like this: { \"humancount\" : 5 , \"caninecount\" : 2 } Here is an example set of responses that might be used for a get Command in the camera example. Note that one response is coded as the \"good\" response (code 200) while the other is for \"error\" response (code 404). The expected values for the good response are the Value Descriptor names for the camera's count data. The expected values for the \"error\" response is the Value Descriptor name for an error message. { \"responses\" :[ { \"code\" : \"200\" , \"description\" : \"ok\" , \"expectedValues\" :[ \"humancount\" , \"caninecount\" ]}, { \"code\" : \"404\" , \"description\" : \"bad request\" , \"expectedValues\" :[ \"cameraerror\" ]} ] } } Understanding Command Parameters Commands are used to send data to Devices (via Device Services) as much as they are used to get data from Devices. Therefore, any Command may have a set of parameters associated with its call. Parameter data is added to the body of the Command request. Parameters are defined via an array of parameterNames on a Command. Here again, this array is just an array of Value Descriptor names. Each Value Descriptor defines the name and type of information to be supplied as a parameter to the Command call. For example, if a Command had a parameterNames array of [depth, duration] , then the receiving command is expecting values that match the depth and duration Value Descriptors. Similar to the way expected values are used to set the keys of the response body, the parameter names are used as keys in a map to pass parameter values in a Command call that has parameters. Here might be what is populated in the body of the Command call when the parameterNames are [depth, duration] . { \"depth\" : 1 , \"duration\" : 10 } If you open the CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} file, see that there are Commands to get people and dog counts (and a command called Counts, which provides both values). There are also commands to get/put the snapshot duration and scan depth. Also note the expected values for the Commands. The expected values should match the name of the Value Descriptors from above that give context to the returned values. In real implementations, the Device Profile may contain many more details (like device resource and resource elements) to assist the Device Service in its communications with Devices. Expected Values Alert! Metadata does not currently check that the expected values match an existing Value Descriptor by name. Therefore, make sure you provide the expected values array carefully when creating Device Profiles.","title":"Defining your device"},{"location":"walk-through/Ch-WalkthroughDeviceProfile/#defining-your-device","text":"A Device Profile can be thought of as a template or as a type or classification of Device. General characteristics about the type of Device, the data theses Devices provide, and how to command them is all provided in a Device Profile. Other pages within these docs provide more details about a Device Profile and its purpose (see Core-Metadata to start). It is typical that as part of the reference information setup sequence, the Device Service provides the Device Profiles for the types of Devices it manages.","title":"Defining your device"},{"location":"walk-through/Ch-WalkthroughDeviceProfile/#adding-a-device-profile","text":"See Core Metadata API RAML at APIs Core Services Metadata Our fictitious Device Service will manage only the human/dog counting camera, so it only needs to make one POST request to create the monitoring camera Device Profile. Since Device Profiles are often represented in YAML, make a muti-part form-data POST with the Device Profile file below to create the Camera Monitor profile. POST to http://localhost:48081/api/v1/deviceprofile/uploadfile No headers FORM-DATA: key: \"file\" value: EdgeX_CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} Walk Through alert! In this step you will want to use the form-data POST format in Postman, with a key named \"file\" of type \"File\". Download and use the provided EdgeX_CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} for this. Each profile has a unique name along with a description, manufacturer, model and collection of labels to assist in queries for particular profiles. These are relatively straightforward attributes of a profile.","title":"Adding a Device Profile"},{"location":"walk-through/Ch-WalkthroughDeviceProfile/#understanding-commands","text":"The Device Profile defines how to communicate with any Device that abides by the profile. In particular, it defines the Commands that can be sent to the Device (via the Device Service). Commands are named and have either a get (for retrieving data from the Device) or put (to send data to the Device) or both. Each Command can have a single get and single put. Both get and put are optional, but it would not make sense to have a Command without at least one get or at least one put. The Command name must be unique for that profile (the Command name does not have to be unique across all of EdgeX - for example, many profiles may contain a \"status\" Command).","title":"Understanding Commands"},{"location":"walk-through/Ch-WalkthroughDeviceProfile/#understanding-command-gets-and-puts","text":"The get and put each have a path which is used by EdgeX to call on the specific Command get or put at the URL address provided for the service. Hypothetically, if the address to a Device Service was \" http://abc:9999 \" and the get Command had a path of \"foo\", then internally, EdgeX would know to use \" http://abc:9999/foo \" to call on the get Command. Get and puts then have response objects (an array of response objects). A get must have at least one response object. A put is not required to have a response. Responses might be \"good\" or \"error\" responses. Each get should have at least one \"good\" response, but it may have several error responses depending on what problems or issues the Device Service may need to reply with. Each response is made up of a code (which suggests if it is a good or error response), a description (human readable information about what is in the response), and an array of expected values. For practical purposes, the code is usually an HTTP status code like 200 (for good responses), 404 or 503 (examples of bad responses). The expected values in a response are an array of Value Descriptor names. If a call to an get Command is expected to return back the human and dog count data, then the response's expected values would be: [humancount, caninecount]. When the actual call to the Device Service is made, the body of the return response from the service is expected to return a value for each of the expected values in a map where the Value Descriptor names are used as keys. Again, using the human and dog counts as an example, if the expected values were [humancount, caninecount] then the body of a good response from the service would contain a map that looks something like this: { \"humancount\" : 5 , \"caninecount\" : 2 } Here is an example set of responses that might be used for a get Command in the camera example. Note that one response is coded as the \"good\" response (code 200) while the other is for \"error\" response (code 404). The expected values for the good response are the Value Descriptor names for the camera's count data. The expected values for the \"error\" response is the Value Descriptor name for an error message. { \"responses\" :[ { \"code\" : \"200\" , \"description\" : \"ok\" , \"expectedValues\" :[ \"humancount\" , \"caninecount\" ]}, { \"code\" : \"404\" , \"description\" : \"bad request\" , \"expectedValues\" :[ \"cameraerror\" ]} ] } }","title":"Understanding Command Gets and Puts"},{"location":"walk-through/Ch-WalkthroughDeviceProfile/#understanding-command-parameters","text":"Commands are used to send data to Devices (via Device Services) as much as they are used to get data from Devices. Therefore, any Command may have a set of parameters associated with its call. Parameter data is added to the body of the Command request. Parameters are defined via an array of parameterNames on a Command. Here again, this array is just an array of Value Descriptor names. Each Value Descriptor defines the name and type of information to be supplied as a parameter to the Command call. For example, if a Command had a parameterNames array of [depth, duration] , then the receiving command is expecting values that match the depth and duration Value Descriptors. Similar to the way expected values are used to set the keys of the response body, the parameter names are used as keys in a map to pass parameter values in a Command call that has parameters. Here might be what is populated in the body of the Command call when the parameterNames are [depth, duration] . { \"depth\" : 1 , \"duration\" : 10 } If you open the CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} file, see that there are Commands to get people and dog counts (and a command called Counts, which provides both values). There are also commands to get/put the snapshot duration and scan depth. Also note the expected values for the Commands. The expected values should match the name of the Value Descriptors from above that give context to the returned values. In real implementations, the Device Profile may contain many more details (like device resource and resource elements) to assist the Device Service in its communications with Devices. Expected Values Alert! Metadata does not currently check that the expected values match an existing Value Descriptor by name. Therefore, make sure you provide the expected values array carefully when creating Device Profiles.","title":"Understanding Command Parameters"},{"location":"walk-through/Ch-WalkthroughDeviceService/","text":"Register your Device Service Once the reference information is established by the Device Service in Core Data and Meta Data, the Device Service can register or define itself in EdgeX. That is, it can proclaim to EdgeX that \"I have arrived and am functional.\" Register with Core Configuration and Registration See APIs Core Services Configuration and Registry Part of that registration process of the Device Service, indeed any EdgeX micro service, is to register itself with the Core Configuration & Registration. In this process, the micro service provides its location to the Config/Reg micro service and picks up any new/latest configuration information from this central service. Since there is no real Device Service in this demonstration, this part of the inter-micro service exchange is not explored here. Create the Device Service See APIs Core Services Metadata The Device Service must then create an instance of itself in Core Metadata. It is in this registration, that the Device Service is associated to the Addressable for the Device Service that is already Core Metadata. Make this POST to Core Metadata to create the Device Service (using the Addressable's unique name to establish the association) POST to http : // localhost : 48081 / api / v1 / deviceservice BODY : { \"name\" : \"camera control device service\" , \"description\" : \"Manage human and dog counting cameras\" , \"labels\" :[ \"camera\" , \"counter\" ], \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"addressable\" : { \"name\" : \"camera control\" }} The name of the Device Service must be unique across all of EdgeX. Note the admin and operating states. The administrative state (aka admin state) provides control of the Device Service by man or other systems. It can be set to locked or unlocked. When a Device Service is set to locked, it is not suppose to respond to any Command requests nor send data from the Devices. The operating state (aka op state) provides an indication on the part of EdgeX about the internal operating status of the Device Service. The operating state is not set externally (as by another system or man), it is a signal from within EdgeX (and potentially the Device Service itself) about the condition of the service. The operating state of the Device Service may be either enabled or disabled. When the operating state of the Device Service is disabled, it is either experiencing some difficulty or going through some process (for example an upgrade) which does not allow it to function in its normal capacity.","title":"Register your Device Service"},{"location":"walk-through/Ch-WalkthroughDeviceService/#register-your-device-service","text":"Once the reference information is established by the Device Service in Core Data and Meta Data, the Device Service can register or define itself in EdgeX. That is, it can proclaim to EdgeX that \"I have arrived and am functional.\"","title":"Register your Device Service"},{"location":"walk-through/Ch-WalkthroughDeviceService/#register-with-core-configuration-and-registration","text":"See APIs Core Services Configuration and Registry Part of that registration process of the Device Service, indeed any EdgeX micro service, is to register itself with the Core Configuration & Registration. In this process, the micro service provides its location to the Config/Reg micro service and picks up any new/latest configuration information from this central service. Since there is no real Device Service in this demonstration, this part of the inter-micro service exchange is not explored here.","title":"Register with Core Configuration and Registration"},{"location":"walk-through/Ch-WalkthroughDeviceService/#create-the-device-service","text":"See APIs Core Services Metadata The Device Service must then create an instance of itself in Core Metadata. It is in this registration, that the Device Service is associated to the Addressable for the Device Service that is already Core Metadata. Make this POST to Core Metadata to create the Device Service (using the Addressable's unique name to establish the association) POST to http : // localhost : 48081 / api / v1 / deviceservice BODY : { \"name\" : \"camera control device service\" , \"description\" : \"Manage human and dog counting cameras\" , \"labels\" :[ \"camera\" , \"counter\" ], \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"addressable\" : { \"name\" : \"camera control\" }} The name of the Device Service must be unique across all of EdgeX. Note the admin and operating states. The administrative state (aka admin state) provides control of the Device Service by man or other systems. It can be set to locked or unlocked. When a Device Service is set to locked, it is not suppose to respond to any Command requests nor send data from the Devices. The operating state (aka op state) provides an indication on the part of EdgeX about the internal operating status of the Device Service. The operating state is not set externally (as by another system or man), it is a signal from within EdgeX (and potentially the Device Service itself) about the condition of the service. The operating state of the Device Service may be either enabled or disabled. When the operating state of the Device Service is disabled, it is either experiencing some difficulty or going through some process (for example an upgrade) which does not allow it to function in its normal capacity.","title":"Create the Device Service"},{"location":"walk-through/Ch-WalkthroughExporting/","text":"Exporting your device data Great, so the data sent by the camera Device makes it way to Core Data. How can that data be sent to an enterprise system or the Cloud? How can that data be used by an edge analytics system (like the Rules Engine provided with EdgeX) to actuate on a Device? Anything wishing to receive the sensor/device data as it comes into EdgeX must register as an \"export\" client. Export Clients In fact, by default, the Rules Engine is automatically registered as a client of the export services and automatically receives all the Events/Readings from Core Data that are sent by Devices. To see all the existing export clients, you can request a list from the Export Client micro service. GET to http://localhost:48071/api/v1/registration The response from Export Client is a list of registered client details - in this case just the Rules Engine is registered. Register an Export Client To register a new client to receive EdgeX data, you will first need to setup a client capable of receiving HTTP REST calls, or an MQTT topic capable of receiving messages from EdgeX. For the purposes of this demonstration, let's say there is an cloud based MQTT Topic that has been setup ready to receive EdgeX Event/Reading data. To register this MQTT endpoint to receive all Event/Reading data, in JSON format, but encrypted, you will need to request Export Client to make a new EdgeX client. POST to http : // localhost : 48071 / api / v1 / registration BODY : { \"name\" : \"MyMQTTTopic\" , \"addressable\" : { \"name\" : \"MyMQTTBroker\" , \"protocol\" : \"TCP\" , \"address\" : \"tcp://m10.cloudmqtt.com\" , \"port\" : 15421 , \"publisher\" : \"EdgeXExportPublisher\" , \"user\" : \"hukfgtoh\" , \"password\" : \"mypass\" , \"topic\" : \"EdgeXDataTopic\" } , \"format\" : \"JSON\" , \"encryption\" : { \"encryptionAlgorithm\" : \"AES\" , \"encryptionKey\" : \"123\" , \"initializingVector\" : \"123\" } , \"enable\" : true , \"destination\" : \"MQTT_TOPIC\" } Note that the Addressable for the REST address is built into the request. Now, should a new Event be posted to Core Data, the Export Distro micro service will attempt to sent the encrypted, JSON-formated Event/Reading data to the MQTT client. Unless you have actually setup the MQTT Topic to receive the messages, Export Distro will fail to deliver the contents and an error will result. You can check the Export Distro log to see the attempt was made and that the EdgeX Export services are working correctly, despite the non-existence of the receiving MQTT Topic. MQTTOutboundServiceActivator: message sent to MQTT broker: Addressable [name=MyMQTTBroker, protocol=TCP, address=tcp://m10.cloudmqtt.com, port=15421, path=null, publisher=EdgeXExportPublisher, user=hukfgtoh, password=mypass, topic=EdgeXDataTopic, toString()=BaseObject [id=null, created=0, modified=0, origin=0]] : 596283c7e4b0011866276e9 Building your own solutions Congratulations, you've made it all the way through the Walkthrough tutorial!","title":"Exporting your device data"},{"location":"walk-through/Ch-WalkthroughExporting/#exporting-your-device-data","text":"Great, so the data sent by the camera Device makes it way to Core Data. How can that data be sent to an enterprise system or the Cloud? How can that data be used by an edge analytics system (like the Rules Engine provided with EdgeX) to actuate on a Device? Anything wishing to receive the sensor/device data as it comes into EdgeX must register as an \"export\" client.","title":"Exporting your device data"},{"location":"walk-through/Ch-WalkthroughExporting/#export-clients","text":"In fact, by default, the Rules Engine is automatically registered as a client of the export services and automatically receives all the Events/Readings from Core Data that are sent by Devices. To see all the existing export clients, you can request a list from the Export Client micro service. GET to http://localhost:48071/api/v1/registration The response from Export Client is a list of registered client details - in this case just the Rules Engine is registered.","title":"Export Clients"},{"location":"walk-through/Ch-WalkthroughExporting/#register-an-export-client","text":"To register a new client to receive EdgeX data, you will first need to setup a client capable of receiving HTTP REST calls, or an MQTT topic capable of receiving messages from EdgeX. For the purposes of this demonstration, let's say there is an cloud based MQTT Topic that has been setup ready to receive EdgeX Event/Reading data. To register this MQTT endpoint to receive all Event/Reading data, in JSON format, but encrypted, you will need to request Export Client to make a new EdgeX client. POST to http : // localhost : 48071 / api / v1 / registration BODY : { \"name\" : \"MyMQTTTopic\" , \"addressable\" : { \"name\" : \"MyMQTTBroker\" , \"protocol\" : \"TCP\" , \"address\" : \"tcp://m10.cloudmqtt.com\" , \"port\" : 15421 , \"publisher\" : \"EdgeXExportPublisher\" , \"user\" : \"hukfgtoh\" , \"password\" : \"mypass\" , \"topic\" : \"EdgeXDataTopic\" } , \"format\" : \"JSON\" , \"encryption\" : { \"encryptionAlgorithm\" : \"AES\" , \"encryptionKey\" : \"123\" , \"initializingVector\" : \"123\" } , \"enable\" : true , \"destination\" : \"MQTT_TOPIC\" } Note that the Addressable for the REST address is built into the request. Now, should a new Event be posted to Core Data, the Export Distro micro service will attempt to sent the encrypted, JSON-formated Event/Reading data to the MQTT client. Unless you have actually setup the MQTT Topic to receive the messages, Export Distro will fail to deliver the contents and an error will result. You can check the Export Distro log to see the attempt was made and that the EdgeX Export services are working correctly, despite the non-existence of the receiving MQTT Topic. MQTTOutboundServiceActivator: message sent to MQTT broker: Addressable [name=MyMQTTBroker, protocol=TCP, address=tcp://m10.cloudmqtt.com, port=15421, path=null, publisher=EdgeXExportPublisher, user=hukfgtoh, password=mypass, topic=EdgeXDataTopic, toString()=BaseObject [id=null, created=0, modified=0, origin=0]] : 596283c7e4b0011866276e9","title":"Register an Export Client"},{"location":"walk-through/Ch-WalkthroughExporting/#building-your-own-solutions","text":"Congratulations, you've made it all the way through the Walkthrough tutorial!","title":"Building your own solutions"},{"location":"walk-through/Ch-WalkthroughProvision/","text":"Provision a Device In the last act of setup, a Device Service often discovers and provisions new Devices it finds and is going to manage on the part of EdgeX. Note the word \"often\" in the last sentence. Not all Device Services will discover new Devices or provision them right away. Depending on the type of Device and how the Devices communicate, it is up to the Device Service to determine how/when to provision a Device. In some rare cases, the provisioning may be triggered by a human request of the Device Service once everything is in place and once the human can provide the information the Device Service needs to physically connect to the Device. Adding your device See APIs Core Services Metadata For the sake of this demonstration, the call to Core Metadata below will provision the human/dog counting monitor camera as if the Device Service discovered it (by some unknown means) and provisioned the Device as part of some startup process. To create a Device, it must be associated to a Device Profile (by name or id), a Device Service (by name or id), and contain one or more Protocols defining its address. When calling each of the POST calls above, the ID was returned by the associated micro service and used in the call below. In this example, the names of Device Profile, Device Service, and Protocols are used. POST to http : // localhost : 48081 / api / v1 / device BODY : { \"name\" : \"countcamera1\" , \"description\" : \"human and dog counting camera #1\" , \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"protocols\" : { \"camera protocol\" : { \"camera address\" : \"camera 1\" }} , \"labels\" : [ \"camera\" , \"counter\" ], \"location\" : \"\" , \"service\" : { \"name\" : \"camera control device service\" } , \"profile\" : { \"name\" : \"camera monitor profile\" }} Note that camera monitor profile was created by the CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} you uploaded in a previous step. Test the Setup With the Device Service and Device now appropriately setup/provisioned in EdgeX, let's try a few of the micro service APIs out to confirm that things have been configured correctly. Check the Device Service See APIs Core Services Metadata To begin, check out that the Device Service is available via Core Metadata. GET to http://localhost:48081/api/v1/deviceservice Note that the associated Addressable is returned with the Device Service. There are many additional APIs on Core Metadata to retrieve a Device Service. As an example, here is one to find all Device Services by label - in this case using the label that was associated to the camera control device service. GET to http://localhost:48081/api/v1/deviceservice/label/camera Check the Device See APIs Core Services Metadata Ensure the monitor camera is among the devices known to Core Metadata. GET to http://localhost:48081/api/v1/device Note that the associated Device Profile, Device Service and Addressable is returned with the Device. Again, there are many additional APIs on Core Metadata to retrieve a Device. As an example, here is one to find all Devices associated to a given Device Profile - in this case using the camera monitor profile Device Profile name. GET to http://localhost:48081/api/v1/device/profilename/camera+monitor+profile","title":"Provision a Device"},{"location":"walk-through/Ch-WalkthroughProvision/#provision-a-device","text":"In the last act of setup, a Device Service often discovers and provisions new Devices it finds and is going to manage on the part of EdgeX. Note the word \"often\" in the last sentence. Not all Device Services will discover new Devices or provision them right away. Depending on the type of Device and how the Devices communicate, it is up to the Device Service to determine how/when to provision a Device. In some rare cases, the provisioning may be triggered by a human request of the Device Service once everything is in place and once the human can provide the information the Device Service needs to physically connect to the Device.","title":"Provision a Device"},{"location":"walk-through/Ch-WalkthroughProvision/#adding-your-device","text":"See APIs Core Services Metadata For the sake of this demonstration, the call to Core Metadata below will provision the human/dog counting monitor camera as if the Device Service discovered it (by some unknown means) and provisioned the Device as part of some startup process. To create a Device, it must be associated to a Device Profile (by name or id), a Device Service (by name or id), and contain one or more Protocols defining its address. When calling each of the POST calls above, the ID was returned by the associated micro service and used in the call below. In this example, the names of Device Profile, Device Service, and Protocols are used. POST to http : // localhost : 48081 / api / v1 / device BODY : { \"name\" : \"countcamera1\" , \"description\" : \"human and dog counting camera #1\" , \"adminState\" : \"unlocked\" , \"operatingState\" : \"enabled\" , \"protocols\" : { \"camera protocol\" : { \"camera address\" : \"camera 1\" }} , \"labels\" : [ \"camera\" , \"counter\" ], \"location\" : \"\" , \"service\" : { \"name\" : \"camera control device service\" } , \"profile\" : { \"name\" : \"camera monitor profile\" }} Note that camera monitor profile was created by the CameraMonitorProfile.yml <EdgeX_CameraMonitorProfile.yml> {.interpreted-text role=\"download\"} you uploaded in a previous step.","title":"Adding your device"},{"location":"walk-through/Ch-WalkthroughProvision/#test-the-setup","text":"With the Device Service and Device now appropriately setup/provisioned in EdgeX, let's try a few of the micro service APIs out to confirm that things have been configured correctly.","title":"Test the Setup"},{"location":"walk-through/Ch-WalkthroughProvision/#check-the-device-service","text":"See APIs Core Services Metadata To begin, check out that the Device Service is available via Core Metadata. GET to http://localhost:48081/api/v1/deviceservice Note that the associated Addressable is returned with the Device Service. There are many additional APIs on Core Metadata to retrieve a Device Service. As an example, here is one to find all Device Services by label - in this case using the label that was associated to the camera control device service. GET to http://localhost:48081/api/v1/deviceservice/label/camera","title":"Check the Device Service"},{"location":"walk-through/Ch-WalkthroughProvision/#check-the-device","text":"See APIs Core Services Metadata Ensure the monitor camera is among the devices known to Core Metadata. GET to http://localhost:48081/api/v1/device Note that the associated Device Profile, Device Service and Addressable is returned with the Device. Again, there are many additional APIs on Core Metadata to retrieve a Device. As an example, here is one to find all Devices associated to a given Device Profile - in this case using the camera monitor profile Device Profile name. GET to http://localhost:48081/api/v1/device/profilename/camera+monitor+profile","title":"Check the Device"},{"location":"walk-through/Ch-WalkthroughReading/","text":"Sending events and reading data In the real world, the human/dog counting camera would start to take pictures, count beings, and send that data to EdgeX. To simulate this activity. in this section, you will make Core Data API calls as if you were the camera's Device and Device Service. Send an Event/Reading See APIs Core Services Core Data Data is submitted to Core Data as an Event. An Event is a collection of sensor readings from a Device (associated to a Device by its ID or name) at a particular point in time. A Reading in an Event is a particular value sensed by the Device and associated to a Value Descriptor (by name) to provide context to the reading. So, the human/dog counting camera might determine that there are current 5 people and 3 dogs in the space it is monitoring. In the EdgeX vernacular, the Device Service upon receiving these sensed values from the Device would create an Event with two Readings - one Reading would contain the key/value pair of humancount:5 and the other Reading would contain the key/value pair of caninecount:3. The Device Service, on creating the Event and associated Reading objects would transmit this information to Core Data via REST call. POST to http : // localhost : 48080 / api / v1 / event BODY : { \"device\" : \"countcamera1\" , \"readings\" :[ { \"name\" : \"humancount\" , \"value\" : \"5\" } , { \"name\" : \"caninecount\" , \"value\" : \"3\" } ] } If desired, the Device Service can also supply an origin property (see below) to the Event or Reading to suggest the time (in Epoch timestamp/milliseconds format) at which the data was sensed/collected. If an origin is not provided, no origin will be set for the Event or Reading, however every Event and Reading is provided a Created and Modified timestamp in the database to give the data some time context. BODY: {\"device\":\"countcamera1\",\"origin\":1471806386919, \"readings\":[{\"name\":\"humancount\",\"value\":\"1\",\"origin\":1471806386919},{\"name\":\"caninecount\",\"value\":\"0\",\"origin\":1471806386919}]} Origin Timestamp Recommendation! Note: Smart devices will often timestamp sensor data and this timestamp can be used as the origin timestamp. In cases where the sensor/device is unable to provide a timestamp (\"dumb\" or brownfield sensors), it is recommended that the Device Service create a timestamp for the sensor data that is applied as the origin timestamp for the Device. Reading data Now that an Event (or two) and associated Readings have been sent to Core Data, you can use the Core Data API to explore that data that is now stored in MongoDB. Recall from the Test Setup section, you checked that no data was yet stored in Core Data. Make the same call and this time, 2 Event records should be the count returned. GET to http://localhost:48080/api/v1/event/count Retrieve 10 of the Events associated to the countcamera1 Device. GET to http://localhost:48080/api/v1/event/device/countcamera1/10 Retrieve 10 of the human count Readings associated to the countcamera1 Device (i.e. - get Readings by Value Descriptor) GET to http://localhost:48080/api/v1/reading/name/humancount/10","title":"Sending events and reading data"},{"location":"walk-through/Ch-WalkthroughReading/#sending-events-and-reading-data","text":"In the real world, the human/dog counting camera would start to take pictures, count beings, and send that data to EdgeX. To simulate this activity. in this section, you will make Core Data API calls as if you were the camera's Device and Device Service.","title":"Sending events and reading data"},{"location":"walk-through/Ch-WalkthroughReading/#send-an-eventreading","text":"See APIs Core Services Core Data Data is submitted to Core Data as an Event. An Event is a collection of sensor readings from a Device (associated to a Device by its ID or name) at a particular point in time. A Reading in an Event is a particular value sensed by the Device and associated to a Value Descriptor (by name) to provide context to the reading. So, the human/dog counting camera might determine that there are current 5 people and 3 dogs in the space it is monitoring. In the EdgeX vernacular, the Device Service upon receiving these sensed values from the Device would create an Event with two Readings - one Reading would contain the key/value pair of humancount:5 and the other Reading would contain the key/value pair of caninecount:3. The Device Service, on creating the Event and associated Reading objects would transmit this information to Core Data via REST call. POST to http : // localhost : 48080 / api / v1 / event BODY : { \"device\" : \"countcamera1\" , \"readings\" :[ { \"name\" : \"humancount\" , \"value\" : \"5\" } , { \"name\" : \"caninecount\" , \"value\" : \"3\" } ] } If desired, the Device Service can also supply an origin property (see below) to the Event or Reading to suggest the time (in Epoch timestamp/milliseconds format) at which the data was sensed/collected. If an origin is not provided, no origin will be set for the Event or Reading, however every Event and Reading is provided a Created and Modified timestamp in the database to give the data some time context. BODY: {\"device\":\"countcamera1\",\"origin\":1471806386919, \"readings\":[{\"name\":\"humancount\",\"value\":\"1\",\"origin\":1471806386919},{\"name\":\"caninecount\",\"value\":\"0\",\"origin\":1471806386919}]} Origin Timestamp Recommendation! Note: Smart devices will often timestamp sensor data and this timestamp can be used as the origin timestamp. In cases where the sensor/device is unable to provide a timestamp (\"dumb\" or brownfield sensors), it is recommended that the Device Service create a timestamp for the sensor data that is applied as the origin timestamp for the Device.","title":"Send an Event/Reading"},{"location":"walk-through/Ch-WalkthroughReading/#reading-data","text":"Now that an Event (or two) and associated Readings have been sent to Core Data, you can use the Core Data API to explore that data that is now stored in MongoDB. Recall from the Test Setup section, you checked that no data was yet stored in Core Data. Make the same call and this time, 2 Event records should be the count returned. GET to http://localhost:48080/api/v1/event/count Retrieve 10 of the Events associated to the countcamera1 Device. GET to http://localhost:48080/api/v1/event/device/countcamera1/10 Retrieve 10 of the human count Readings associated to the countcamera1 Device (i.e. - get Readings by Value Descriptor) GET to http://localhost:48080/api/v1/reading/name/humancount/10","title":"Reading data"},{"location":"walk-through/Ch-WalkthroughRunning/","text":"Running EdgeX If you have already followed Getting Started Users you will have already downloaded and started these containers, and you can skip this step and go right to the the Walkthrough Use Case Download the docker-compose file After installing Docker and Docker Compose, you need to get a Docker Compose file. EdgeX Foundry has over 12 microservices, each deployed in their own Docker container, and the Docker Compose file will make it easier to download and run them all. A Docker Compose file is a manifest file, which lists: The Docker containers (or more precisely the Docker container images) that should be downloaded, The order in which the containers should be started The parameters under which the containers should be run It is recommended that you use the lastest version of EdgeX Foundry. As of this writing, the latest version can be found here: https://github.com/edgexfoundry/developer-scripts/raw/master/releases/geneva/compose-files/docker-compose-geneva-redis.yml Save this file as docker-compose.yml in your working directory so that the following commands will find it. Download the containers Once you have downloaded the docker-compose.yml file, run the following command to download the containers for each of the EdgeX Foundry microservices. Docker Command Description Suggested Wait Time After Completing docker-compose pull Pull down, but don\u2019t start, all the EdgeX Foundry microservices Docker Compose will indicate when all the containers have been pulled successfully Starting EdgeX For this Walkthrough you will need to run all the services with the exception of any device services, including the device-virtual. The reason is that many of the API calls you make as part of this walk through are actually accomplished by the virtual device service - or any device service for that matter. In this walk through, your manual call of the EdgeX APIs often simulate the work that a device service would do to get a new device setup and to send data to/through EdgeX. Run the following commands to start the core, supporting and export micro services of EdgeX. Docker Command Description Suggested Wait Time After Completing docker-compose up -d volume Start the EdgeX Foundry file volume\u2013must be done before the other services are started A couple of seconds. In the time it takes to type the next command it should be ready. docker-compose up -d consul Start the configuration and registry microservice which all services must register with and get their configuration from A couple of seconds docker-compose up -d config-seed Populate the configuration/registry microservice A couple of seconds docker-compose up -d mongo Start the NoSQL MongoDB container 10 seconds docker-compose up -d logging Start the logging microservice - used by all micro services that make log entries A couple of seconds docker-compose up -d notifications Start the notifications and alerts microservice\u2013used by many of the microservices A couple of seconds docker-compose up -d metadata Start the Core Metadata microservice A couple of seconds docker-compose up -d data Start the Core Data microservice A couple of seconds docker-compose up -d command Start the Core Command microservice A couple of seconds docker-compose up -d scheduler Start the scheduling microservice -used by many of the microservices A couple of seconds docker-compose up -d export-client Start the Export Client registration microservice A couple of seconds docker-compose up -d export-distro Start the Export Distribution microservice A couple of seconds docker-compose up -d rulesengine Start the Rules Engine microservice Note: this service is still implemented in Java and takes more time to start 1 minute Run \"docker-compose ps\" to confirm that all the containers have been downloaded and started. (Note: initialization or seed containers, like config-seed, will have exited as there job is just to initialize the associated service and then exit.)","title":"Running EdgeX"},{"location":"walk-through/Ch-WalkthroughRunning/#running-edgex","text":"If you have already followed Getting Started Users you will have already downloaded and started these containers, and you can skip this step and go right to the the Walkthrough Use Case","title":"Running EdgeX"},{"location":"walk-through/Ch-WalkthroughRunning/#download-the-docker-compose-file","text":"After installing Docker and Docker Compose, you need to get a Docker Compose file. EdgeX Foundry has over 12 microservices, each deployed in their own Docker container, and the Docker Compose file will make it easier to download and run them all. A Docker Compose file is a manifest file, which lists: The Docker containers (or more precisely the Docker container images) that should be downloaded, The order in which the containers should be started The parameters under which the containers should be run It is recommended that you use the lastest version of EdgeX Foundry. As of this writing, the latest version can be found here: https://github.com/edgexfoundry/developer-scripts/raw/master/releases/geneva/compose-files/docker-compose-geneva-redis.yml Save this file as docker-compose.yml in your working directory so that the following commands will find it.","title":"Download the docker-compose file"},{"location":"walk-through/Ch-WalkthroughRunning/#download-the-containers","text":"Once you have downloaded the docker-compose.yml file, run the following command to download the containers for each of the EdgeX Foundry microservices. Docker Command Description Suggested Wait Time After Completing docker-compose pull Pull down, but don\u2019t start, all the EdgeX Foundry microservices Docker Compose will indicate when all the containers have been pulled successfully","title":"Download the containers"},{"location":"walk-through/Ch-WalkthroughRunning/#starting-edgex","text":"For this Walkthrough you will need to run all the services with the exception of any device services, including the device-virtual. The reason is that many of the API calls you make as part of this walk through are actually accomplished by the virtual device service - or any device service for that matter. In this walk through, your manual call of the EdgeX APIs often simulate the work that a device service would do to get a new device setup and to send data to/through EdgeX. Run the following commands to start the core, supporting and export micro services of EdgeX. Docker Command Description Suggested Wait Time After Completing docker-compose up -d volume Start the EdgeX Foundry file volume\u2013must be done before the other services are started A couple of seconds. In the time it takes to type the next command it should be ready. docker-compose up -d consul Start the configuration and registry microservice which all services must register with and get their configuration from A couple of seconds docker-compose up -d config-seed Populate the configuration/registry microservice A couple of seconds docker-compose up -d mongo Start the NoSQL MongoDB container 10 seconds docker-compose up -d logging Start the logging microservice - used by all micro services that make log entries A couple of seconds docker-compose up -d notifications Start the notifications and alerts microservice\u2013used by many of the microservices A couple of seconds docker-compose up -d metadata Start the Core Metadata microservice A couple of seconds docker-compose up -d data Start the Core Data microservice A couple of seconds docker-compose up -d command Start the Core Command microservice A couple of seconds docker-compose up -d scheduler Start the scheduling microservice -used by many of the microservices A couple of seconds docker-compose up -d export-client Start the Export Client registration microservice A couple of seconds docker-compose up -d export-distro Start the Export Distribution microservice A couple of seconds docker-compose up -d rulesengine Start the Rules Engine microservice Note: this service is still implemented in Java and takes more time to start 1 minute Run \"docker-compose ps\" to confirm that all the containers have been downloaded and started. (Note: initialization or seed containers, like config-seed, will have exited as there job is just to initialize the associated service and then exit.)","title":"Starting EdgeX"},{"location":"walk-through/Ch-WalkthroughSetup/","text":"Setup up your environment Install Docker & Docker Compose To run Dockerized EdgeX Foundry, you need to install Docker. See https://docs.docker.com/install/ to learn how to obtain and install Docker. If you are new to using Docker, the same web site provides you additional information. The following short video has is also very informative https://www.youtube.com/watch?time_continue=3&v=VhabrYF1nms Docker Compose is used to orchestrate the fetch (or pull), installation, and the start and stop of the EdgeX Foundry microservice containers. See: https://docs.docker.com/compose/ to learn more about Docker Compose. Docker Compose is automatically installed with Docker for Mac and Windows users. See: https://docs.docker.com/compose/install/ to determine if your Docker installation already contains Docker Compose, and how to install Compose if it does not. You do not need to be an expert with Docker to run EdgeX Foundry. The instructions in this guide provide you with the steps to get EdgeX Foundry running in your environment. Some basic knowledge of these two technologies of Docker and Docker Compose, are nice to have, but not required. Install Postman You can follow this walkthrough making HTTP calls from the command-line with a tool like curl , but it's easier if you use a tool designed for testing REST APIs. For that we like to use Postman . You can download the native Postman app for your operating system. Walk Through Alert! It is assumed that for the purposes of this walk through demonstration all API micro services are running on \"localhost\". If this is not the case, substitute your hostname for localhost. any POST call has the associated CONTENT-TYPE=application/JSON header associated to it unless explicitly stated otherwise.","title":"Setup up your environment"},{"location":"walk-through/Ch-WalkthroughSetup/#setup-up-your-environment","text":"","title":"Setup up your environment"},{"location":"walk-through/Ch-WalkthroughSetup/#install-docker-docker-compose","text":"To run Dockerized EdgeX Foundry, you need to install Docker. See https://docs.docker.com/install/ to learn how to obtain and install Docker. If you are new to using Docker, the same web site provides you additional information. The following short video has is also very informative https://www.youtube.com/watch?time_continue=3&v=VhabrYF1nms Docker Compose is used to orchestrate the fetch (or pull), installation, and the start and stop of the EdgeX Foundry microservice containers. See: https://docs.docker.com/compose/ to learn more about Docker Compose. Docker Compose is automatically installed with Docker for Mac and Windows users. See: https://docs.docker.com/compose/install/ to determine if your Docker installation already contains Docker Compose, and how to install Compose if it does not. You do not need to be an expert with Docker to run EdgeX Foundry. The instructions in this guide provide you with the steps to get EdgeX Foundry running in your environment. Some basic knowledge of these two technologies of Docker and Docker Compose, are nice to have, but not required.","title":"Install Docker &amp; Docker Compose"},{"location":"walk-through/Ch-WalkthroughSetup/#install-postman","text":"You can follow this walkthrough making HTTP calls from the command-line with a tool like curl , but it's easier if you use a tool designed for testing REST APIs. For that we like to use Postman . You can download the native Postman app for your operating system. Walk Through Alert! It is assumed that for the purposes of this walk through demonstration all API micro services are running on \"localhost\". If this is not the case, substitute your hostname for localhost. any POST call has the associated CONTENT-TYPE=application/JSON header associated to it unless explicitly stated otherwise.","title":"Install Postman"},{"location":"walk-through/Ch-WalkthroughUseCase/","text":"Example Use Case Suppose you had a new device that you wanted to connect to EdgeX. The device was a camera that took a picture and then had an on-board chip that analyzed the picture and reported the number of humans and canines (dogs) it saw. How often the camera takes a picture and reports its findings can be configured. In fact, the camera device could be sent two actuation commands - that is sent two requests for which it must respond and do something. You could send a request to set its time, in seconds, between picture snapshots (and then calculating the number of humans and dogs it finds in that resulting image). You could also request it to set the scan depth, in feet, of the camera - that is set how far out the camera looks. The farther out it looks, the less accurate the count of humans and dogs becomes, so this is something the manufacturer wants to allow the user to set based on use case needs. In EdgeX, the camera must be represented by a Device. Each Device is managed by a Device Service micro service. The Device Service communicates with the underlying hardware - in this case the camera - in the protocol of choice for that Device. The Device Service collects the data from the Devices it manages and passes that data into EdgeX (into Core Data). In this case, the Device Service would be collecting the count of humans and dogs that the camera sees. The Device Service also serves to translate the request for actuation from EdgeX and the rest of the world into protocol requests that the physical Device would understand. So in this example, the Device Service would take requests to set the duration between snapshots and to set the scan depth and translate those requests into protocol commands that the camera understood. Exactly how this camera physically connects to the host machine running EdgeX and how the Device Service works under the covers to communicate with the camera Device is immaterial for the point of this demonstration.","title":"Example Use Case"},{"location":"walk-through/Ch-WalkthroughUseCase/#example-use-case","text":"Suppose you had a new device that you wanted to connect to EdgeX. The device was a camera that took a picture and then had an on-board chip that analyzed the picture and reported the number of humans and canines (dogs) it saw. How often the camera takes a picture and reports its findings can be configured. In fact, the camera device could be sent two actuation commands - that is sent two requests for which it must respond and do something. You could send a request to set its time, in seconds, between picture snapshots (and then calculating the number of humans and dogs it finds in that resulting image). You could also request it to set the scan depth, in feet, of the camera - that is set how far out the camera looks. The farther out it looks, the less accurate the count of humans and dogs becomes, so this is something the manufacturer wants to allow the user to set based on use case needs. In EdgeX, the camera must be represented by a Device. Each Device is managed by a Device Service micro service. The Device Service communicates with the underlying hardware - in this case the camera - in the protocol of choice for that Device. The Device Service collects the data from the Devices it manages and passes that data into EdgeX (into Core Data). In this case, the Device Service would be collecting the count of humans and dogs that the camera sees. The Device Service also serves to translate the request for actuation from EdgeX and the rest of the world into protocol requests that the physical Device would understand. So in this example, the Device Service would take requests to set the duration between snapshots and to set the scan depth and translate those requests into protocol commands that the camera understood. Exactly how this camera physically connects to the host machine running EdgeX and how the Device Service works under the covers to communicate with the camera Device is immaterial for the point of this demonstration.","title":"Example Use Case"}]}